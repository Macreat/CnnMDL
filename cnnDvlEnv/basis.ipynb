{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CONVOLUTIONAL NEURAL NETWORK NB (CNN-First on DL)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## libraries and env configuration \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "#packages from tensor flow\n",
        "import tensorflow as Tf\n",
        "\n",
        "# tensor flow for optimizing the model \n",
        "\n",
        "\n",
        "# basis packages \n",
        "import os as os \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy \n",
        "import ssl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available and used :   1\n"
          ]
        }
      ],
      "source": [
        "physicalDevice = Tf.config.experimental.list_physical_devices('GPU')\n",
        "Tf.config.experimental.set_memory_growth(physicalDevice[0], True)\n",
        "print(\"Num GPUs Available and used :  \", len(physicalDevice))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '2' # Suppress TensorFlow logging (1)\t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## data preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load MNIST dataset\n",
        "(xTrainMnist,yTrainMnist),(xTestMnist,yTestMnist)=Tf.keras.datasets.mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### normalization "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "255"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xTrainMnist.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "255"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xTestMnist.max()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "xTestMnist = xTestMnist.astype('float32') / 255 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### changing normalization for data train: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' also could use : \\nmean = np.mean(xTrainMnist)\\nprint(mean)\\n'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# we normalize tghe data lessing the mean and the standar deviation to xtrain data in order to have a better performance on the training process \n",
        "\n",
        "MnistMean = xTrainMnist.mean()\n",
        "MnistStd = xTrainMnist.std()\n",
        "\n",
        "# normalize the data dividing by the sd assecuring the none zero value of the sd\n",
        "\n",
        "xTrainMnist = (xTrainMnist-MnistMean)/(MnistStd+1e-7)\n",
        "\n",
        "# also normalize the test data using mean and std from training data cause the idea is that the network doesnt know these parameters of the test set\n",
        "\n",
        "xTestMnist = (xTestMnist - MnistMean)/(MnistStd+1e-7)\n",
        "\n",
        "\"\"\" also could use : \n",
        "mean = np.mean(xTrainMnist)\n",
        "print(mean)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### split train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "xTrainMnist, xValidMnist = xTrainMnist[5000:],xTrainMnist[:5000]\n",
        "yTrainMnist, yValidMnist = yTrainMnist[5000:],yTrainMnist[:5000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "look dimension size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((55000, 28, 28), (55000,), (5000, 28, 28), (5000,), (10000, 28, 28), (10000,))"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xTrainMnist.shape, yTrainMnist.shape, xValidMnist.shape, yValidMnist.shape , xTestMnist.shape, yTestMnist.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### data argumentation for best performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "datagen = Tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    width_shift_range = 0.1,\n",
        "    height_shift_range = 0.1,\n",
        "    horizontal_flip = True,\n",
        "    vertical_flip = True\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### see number of classes/labels in order to binarize "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(np.unique(yTrainMnist)) # 10 classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####  binarizing the labels in order to use only categorical cross entropy without sparse \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "yTrainMnist = Tf.keras.utils.to_categorical(yTrainMnist,num_classes=10)\n",
        "yTestMnist = Tf.keras.utils.to_categorical(yTestMnist, num_classes= 10)\n",
        "yValidMnist = Tf.keras.utils.to_categorical(yValidMnist, num_classes=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], dtype=float32)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "yTrainMnist[0] # one hot encoding of the labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
            "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ]
        }
      ],
      "source": [
        "print(yTrainMnist[0:10]) # display the first 10 labels of the training set\n",
        "print(yTestMnist[0:10]) # display the first 10 labels of the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((55000, 28, 28), (55000, 10), (10000, 28, 28), (10000, 10))"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xTrainMnist.shape, yTrainMnist.shape, xTestMnist.shape, yTestMnist.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## creating another structure for the sequential model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "proceed to use a kernel recognition filter on every layer, besides of another techniques to avoid overfitting: \n",
        "\n",
        "- dropout\n",
        "- batch normalization \n",
        "- flatten \n",
        "- global average pooling\n",
        "- regularizacion l1 o l2\n",
        "- estructura de hyperparámetros\n",
        "- funciones de activacion \n",
        "- (PRUNNING & SPARSITY ¿?) \n",
        "    * Función: Eliminan conexiones o neuronas innecesarias en la red, reduciendo la complejidad del modelo y mejorando la eficiencia computacional.​\n",
        "\n",
        "    * Implementación: Se aplican después del entrenamiento inicial para identificar y eliminar pesos insignificantes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "KernelBase = 32 \n",
        "WeightRegularizer = 1e-4 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "MnistModel = Tf.keras.models.Sequential() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "making layer by layer \n",
        "First one is a convolutional sequence of layers : \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "MnistModel.add(Tf.keras.layers.Conv2D(KernelBase, (3,3), padding = 'same', input_shape=(28,28,1), kernel_regularizer = Tf.keras.regularizers.l2(WeightRegularizer)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "once we add the first convolutional layer with 32 filters of size 3x3 and a regularization term to avoid overfitting\n",
        "proceed adding the activation function ReLU to the output of the convolutional layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "MnistModel.add(Tf.keras.layers.Activation('relu'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " finally goes with the batch normalization to normalize the output of the previous layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "MnistModel.add(Tf.keras.layers.BatchNormalization())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "also implement a maxpooling 2d layer and dropout on this another layer : \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "MnistModel.add(Tf.keras.layers.Conv2D(KernelBase, (3, 3), padding='same', activation='relu', kernel_regularizer=Tf.keras.regularizers.l2(WeightRegularizer), input_shape=(28, 28, 1)))\n",
        "MnistModel.add(Tf.keras.layers.LeakyReLU(alpha=0.1)) # we use a different activation function to see if it improves the performance of the model\n",
        "MnistModel.add(Tf.keras.layers.BatchNormalization())\n",
        "MnistModel.add(Tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')) \n",
        "MnistModel.add(Tf.keras.layers.Dropout(0.25)) # we add a dropout layer to reduce overfitting "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "now, we nee to do the last classification layer with a dense and a flatten function "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add a Flatten layer to convert the 2D images to 1D vectors  (transform the img to an array)\n",
        "MnistModel.add(Tf.keras.layers.Flatten()) # without the input size "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "finally,  10 classes for the output layer with a softmax activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "MnistModel.add(Tf.keras.layers.Dense(10,activation= 'softmax'))  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "lets see a summary of the structure: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 28, 28, 32)        320       \n",
            "                                                                 \n",
            " activation (Activation)     (None, 28, 28, 32)        0         \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 28, 28, 32)       128       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 28, 28, 32)        9248      \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 28, 28, 32)        0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 28, 28, 32)       128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 14, 14, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 14, 14, 32)        0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 6272)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                62730     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 72,554\n",
            "Trainable params: 72,426\n",
            "Non-trainable params: 128\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "MnistModel.summary() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "proceed compiling the model \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "MnistModel.compile(optimizer=Tf.keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy']) # we use adam optimizer with a learning rate of 0.001 and categorical cross entropy as loss function\n",
        "# use addam optimizer with a learning rate of 0.001 and categorical cross entropy as loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "also we can do a callback checkpoint when were we are fitting the model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "CBCheckPoint = Tf.keras.callbacks.ModelCheckpoint(\"best_model,keras\", verbose = 1, save_best_only = True, monitor = \"val_accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### proceding training the model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Asegúrate de que los datos tengan 4 dimensiones\n",
        "xTrainMnist = np.expand_dims(xTrainMnist, axis=-1)  # Agrega la dimensión del canal\n",
        "xValidMnist = np.expand_dims(xValidMnist, axis=-1)  # Agrega la dimensión del canal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "\n",
            "Epoch 1: val_accuracy improved from -inf to 0.66840, saving model to best_model,keras\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "429/429 - 72s - loss: 1.2227 - accuracy: 0.6190 - val_loss: 0.9820 - val_accuracy: 0.6684 - 72s/epoch - 167ms/step\n",
            "Epoch 2/1000\n",
            "\n",
            "Epoch 2: val_accuracy improved from 0.66840 to 0.80460, saving model to best_model,keras\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "429/429 - 18s - loss: 0.7864 - accuracy: 0.7418 - val_loss: 0.6447 - val_accuracy: 0.8046 - 18s/epoch - 43ms/step\n",
            "Epoch 3/1000\n",
            "\n",
            "Epoch 3: val_accuracy improved from 0.80460 to 0.83920, saving model to best_model,keras\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "429/429 - 18s - loss: 0.6536 - accuracy: 0.7818 - val_loss: 0.5268 - val_accuracy: 0.8392 - 18s/epoch - 43ms/step\n",
            "Epoch 4/1000\n",
            "\n",
            "Epoch 4: val_accuracy improved from 0.83920 to 0.83960, saving model to best_model,keras\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "429/429 - 20s - loss: 0.5638 - accuracy: 0.8136 - val_loss: 0.5055 - val_accuracy: 0.8396 - 20s/epoch - 47ms/step\n",
            "Epoch 5/1000\n",
            "\n",
            "Epoch 5: val_accuracy improved from 0.83960 to 0.87880, saving model to best_model,keras\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "429/429 - 25s - loss: 0.5122 - accuracy: 0.8297 - val_loss: 0.3733 - val_accuracy: 0.8788 - 25s/epoch - 59ms/step\n",
            "Epoch 6/1000\n",
            "\n",
            "Epoch 6: val_accuracy did not improve from 0.87880\n",
            "429/429 - 20s - loss: 0.4784 - accuracy: 0.8416 - val_loss: 0.3632 - val_accuracy: 0.8764 - 20s/epoch - 48ms/step\n",
            "Epoch 7/1000\n",
            "\n",
            "Epoch 7: val_accuracy did not improve from 0.87880\n",
            "429/429 - 19s - loss: 0.4568 - accuracy: 0.8480 - val_loss: 0.4126 - val_accuracy: 0.8702 - 19s/epoch - 45ms/step\n",
            "Epoch 8/1000\n",
            "\n",
            "Epoch 8: val_accuracy did not improve from 0.87880\n",
            "429/429 - 21s - loss: 0.4319 - accuracy: 0.8570 - val_loss: 0.3899 - val_accuracy: 0.8728 - 21s/epoch - 49ms/step\n",
            "Epoch 9/1000\n",
            "\n",
            "Epoch 9: val_accuracy improved from 0.87880 to 0.90080, saving model to best_model,keras\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "429/429 - 34s - loss: 0.4208 - accuracy: 0.8608 - val_loss: 0.3139 - val_accuracy: 0.9008 - 34s/epoch - 79ms/step\n",
            "Epoch 10/1000\n",
            "\n",
            "Epoch 10: val_accuracy improved from 0.90080 to 0.92040, saving model to best_model,keras\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "429/429 - 36s - loss: 0.4129 - accuracy: 0.8638 - val_loss: 0.2528 - val_accuracy: 0.9204 - 36s/epoch - 85ms/step\n",
            "Epoch 11/1000\n",
            "\n",
            "Epoch 11: val_accuracy did not improve from 0.92040\n",
            "429/429 - 21s - loss: 0.3986 - accuracy: 0.8698 - val_loss: 0.5123 - val_accuracy: 0.8360 - 21s/epoch - 49ms/step\n",
            "Epoch 12/1000\n",
            "\n",
            "Epoch 12: val_accuracy did not improve from 0.92040\n",
            "429/429 - 51s - loss: 0.3987 - accuracy: 0.8692 - val_loss: 0.3744 - val_accuracy: 0.8688 - 51s/epoch - 120ms/step\n",
            "Epoch 13/1000\n",
            "\n",
            "Epoch 13: val_accuracy did not improve from 0.92040\n",
            "429/429 - 40s - loss: 0.3888 - accuracy: 0.8734 - val_loss: 0.3060 - val_accuracy: 0.9012 - 40s/epoch - 92ms/step\n",
            "Epoch 14/1000\n",
            "\n",
            "Epoch 14: val_accuracy did not improve from 0.92040\n",
            "429/429 - 28s - loss: 0.3829 - accuracy: 0.8736 - val_loss: 0.2919 - val_accuracy: 0.9056 - 28s/epoch - 65ms/step\n",
            "Epoch 15/1000\n",
            "\n",
            "Epoch 15: val_accuracy did not improve from 0.92040\n",
            "429/429 - 22s - loss: 0.3730 - accuracy: 0.8778 - val_loss: 0.3104 - val_accuracy: 0.9030 - 22s/epoch - 52ms/step\n",
            "Epoch 16/1000\n",
            "\n",
            "Epoch 16: val_accuracy did not improve from 0.92040\n",
            "429/429 - 23s - loss: 0.3700 - accuracy: 0.8789 - val_loss: 0.2694 - val_accuracy: 0.9168 - 23s/epoch - 52ms/step\n",
            "Epoch 17/1000\n",
            "\n",
            "Epoch 17: val_accuracy improved from 0.92040 to 0.92340, saving model to best_model,keras\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "429/429 - 24s - loss: 0.3672 - accuracy: 0.8808 - val_loss: 0.2521 - val_accuracy: 0.9234 - 24s/epoch - 57ms/step\n",
            "Epoch 18/1000\n",
            "\n",
            "Epoch 18: val_accuracy did not improve from 0.92340\n",
            "429/429 - 22s - loss: 0.3627 - accuracy: 0.8826 - val_loss: 0.2737 - val_accuracy: 0.9138 - 22s/epoch - 52ms/step\n",
            "Epoch 19/1000\n",
            "\n",
            "Epoch 19: val_accuracy did not improve from 0.92340\n",
            "429/429 - 22s - loss: 0.3602 - accuracy: 0.8813 - val_loss: 0.3797 - val_accuracy: 0.8714 - 22s/epoch - 52ms/step\n",
            "Epoch 20/1000\n",
            "\n",
            "Epoch 20: val_accuracy did not improve from 0.92340\n",
            "429/429 - 22s - loss: 0.3570 - accuracy: 0.8847 - val_loss: 0.2715 - val_accuracy: 0.9062 - 22s/epoch - 50ms/step\n",
            "Epoch 21/1000\n",
            "\n",
            "Epoch 21: val_accuracy did not improve from 0.92340\n",
            "429/429 - 23s - loss: 0.3545 - accuracy: 0.8844 - val_loss: 0.2576 - val_accuracy: 0.9172 - 23s/epoch - 53ms/step\n",
            "Epoch 22/1000\n",
            "\n",
            "Epoch 22: val_accuracy did not improve from 0.92340\n",
            "429/429 - 24s - loss: 0.3495 - accuracy: 0.8864 - val_loss: 0.2842 - val_accuracy: 0.9156 - 24s/epoch - 57ms/step\n",
            "Epoch 23/1000\n",
            "\n",
            "Epoch 23: val_accuracy improved from 0.92340 to 0.92560, saving model to best_model,keras\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "429/429 - 26s - loss: 0.3423 - accuracy: 0.8894 - val_loss: 0.2415 - val_accuracy: 0.9256 - 26s/epoch - 59ms/step\n",
            "Epoch 24/1000\n",
            "\n",
            "Epoch 24: val_accuracy did not improve from 0.92560\n",
            "429/429 - 24s - loss: 0.3440 - accuracy: 0.8882 - val_loss: 0.2945 - val_accuracy: 0.9026 - 24s/epoch - 56ms/step\n",
            "Epoch 25/1000\n",
            "\n",
            "Epoch 25: val_accuracy did not improve from 0.92560\n",
            "429/429 - 24s - loss: 0.3376 - accuracy: 0.8899 - val_loss: 0.2461 - val_accuracy: 0.9252 - 24s/epoch - 57ms/step\n",
            "Epoch 26/1000\n",
            "\n",
            "Epoch 26: val_accuracy did not improve from 0.92560\n",
            "429/429 - 29s - loss: 0.3378 - accuracy: 0.8889 - val_loss: 0.2587 - val_accuracy: 0.9150 - 29s/epoch - 67ms/step\n",
            "Epoch 27/1000\n",
            "\n",
            "Epoch 27: val_accuracy did not improve from 0.92560\n",
            "429/429 - 24s - loss: 0.3375 - accuracy: 0.8912 - val_loss: 0.2538 - val_accuracy: 0.9168 - 24s/epoch - 57ms/step\n",
            "Epoch 28/1000\n",
            "\n",
            "Epoch 28: val_accuracy did not improve from 0.92560\n",
            "429/429 - 22s - loss: 0.3398 - accuracy: 0.8904 - val_loss: 0.2407 - val_accuracy: 0.9206 - 22s/epoch - 50ms/step\n",
            "Epoch 29/1000\n",
            "\n",
            "Epoch 29: val_accuracy did not improve from 0.92560\n",
            "429/429 - 21s - loss: 0.3335 - accuracy: 0.8915 - val_loss: 0.2481 - val_accuracy: 0.9244 - 21s/epoch - 48ms/step\n",
            "Epoch 30/1000\n",
            "\n",
            "Epoch 30: val_accuracy did not improve from 0.92560\n",
            "429/429 - 21s - loss: 0.3287 - accuracy: 0.8936 - val_loss: 0.2509 - val_accuracy: 0.9216 - 21s/epoch - 48ms/step\n",
            "Epoch 31/1000\n",
            "\n",
            "Epoch 31: val_accuracy did not improve from 0.92560\n",
            "429/429 - 20s - loss: 0.3276 - accuracy: 0.8928 - val_loss: 0.2579 - val_accuracy: 0.9152 - 20s/epoch - 47ms/step\n",
            "Epoch 32/1000\n",
            "\n",
            "Epoch 32: val_accuracy did not improve from 0.92560\n",
            "429/429 - 22s - loss: 0.3267 - accuracy: 0.8955 - val_loss: 0.2863 - val_accuracy: 0.9024 - 22s/epoch - 52ms/step\n",
            "Epoch 33/1000\n",
            "\n",
            "Epoch 33: val_accuracy did not improve from 0.92560\n",
            "429/429 - 21s - loss: 0.3262 - accuracy: 0.8954 - val_loss: 0.2626 - val_accuracy: 0.9158 - 21s/epoch - 48ms/step\n",
            "Epoch 34/1000\n",
            "\n",
            "Epoch 34: val_accuracy improved from 0.92560 to 0.93680, saving model to best_model,keras\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "429/429 - 23s - loss: 0.3218 - accuracy: 0.8959 - val_loss: 0.2107 - val_accuracy: 0.9368 - 23s/epoch - 53ms/step\n",
            "Epoch 35/1000\n",
            "\n",
            "Epoch 35: val_accuracy did not improve from 0.93680\n",
            "429/429 - 23s - loss: 0.3226 - accuracy: 0.8961 - val_loss: 0.2723 - val_accuracy: 0.9078 - 23s/epoch - 53ms/step\n",
            "Epoch 36/1000\n",
            "\n",
            "Epoch 36: val_accuracy did not improve from 0.93680\n",
            "429/429 - 21s - loss: 0.3285 - accuracy: 0.8922 - val_loss: 0.2594 - val_accuracy: 0.9182 - 21s/epoch - 49ms/step\n",
            "Epoch 37/1000\n",
            "\n",
            "Epoch 37: val_accuracy did not improve from 0.93680\n",
            "429/429 - 20s - loss: 0.3217 - accuracy: 0.8967 - val_loss: 0.2354 - val_accuracy: 0.9254 - 20s/epoch - 46ms/step\n",
            "Epoch 38/1000\n",
            "\n",
            "Epoch 38: val_accuracy did not improve from 0.93680\n",
            "429/429 - 20s - loss: 0.3217 - accuracy: 0.8954 - val_loss: 0.2521 - val_accuracy: 0.9184 - 20s/epoch - 46ms/step\n",
            "Epoch 39/1000\n",
            "\n",
            "Epoch 39: val_accuracy did not improve from 0.93680\n",
            "429/429 - 20s - loss: 0.3179 - accuracy: 0.8990 - val_loss: 0.2442 - val_accuracy: 0.9218 - 20s/epoch - 47ms/step\n",
            "Epoch 40/1000\n",
            "\n",
            "Epoch 40: val_accuracy did not improve from 0.93680\n",
            "429/429 - 22s - loss: 0.3198 - accuracy: 0.8966 - val_loss: 0.2329 - val_accuracy: 0.9220 - 22s/epoch - 52ms/step\n",
            "Epoch 41/1000\n",
            "\n",
            "Epoch 41: val_accuracy did not improve from 0.93680\n",
            "429/429 - 28s - loss: 0.3160 - accuracy: 0.8981 - val_loss: 0.2882 - val_accuracy: 0.9084 - 28s/epoch - 64ms/step\n",
            "Epoch 42/1000\n",
            "\n",
            "Epoch 42: val_accuracy did not improve from 0.93680\n",
            "429/429 - 23s - loss: 0.3178 - accuracy: 0.8983 - val_loss: 0.2279 - val_accuracy: 0.9282 - 23s/epoch - 54ms/step\n",
            "Epoch 43/1000\n",
            "\n",
            "Epoch 43: val_accuracy did not improve from 0.93680\n",
            "429/429 - 20s - loss: 0.3147 - accuracy: 0.8996 - val_loss: 0.2600 - val_accuracy: 0.9146 - 20s/epoch - 47ms/step\n",
            "Epoch 44/1000\n",
            "\n",
            "Epoch 44: val_accuracy did not improve from 0.93680\n",
            "429/429 - 23s - loss: 0.3213 - accuracy: 0.8978 - val_loss: 0.2394 - val_accuracy: 0.9250 - 23s/epoch - 53ms/step\n",
            "Epoch 45/1000\n",
            "\n",
            "Epoch 45: val_accuracy did not improve from 0.93680\n",
            "429/429 - 23s - loss: 0.3110 - accuracy: 0.9000 - val_loss: 0.2280 - val_accuracy: 0.9284 - 23s/epoch - 53ms/step\n",
            "Epoch 46/1000\n",
            "\n",
            "Epoch 46: val_accuracy did not improve from 0.93680\n",
            "429/429 - 22s - loss: 0.3143 - accuracy: 0.8998 - val_loss: 0.2564 - val_accuracy: 0.9178 - 22s/epoch - 51ms/step\n",
            "Epoch 47/1000\n",
            "\n",
            "Epoch 47: val_accuracy did not improve from 0.93680\n",
            "429/429 - 22s - loss: 0.3101 - accuracy: 0.9012 - val_loss: 0.2152 - val_accuracy: 0.9342 - 22s/epoch - 52ms/step\n",
            "Epoch 48/1000\n",
            "\n",
            "Epoch 48: val_accuracy did not improve from 0.93680\n",
            "429/429 - 22s - loss: 0.3115 - accuracy: 0.9003 - val_loss: 0.2377 - val_accuracy: 0.9222 - 22s/epoch - 52ms/step\n",
            "Epoch 49/1000\n",
            "\n",
            "Epoch 49: val_accuracy did not improve from 0.93680\n",
            "429/429 - 22s - loss: 0.3123 - accuracy: 0.9000 - val_loss: 0.2450 - val_accuracy: 0.9250 - 22s/epoch - 51ms/step\n",
            "Epoch 50/1000\n",
            "\n",
            "Epoch 50: val_accuracy did not improve from 0.93680\n",
            "429/429 - 23s - loss: 0.3120 - accuracy: 0.9002 - val_loss: 0.2372 - val_accuracy: 0.9276 - 23s/epoch - 53ms/step\n",
            "Epoch 51/1000\n",
            "\n",
            "Epoch 51: val_accuracy did not improve from 0.93680\n",
            "429/429 - 22s - loss: 0.3057 - accuracy: 0.9027 - val_loss: 0.2308 - val_accuracy: 0.9250 - 22s/epoch - 51ms/step\n",
            "Epoch 52/1000\n",
            "\n",
            "Epoch 52: val_accuracy did not improve from 0.93680\n",
            "429/429 - 23s - loss: 0.3120 - accuracy: 0.8998 - val_loss: 0.2219 - val_accuracy: 0.9298 - 23s/epoch - 54ms/step\n",
            "Epoch 53/1000\n",
            "\n",
            "Epoch 53: val_accuracy did not improve from 0.93680\n",
            "429/429 - 21s - loss: 0.3067 - accuracy: 0.9039 - val_loss: 0.2507 - val_accuracy: 0.9196 - 21s/epoch - 48ms/step\n",
            "Epoch 54/1000\n",
            "\n",
            "Epoch 54: val_accuracy did not improve from 0.93680\n",
            "429/429 - 20s - loss: 0.3044 - accuracy: 0.9040 - val_loss: 0.2575 - val_accuracy: 0.9156 - 20s/epoch - 46ms/step\n",
            "Epoch 55/1000\n",
            "\n",
            "Epoch 55: val_accuracy did not improve from 0.93680\n",
            "429/429 - 21s - loss: 0.3013 - accuracy: 0.9042 - val_loss: 0.2313 - val_accuracy: 0.9290 - 21s/epoch - 48ms/step\n",
            "Epoch 56/1000\n",
            "\n",
            "Epoch 56: val_accuracy did not improve from 0.93680\n",
            "429/429 - 20s - loss: 0.3065 - accuracy: 0.9026 - val_loss: 0.2495 - val_accuracy: 0.9174 - 20s/epoch - 46ms/step\n",
            "Epoch 57/1000\n",
            "\n",
            "Epoch 57: val_accuracy did not improve from 0.93680\n",
            "429/429 - 19s - loss: 0.3041 - accuracy: 0.9027 - val_loss: 0.3469 - val_accuracy: 0.8906 - 19s/epoch - 44ms/step\n",
            "Epoch 58/1000\n",
            "\n",
            "Epoch 58: val_accuracy improved from 0.93680 to 0.93720, saving model to best_model,keras\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "429/429 - 22s - loss: 0.3064 - accuracy: 0.9023 - val_loss: 0.2058 - val_accuracy: 0.9372 - 22s/epoch - 50ms/step\n",
            "Epoch 59/1000\n",
            "\n",
            "Epoch 59: val_accuracy did not improve from 0.93720\n",
            "429/429 - 19s - loss: 0.3044 - accuracy: 0.9031 - val_loss: 0.2247 - val_accuracy: 0.9290 - 19s/epoch - 43ms/step\n",
            "Epoch 60/1000\n",
            "\n",
            "Epoch 60: val_accuracy did not improve from 0.93720\n",
            "429/429 - 19s - loss: 0.2986 - accuracy: 0.9050 - val_loss: 0.2201 - val_accuracy: 0.9352 - 19s/epoch - 44ms/step\n",
            "Epoch 61/1000\n",
            "\n",
            "Epoch 61: val_accuracy improved from 0.93720 to 0.93880, saving model to best_model,keras\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "429/429 - 20s - loss: 0.2999 - accuracy: 0.9044 - val_loss: 0.2004 - val_accuracy: 0.9388 - 20s/epoch - 46ms/step\n",
            "Epoch 62/1000\n",
            "\n",
            "Epoch 62: val_accuracy did not improve from 0.93880\n",
            "429/429 - 22s - loss: 0.3009 - accuracy: 0.9046 - val_loss: 0.2252 - val_accuracy: 0.9312 - 22s/epoch - 52ms/step\n",
            "Epoch 63/1000\n",
            "\n",
            "Epoch 63: val_accuracy did not improve from 0.93880\n",
            "429/429 - 21s - loss: 0.3006 - accuracy: 0.9032 - val_loss: 0.2263 - val_accuracy: 0.9290 - 21s/epoch - 48ms/step\n",
            "Epoch 64/1000\n",
            "\n",
            "Epoch 64: val_accuracy did not improve from 0.93880\n",
            "429/429 - 20s - loss: 0.2982 - accuracy: 0.9065 - val_loss: 0.2289 - val_accuracy: 0.9290 - 20s/epoch - 47ms/step\n",
            "Epoch 65/1000\n",
            "\n",
            "Epoch 65: val_accuracy did not improve from 0.93880\n",
            "429/429 - 21s - loss: 0.2999 - accuracy: 0.9051 - val_loss: 0.2495 - val_accuracy: 0.9256 - 21s/epoch - 48ms/step\n",
            "Epoch 66/1000\n",
            "\n",
            "Epoch 66: val_accuracy did not improve from 0.93880\n",
            "429/429 - 21s - loss: 0.2975 - accuracy: 0.9053 - val_loss: 0.2221 - val_accuracy: 0.9334 - 21s/epoch - 48ms/step\n",
            "Epoch 67/1000\n",
            "\n",
            "Epoch 67: val_accuracy did not improve from 0.93880\n",
            "429/429 - 20s - loss: 0.3012 - accuracy: 0.9062 - val_loss: 0.2084 - val_accuracy: 0.9366 - 20s/epoch - 47ms/step\n",
            "Epoch 68/1000\n",
            "\n",
            "Epoch 68: val_accuracy did not improve from 0.93880\n",
            "429/429 - 21s - loss: 0.2949 - accuracy: 0.9062 - val_loss: 0.2253 - val_accuracy: 0.9300 - 21s/epoch - 50ms/step\n",
            "Epoch 69/1000\n",
            "\n",
            "Epoch 69: val_accuracy did not improve from 0.93880\n",
            "429/429 - 21s - loss: 0.2960 - accuracy: 0.9055 - val_loss: 0.2252 - val_accuracy: 0.9318 - 21s/epoch - 49ms/step\n",
            "Epoch 70/1000\n",
            "\n",
            "Epoch 70: val_accuracy did not improve from 0.93880\n",
            "429/429 - 20s - loss: 0.2958 - accuracy: 0.9074 - val_loss: 0.2300 - val_accuracy: 0.9278 - 20s/epoch - 47ms/step\n",
            "Epoch 71/1000\n",
            "\n",
            "Epoch 71: val_accuracy did not improve from 0.93880\n",
            "429/429 - 21s - loss: 0.2963 - accuracy: 0.9056 - val_loss: 0.2049 - val_accuracy: 0.9382 - 21s/epoch - 49ms/step\n",
            "Epoch 72/1000\n",
            "\n",
            "Epoch 72: val_accuracy did not improve from 0.93880\n",
            "429/429 - 19s - loss: 0.2980 - accuracy: 0.9071 - val_loss: 0.2779 - val_accuracy: 0.9140 - 19s/epoch - 45ms/step\n",
            "Epoch 73/1000\n",
            "\n",
            "Epoch 73: val_accuracy did not improve from 0.93880\n",
            "429/429 - 19s - loss: 0.2938 - accuracy: 0.9070 - val_loss: 0.2531 - val_accuracy: 0.9220 - 19s/epoch - 44ms/step\n",
            "Epoch 74/1000\n",
            "\n",
            "Epoch 74: val_accuracy did not improve from 0.93880\n",
            "429/429 - 19s - loss: 0.2957 - accuracy: 0.9069 - val_loss: 0.2322 - val_accuracy: 0.9268 - 19s/epoch - 44ms/step\n",
            "Epoch 75/1000\n",
            "\n",
            "Epoch 75: val_accuracy did not improve from 0.93880\n",
            "429/429 - 18s - loss: 0.2930 - accuracy: 0.9064 - val_loss: 0.2832 - val_accuracy: 0.9084 - 18s/epoch - 43ms/step\n",
            "Epoch 76/1000\n",
            "\n",
            "Epoch 76: val_accuracy did not improve from 0.93880\n",
            "429/429 - 21s - loss: 0.2954 - accuracy: 0.9068 - val_loss: 0.2216 - val_accuracy: 0.9342 - 21s/epoch - 49ms/step\n",
            "Epoch 77/1000\n",
            "\n",
            "Epoch 77: val_accuracy improved from 0.93880 to 0.94140, saving model to best_model,keras\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "429/429 - 21s - loss: 0.2914 - accuracy: 0.9080 - val_loss: 0.2059 - val_accuracy: 0.9414 - 21s/epoch - 49ms/step\n",
            "Epoch 78/1000\n",
            "\n",
            "Epoch 78: val_accuracy did not improve from 0.94140\n",
            "429/429 - 19s - loss: 0.2930 - accuracy: 0.9069 - val_loss: 0.2327 - val_accuracy: 0.9314 - 19s/epoch - 45ms/step\n",
            "Epoch 79/1000\n",
            "\n",
            "Epoch 79: val_accuracy did not improve from 0.94140\n",
            "429/429 - 20s - loss: 0.2913 - accuracy: 0.9085 - val_loss: 0.2064 - val_accuracy: 0.9406 - 20s/epoch - 47ms/step\n",
            "Epoch 80/1000\n",
            "\n",
            "Epoch 80: val_accuracy did not improve from 0.94140\n",
            "429/429 - 19s - loss: 0.2949 - accuracy: 0.9074 - val_loss: 0.2176 - val_accuracy: 0.9298 - 19s/epoch - 45ms/step\n",
            "Epoch 81/1000\n",
            "\n",
            "Epoch 81: val_accuracy did not improve from 0.94140\n",
            "429/429 - 19s - loss: 0.2863 - accuracy: 0.9084 - val_loss: 0.2324 - val_accuracy: 0.9314 - 19s/epoch - 45ms/step\n",
            "Epoch 82/1000\n",
            "\n",
            "Epoch 82: val_accuracy did not improve from 0.94140\n",
            "429/429 - 19s - loss: 0.2946 - accuracy: 0.9068 - val_loss: 0.2192 - val_accuracy: 0.9324 - 19s/epoch - 45ms/step\n",
            "Epoch 83/1000\n",
            "\n",
            "Epoch 83: val_accuracy did not improve from 0.94140\n",
            "429/429 - 21s - loss: 0.2886 - accuracy: 0.9084 - val_loss: 0.2343 - val_accuracy: 0.9286 - 21s/epoch - 49ms/step\n",
            "Epoch 84/1000\n",
            "\n",
            "Epoch 84: val_accuracy did not improve from 0.94140\n",
            "429/429 - 20s - loss: 0.2889 - accuracy: 0.9091 - val_loss: 0.2343 - val_accuracy: 0.9222 - 20s/epoch - 46ms/step\n",
            "Epoch 85/1000\n",
            "\n",
            "Epoch 85: val_accuracy improved from 0.94140 to 0.94280, saving model to best_model,keras\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "429/429 - 23s - loss: 0.2926 - accuracy: 0.9078 - val_loss: 0.1889 - val_accuracy: 0.9428 - 23s/epoch - 54ms/step\n",
            "Epoch 86/1000\n",
            "\n",
            "Epoch 86: val_accuracy did not improve from 0.94280\n",
            "429/429 - 22s - loss: 0.2867 - accuracy: 0.9084 - val_loss: 0.2002 - val_accuracy: 0.9418 - 22s/epoch - 52ms/step\n",
            "Epoch 87/1000\n",
            "\n",
            "Epoch 87: val_accuracy did not improve from 0.94280\n",
            "429/429 - 21s - loss: 0.2939 - accuracy: 0.9076 - val_loss: 0.2620 - val_accuracy: 0.9182 - 21s/epoch - 48ms/step\n",
            "Epoch 88/1000\n",
            "\n",
            "Epoch 88: val_accuracy did not improve from 0.94280\n",
            "429/429 - 21s - loss: 0.2898 - accuracy: 0.9089 - val_loss: 0.2277 - val_accuracy: 0.9296 - 21s/epoch - 49ms/step\n",
            "Epoch 89/1000\n",
            "\n",
            "Epoch 89: val_accuracy did not improve from 0.94280\n",
            "429/429 - 21s - loss: 0.2878 - accuracy: 0.9098 - val_loss: 0.1890 - val_accuracy: 0.9424 - 21s/epoch - 49ms/step\n",
            "Epoch 90/1000\n",
            "\n",
            "Epoch 90: val_accuracy did not improve from 0.94280\n",
            "429/429 - 21s - loss: 0.2844 - accuracy: 0.9094 - val_loss: 0.2007 - val_accuracy: 0.9394 - 21s/epoch - 49ms/step\n",
            "Epoch 91/1000\n",
            "\n",
            "Epoch 91: val_accuracy did not improve from 0.94280\n",
            "429/429 - 22s - loss: 0.2841 - accuracy: 0.9108 - val_loss: 0.2267 - val_accuracy: 0.9256 - 22s/epoch - 50ms/step\n",
            "Epoch 92/1000\n",
            "\n",
            "Epoch 92: val_accuracy did not improve from 0.94280\n",
            "429/429 - 21s - loss: 0.2909 - accuracy: 0.9082 - val_loss: 0.1993 - val_accuracy: 0.9400 - 21s/epoch - 49ms/step\n",
            "Epoch 93/1000\n",
            "\n",
            "Epoch 93: val_accuracy did not improve from 0.94280\n",
            "429/429 - 22s - loss: 0.2862 - accuracy: 0.9078 - val_loss: 0.2111 - val_accuracy: 0.9374 - 22s/epoch - 51ms/step\n",
            "Epoch 94/1000\n",
            "\n",
            "Epoch 94: val_accuracy did not improve from 0.94280\n",
            "429/429 - 22s - loss: 0.2868 - accuracy: 0.9094 - val_loss: 0.2103 - val_accuracy: 0.9350 - 22s/epoch - 50ms/step\n",
            "Epoch 95/1000\n",
            "\n",
            "Epoch 95: val_accuracy improved from 0.94280 to 0.94580, saving model to best_model,keras\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "429/429 - 23s - loss: 0.2893 - accuracy: 0.9084 - val_loss: 0.1911 - val_accuracy: 0.9458 - 23s/epoch - 54ms/step\n",
            "Epoch 96/1000\n",
            "\n",
            "Epoch 96: val_accuracy did not improve from 0.94580\n",
            "429/429 - 23s - loss: 0.2854 - accuracy: 0.9103 - val_loss: 0.2078 - val_accuracy: 0.9366 - 23s/epoch - 54ms/step\n",
            "Epoch 97/1000\n",
            "\n",
            "Epoch 97: val_accuracy did not improve from 0.94580\n",
            "429/429 - 24s - loss: 0.2895 - accuracy: 0.9081 - val_loss: 0.2484 - val_accuracy: 0.9230 - 24s/epoch - 56ms/step\n",
            "Epoch 98/1000\n",
            "\n",
            "Epoch 98: val_accuracy did not improve from 0.94580\n",
            "429/429 - 21s - loss: 0.2900 - accuracy: 0.9081 - val_loss: 0.2274 - val_accuracy: 0.9332 - 21s/epoch - 50ms/step\n",
            "Epoch 99/1000\n",
            "\n",
            "Epoch 99: val_accuracy did not improve from 0.94580\n",
            "429/429 - 20s - loss: 0.2879 - accuracy: 0.9089 - val_loss: 0.2438 - val_accuracy: 0.9204 - 20s/epoch - 46ms/step\n",
            "Epoch 100/1000\n",
            "\n",
            "Epoch 100: val_accuracy improved from 0.94580 to 0.95000, saving model to best_model,keras\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "429/429 - 21s - loss: 0.2884 - accuracy: 0.9093 - val_loss: 0.1876 - val_accuracy: 0.9500 - 21s/epoch - 49ms/step\n",
            "Epoch 101/1000\n",
            "\n",
            "Epoch 101: val_accuracy did not improve from 0.95000\n",
            "429/429 - 20s - loss: 0.2855 - accuracy: 0.9110 - val_loss: 0.2145 - val_accuracy: 0.9412 - 20s/epoch - 47ms/step\n",
            "Epoch 102/1000\n",
            "\n",
            "Epoch 102: val_accuracy did not improve from 0.95000\n",
            "429/429 - 20s - loss: 0.2844 - accuracy: 0.9096 - val_loss: 0.2160 - val_accuracy: 0.9382 - 20s/epoch - 47ms/step\n",
            "Epoch 103/1000\n",
            "\n",
            "Epoch 103: val_accuracy did not improve from 0.95000\n",
            "429/429 - 21s - loss: 0.2896 - accuracy: 0.9085 - val_loss: 0.2232 - val_accuracy: 0.9286 - 21s/epoch - 48ms/step\n",
            "Epoch 104/1000\n",
            "\n",
            "Epoch 104: val_accuracy did not improve from 0.95000\n",
            "429/429 - 20s - loss: 0.2851 - accuracy: 0.9108 - val_loss: 0.2270 - val_accuracy: 0.9304 - 20s/epoch - 47ms/step\n",
            "Epoch 105/1000\n",
            "\n",
            "Epoch 105: val_accuracy did not improve from 0.95000\n",
            "429/429 - 20s - loss: 0.2806 - accuracy: 0.9114 - val_loss: 0.2071 - val_accuracy: 0.9366 - 20s/epoch - 47ms/step\n",
            "Epoch 106/1000\n",
            "\n",
            "Epoch 106: val_accuracy did not improve from 0.95000\n",
            "429/429 - 21s - loss: 0.2854 - accuracy: 0.9104 - val_loss: 0.2321 - val_accuracy: 0.9270 - 21s/epoch - 48ms/step\n",
            "Epoch 107/1000\n",
            "\n",
            "Epoch 107: val_accuracy did not improve from 0.95000\n",
            "429/429 - 21s - loss: 0.2834 - accuracy: 0.9115 - val_loss: 0.2329 - val_accuracy: 0.9264 - 21s/epoch - 49ms/step\n",
            "Epoch 108/1000\n",
            "\n",
            "Epoch 108: val_accuracy did not improve from 0.95000\n",
            "429/429 - 21s - loss: 0.2854 - accuracy: 0.9109 - val_loss: 0.2357 - val_accuracy: 0.9240 - 21s/epoch - 50ms/step\n",
            "Epoch 109/1000\n",
            "\n",
            "Epoch 109: val_accuracy did not improve from 0.95000\n",
            "429/429 - 22s - loss: 0.2790 - accuracy: 0.9131 - val_loss: 0.2603 - val_accuracy: 0.9150 - 22s/epoch - 51ms/step\n",
            "Epoch 110/1000\n",
            "\n",
            "Epoch 110: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2844 - accuracy: 0.9109 - val_loss: 0.2285 - val_accuracy: 0.9310 - 19s/epoch - 44ms/step\n",
            "Epoch 111/1000\n",
            "\n",
            "Epoch 111: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2808 - accuracy: 0.9127 - val_loss: 0.2004 - val_accuracy: 0.9440 - 19s/epoch - 45ms/step\n",
            "Epoch 112/1000\n",
            "\n",
            "Epoch 112: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2809 - accuracy: 0.9118 - val_loss: 0.2395 - val_accuracy: 0.9280 - 19s/epoch - 43ms/step\n",
            "Epoch 113/1000\n",
            "\n",
            "Epoch 113: val_accuracy did not improve from 0.95000\n",
            "429/429 - 18s - loss: 0.2790 - accuracy: 0.9123 - val_loss: 0.2185 - val_accuracy: 0.9302 - 18s/epoch - 43ms/step\n",
            "Epoch 114/1000\n",
            "\n",
            "Epoch 114: val_accuracy did not improve from 0.95000\n",
            "429/429 - 18s - loss: 0.2824 - accuracy: 0.9114 - val_loss: 0.2119 - val_accuracy: 0.9348 - 18s/epoch - 42ms/step\n",
            "Epoch 115/1000\n",
            "\n",
            "Epoch 115: val_accuracy did not improve from 0.95000\n",
            "429/429 - 18s - loss: 0.2778 - accuracy: 0.9129 - val_loss: 0.2346 - val_accuracy: 0.9278 - 18s/epoch - 42ms/step\n",
            "Epoch 116/1000\n",
            "\n",
            "Epoch 116: val_accuracy did not improve from 0.95000\n",
            "429/429 - 18s - loss: 0.2777 - accuracy: 0.9133 - val_loss: 0.2065 - val_accuracy: 0.9360 - 18s/epoch - 43ms/step\n",
            "Epoch 117/1000\n",
            "\n",
            "Epoch 117: val_accuracy did not improve from 0.95000\n",
            "429/429 - 18s - loss: 0.2798 - accuracy: 0.9126 - val_loss: 0.2149 - val_accuracy: 0.9340 - 18s/epoch - 42ms/step\n",
            "Epoch 118/1000\n",
            "\n",
            "Epoch 118: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2838 - accuracy: 0.9114 - val_loss: 0.2099 - val_accuracy: 0.9384 - 19s/epoch - 44ms/step\n",
            "Epoch 119/1000\n",
            "\n",
            "Epoch 119: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2857 - accuracy: 0.9103 - val_loss: 0.2261 - val_accuracy: 0.9312 - 19s/epoch - 45ms/step\n",
            "Epoch 120/1000\n",
            "\n",
            "Epoch 120: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2824 - accuracy: 0.9110 - val_loss: 0.2039 - val_accuracy: 0.9404 - 19s/epoch - 44ms/step\n",
            "Epoch 121/1000\n",
            "\n",
            "Epoch 121: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2808 - accuracy: 0.9122 - val_loss: 0.2038 - val_accuracy: 0.9380 - 19s/epoch - 45ms/step\n",
            "Epoch 122/1000\n",
            "\n",
            "Epoch 122: val_accuracy did not improve from 0.95000\n",
            "429/429 - 18s - loss: 0.2830 - accuracy: 0.9113 - val_loss: 0.1993 - val_accuracy: 0.9360 - 18s/epoch - 43ms/step\n",
            "Epoch 123/1000\n",
            "\n",
            "Epoch 123: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2810 - accuracy: 0.9129 - val_loss: 0.2137 - val_accuracy: 0.9372 - 19s/epoch - 44ms/step\n",
            "Epoch 124/1000\n",
            "\n",
            "Epoch 124: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2758 - accuracy: 0.9135 - val_loss: 0.2197 - val_accuracy: 0.9334 - 19s/epoch - 44ms/step\n",
            "Epoch 125/1000\n",
            "\n",
            "Epoch 125: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2808 - accuracy: 0.9118 - val_loss: 0.2167 - val_accuracy: 0.9310 - 19s/epoch - 45ms/step\n",
            "Epoch 126/1000\n",
            "\n",
            "Epoch 126: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2820 - accuracy: 0.9125 - val_loss: 0.2137 - val_accuracy: 0.9318 - 19s/epoch - 44ms/step\n",
            "Epoch 127/1000\n",
            "\n",
            "Epoch 127: val_accuracy did not improve from 0.95000\n",
            "429/429 - 20s - loss: 0.2837 - accuracy: 0.9112 - val_loss: 0.2205 - val_accuracy: 0.9342 - 20s/epoch - 47ms/step\n",
            "Epoch 128/1000\n",
            "\n",
            "Epoch 128: val_accuracy did not improve from 0.95000\n",
            "429/429 - 20s - loss: 0.2814 - accuracy: 0.9119 - val_loss: 0.2050 - val_accuracy: 0.9410 - 20s/epoch - 46ms/step\n",
            "Epoch 129/1000\n",
            "\n",
            "Epoch 129: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2808 - accuracy: 0.9121 - val_loss: 0.1986 - val_accuracy: 0.9404 - 19s/epoch - 45ms/step\n",
            "Epoch 130/1000\n",
            "\n",
            "Epoch 130: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2774 - accuracy: 0.9131 - val_loss: 0.2024 - val_accuracy: 0.9454 - 19s/epoch - 45ms/step\n",
            "Epoch 131/1000\n",
            "\n",
            "Epoch 131: val_accuracy did not improve from 0.95000\n",
            "429/429 - 20s - loss: 0.2806 - accuracy: 0.9129 - val_loss: 0.2434 - val_accuracy: 0.9266 - 20s/epoch - 46ms/step\n",
            "Epoch 132/1000\n",
            "\n",
            "Epoch 132: val_accuracy did not improve from 0.95000\n",
            "429/429 - 20s - loss: 0.2775 - accuracy: 0.9137 - val_loss: 0.1919 - val_accuracy: 0.9424 - 20s/epoch - 47ms/step\n",
            "Epoch 133/1000\n",
            "\n",
            "Epoch 133: val_accuracy did not improve from 0.95000\n",
            "429/429 - 20s - loss: 0.2795 - accuracy: 0.9125 - val_loss: 0.2182 - val_accuracy: 0.9350 - 20s/epoch - 47ms/step\n",
            "Epoch 134/1000\n",
            "\n",
            "Epoch 134: val_accuracy did not improve from 0.95000\n",
            "429/429 - 20s - loss: 0.2803 - accuracy: 0.9133 - val_loss: 0.2172 - val_accuracy: 0.9284 - 20s/epoch - 47ms/step\n",
            "Epoch 135/1000\n",
            "\n",
            "Epoch 135: val_accuracy did not improve from 0.95000\n",
            "429/429 - 20s - loss: 0.2784 - accuracy: 0.9121 - val_loss: 0.2126 - val_accuracy: 0.9370 - 20s/epoch - 48ms/step\n",
            "Epoch 136/1000\n",
            "\n",
            "Epoch 136: val_accuracy did not improve from 0.95000\n",
            "429/429 - 21s - loss: 0.2845 - accuracy: 0.9119 - val_loss: 0.2238 - val_accuracy: 0.9350 - 21s/epoch - 48ms/step\n",
            "Epoch 137/1000\n",
            "\n",
            "Epoch 137: val_accuracy did not improve from 0.95000\n",
            "429/429 - 21s - loss: 0.2776 - accuracy: 0.9142 - val_loss: 0.1933 - val_accuracy: 0.9456 - 21s/epoch - 50ms/step\n",
            "Epoch 138/1000\n",
            "\n",
            "Epoch 138: val_accuracy did not improve from 0.95000\n",
            "429/429 - 20s - loss: 0.2770 - accuracy: 0.9130 - val_loss: 0.2251 - val_accuracy: 0.9330 - 20s/epoch - 47ms/step\n",
            "Epoch 139/1000\n",
            "\n",
            "Epoch 139: val_accuracy did not improve from 0.95000\n",
            "429/429 - 22s - loss: 0.2761 - accuracy: 0.9142 - val_loss: 0.2261 - val_accuracy: 0.9330 - 22s/epoch - 52ms/step\n",
            "Epoch 140/1000\n",
            "\n",
            "Epoch 140: val_accuracy did not improve from 0.95000\n",
            "429/429 - 20s - loss: 0.2807 - accuracy: 0.9133 - val_loss: 0.1891 - val_accuracy: 0.9406 - 20s/epoch - 47ms/step\n",
            "Epoch 141/1000\n",
            "\n",
            "Epoch 141: val_accuracy did not improve from 0.95000\n",
            "429/429 - 20s - loss: 0.2782 - accuracy: 0.9139 - val_loss: 0.2218 - val_accuracy: 0.9278 - 20s/epoch - 46ms/step\n",
            "Epoch 142/1000\n",
            "\n",
            "Epoch 142: val_accuracy did not improve from 0.95000\n",
            "429/429 - 20s - loss: 0.2789 - accuracy: 0.9128 - val_loss: 0.2478 - val_accuracy: 0.9234 - 20s/epoch - 46ms/step\n",
            "Epoch 143/1000\n",
            "\n",
            "Epoch 143: val_accuracy did not improve from 0.95000\n",
            "429/429 - 20s - loss: 0.2773 - accuracy: 0.9129 - val_loss: 0.1943 - val_accuracy: 0.9474 - 20s/epoch - 47ms/step\n",
            "Epoch 144/1000\n",
            "\n",
            "Epoch 144: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2821 - accuracy: 0.9120 - val_loss: 0.2583 - val_accuracy: 0.9194 - 19s/epoch - 44ms/step\n",
            "Epoch 145/1000\n",
            "\n",
            "Epoch 145: val_accuracy did not improve from 0.95000\n",
            "429/429 - 18s - loss: 0.2793 - accuracy: 0.9123 - val_loss: 0.2052 - val_accuracy: 0.9392 - 18s/epoch - 43ms/step\n",
            "Epoch 146/1000\n",
            "\n",
            "Epoch 146: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2754 - accuracy: 0.9144 - val_loss: 0.2213 - val_accuracy: 0.9386 - 19s/epoch - 43ms/step\n",
            "Epoch 147/1000\n",
            "\n",
            "Epoch 147: val_accuracy did not improve from 0.95000\n",
            "429/429 - 18s - loss: 0.2792 - accuracy: 0.9108 - val_loss: 0.1844 - val_accuracy: 0.9500 - 18s/epoch - 43ms/step\n",
            "Epoch 148/1000\n",
            "\n",
            "Epoch 148: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2745 - accuracy: 0.9155 - val_loss: 0.1932 - val_accuracy: 0.9422 - 19s/epoch - 44ms/step\n",
            "Epoch 149/1000\n",
            "\n",
            "Epoch 149: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2788 - accuracy: 0.9120 - val_loss: 0.2386 - val_accuracy: 0.9260 - 19s/epoch - 45ms/step\n",
            "Epoch 150/1000\n",
            "\n",
            "Epoch 150: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2766 - accuracy: 0.9134 - val_loss: 0.1913 - val_accuracy: 0.9444 - 19s/epoch - 44ms/step\n",
            "Epoch 151/1000\n",
            "\n",
            "Epoch 151: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2762 - accuracy: 0.9138 - val_loss: 0.2204 - val_accuracy: 0.9306 - 19s/epoch - 44ms/step\n",
            "Epoch 152/1000\n",
            "\n",
            "Epoch 152: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2766 - accuracy: 0.9127 - val_loss: 0.1875 - val_accuracy: 0.9462 - 19s/epoch - 45ms/step\n",
            "Epoch 153/1000\n",
            "\n",
            "Epoch 153: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2782 - accuracy: 0.9120 - val_loss: 0.2039 - val_accuracy: 0.9412 - 19s/epoch - 45ms/step\n",
            "Epoch 154/1000\n",
            "\n",
            "Epoch 154: val_accuracy did not improve from 0.95000\n",
            "429/429 - 21s - loss: 0.2791 - accuracy: 0.9125 - val_loss: 0.2318 - val_accuracy: 0.9272 - 21s/epoch - 50ms/step\n",
            "Epoch 155/1000\n",
            "\n",
            "Epoch 155: val_accuracy did not improve from 0.95000\n",
            "429/429 - 20s - loss: 0.2774 - accuracy: 0.9145 - val_loss: 0.2546 - val_accuracy: 0.9250 - 20s/epoch - 46ms/step\n",
            "Epoch 156/1000\n",
            "\n",
            "Epoch 156: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2789 - accuracy: 0.9137 - val_loss: 0.2378 - val_accuracy: 0.9304 - 19s/epoch - 43ms/step\n",
            "Epoch 157/1000\n",
            "\n",
            "Epoch 157: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2756 - accuracy: 0.9149 - val_loss: 0.1949 - val_accuracy: 0.9430 - 19s/epoch - 44ms/step\n",
            "Epoch 158/1000\n",
            "\n",
            "Epoch 158: val_accuracy did not improve from 0.95000\n",
            "429/429 - 18s - loss: 0.2731 - accuracy: 0.9153 - val_loss: 0.2130 - val_accuracy: 0.9360 - 18s/epoch - 43ms/step\n",
            "Epoch 159/1000\n",
            "\n",
            "Epoch 159: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2761 - accuracy: 0.9142 - val_loss: 0.2226 - val_accuracy: 0.9280 - 19s/epoch - 43ms/step\n",
            "Epoch 160/1000\n",
            "\n",
            "Epoch 160: val_accuracy did not improve from 0.95000\n",
            "429/429 - 18s - loss: 0.2736 - accuracy: 0.9134 - val_loss: 0.2186 - val_accuracy: 0.9326 - 18s/epoch - 43ms/step\n",
            "Epoch 161/1000\n",
            "\n",
            "Epoch 161: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2762 - accuracy: 0.9147 - val_loss: 0.1995 - val_accuracy: 0.9402 - 19s/epoch - 43ms/step\n",
            "Epoch 162/1000\n",
            "\n",
            "Epoch 162: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2785 - accuracy: 0.9125 - val_loss: 0.1940 - val_accuracy: 0.9462 - 19s/epoch - 45ms/step\n",
            "Epoch 163/1000\n",
            "\n",
            "Epoch 163: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2783 - accuracy: 0.9127 - val_loss: 0.2057 - val_accuracy: 0.9392 - 19s/epoch - 45ms/step\n",
            "Epoch 164/1000\n",
            "\n",
            "Epoch 164: val_accuracy did not improve from 0.95000\n",
            "429/429 - 20s - loss: 0.2741 - accuracy: 0.9147 - val_loss: 0.2119 - val_accuracy: 0.9382 - 20s/epoch - 46ms/step\n",
            "Epoch 165/1000\n",
            "\n",
            "Epoch 165: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2778 - accuracy: 0.9132 - val_loss: 0.2003 - val_accuracy: 0.9410 - 19s/epoch - 44ms/step\n",
            "Epoch 166/1000\n",
            "\n",
            "Epoch 166: val_accuracy did not improve from 0.95000\n",
            "429/429 - 20s - loss: 0.2755 - accuracy: 0.9147 - val_loss: 0.1895 - val_accuracy: 0.9458 - 20s/epoch - 46ms/step\n",
            "Epoch 167/1000\n",
            "\n",
            "Epoch 167: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2744 - accuracy: 0.9146 - val_loss: 0.2231 - val_accuracy: 0.9300 - 19s/epoch - 45ms/step\n",
            "Epoch 168/1000\n",
            "\n",
            "Epoch 168: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2757 - accuracy: 0.9130 - val_loss: 0.2119 - val_accuracy: 0.9384 - 19s/epoch - 44ms/step\n",
            "Epoch 169/1000\n",
            "\n",
            "Epoch 169: val_accuracy did not improve from 0.95000\n",
            "429/429 - 17s - loss: 0.2821 - accuracy: 0.9119 - val_loss: 0.2319 - val_accuracy: 0.9284 - 17s/epoch - 40ms/step\n",
            "Epoch 170/1000\n",
            "\n",
            "Epoch 170: val_accuracy did not improve from 0.95000\n",
            "429/429 - 17s - loss: 0.2757 - accuracy: 0.9139 - val_loss: 0.2554 - val_accuracy: 0.9286 - 17s/epoch - 39ms/step\n",
            "Epoch 171/1000\n",
            "\n",
            "Epoch 171: val_accuracy did not improve from 0.95000\n",
            "429/429 - 20s - loss: 0.2724 - accuracy: 0.9135 - val_loss: 0.2042 - val_accuracy: 0.9406 - 20s/epoch - 46ms/step\n",
            "Epoch 172/1000\n",
            "\n",
            "Epoch 172: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2756 - accuracy: 0.9148 - val_loss: 0.2383 - val_accuracy: 0.9256 - 19s/epoch - 44ms/step\n",
            "Epoch 173/1000\n",
            "\n",
            "Epoch 173: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2732 - accuracy: 0.9148 - val_loss: 0.1957 - val_accuracy: 0.9406 - 19s/epoch - 45ms/step\n",
            "Epoch 174/1000\n",
            "\n",
            "Epoch 174: val_accuracy did not improve from 0.95000\n",
            "429/429 - 20s - loss: 0.2747 - accuracy: 0.9135 - val_loss: 0.1937 - val_accuracy: 0.9442 - 20s/epoch - 46ms/step\n",
            "Epoch 175/1000\n",
            "\n",
            "Epoch 175: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2750 - accuracy: 0.9146 - val_loss: 0.2277 - val_accuracy: 0.9288 - 19s/epoch - 45ms/step\n",
            "Epoch 176/1000\n",
            "\n",
            "Epoch 176: val_accuracy did not improve from 0.95000\n",
            "429/429 - 20s - loss: 0.2733 - accuracy: 0.9149 - val_loss: 0.1959 - val_accuracy: 0.9434 - 20s/epoch - 46ms/step\n",
            "Epoch 177/1000\n",
            "\n",
            "Epoch 177: val_accuracy did not improve from 0.95000\n",
            "429/429 - 20s - loss: 0.2768 - accuracy: 0.9120 - val_loss: 0.2134 - val_accuracy: 0.9352 - 20s/epoch - 47ms/step\n",
            "Epoch 178/1000\n",
            "\n",
            "Epoch 178: val_accuracy did not improve from 0.95000\n",
            "429/429 - 20s - loss: 0.2754 - accuracy: 0.9140 - val_loss: 0.2301 - val_accuracy: 0.9348 - 20s/epoch - 47ms/step\n",
            "Epoch 179/1000\n",
            "\n",
            "Epoch 179: val_accuracy did not improve from 0.95000\n",
            "429/429 - 21s - loss: 0.2724 - accuracy: 0.9153 - val_loss: 0.1997 - val_accuracy: 0.9410 - 21s/epoch - 48ms/step\n",
            "Epoch 180/1000\n",
            "\n",
            "Epoch 180: val_accuracy did not improve from 0.95000\n",
            "429/429 - 20s - loss: 0.2743 - accuracy: 0.9133 - val_loss: 0.2239 - val_accuracy: 0.9274 - 20s/epoch - 47ms/step\n",
            "Epoch 181/1000\n",
            "\n",
            "Epoch 181: val_accuracy did not improve from 0.95000\n",
            "429/429 - 23s - loss: 0.2716 - accuracy: 0.9150 - val_loss: 0.2239 - val_accuracy: 0.9318 - 23s/epoch - 53ms/step\n",
            "Epoch 182/1000\n",
            "\n",
            "Epoch 182: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2736 - accuracy: 0.9144 - val_loss: 0.2095 - val_accuracy: 0.9434 - 19s/epoch - 44ms/step\n",
            "Epoch 183/1000\n",
            "\n",
            "Epoch 183: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2748 - accuracy: 0.9149 - val_loss: 0.2123 - val_accuracy: 0.9360 - 19s/epoch - 43ms/step\n",
            "Epoch 184/1000\n",
            "\n",
            "Epoch 184: val_accuracy did not improve from 0.95000\n",
            "429/429 - 18s - loss: 0.2748 - accuracy: 0.9147 - val_loss: 0.2021 - val_accuracy: 0.9398 - 18s/epoch - 43ms/step\n",
            "Epoch 185/1000\n",
            "\n",
            "Epoch 185: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2744 - accuracy: 0.9141 - val_loss: 0.2113 - val_accuracy: 0.9368 - 19s/epoch - 43ms/step\n",
            "Epoch 186/1000\n",
            "\n",
            "Epoch 186: val_accuracy did not improve from 0.95000\n",
            "429/429 - 18s - loss: 0.2729 - accuracy: 0.9131 - val_loss: 0.2269 - val_accuracy: 0.9292 - 18s/epoch - 42ms/step\n",
            "Epoch 187/1000\n",
            "\n",
            "Epoch 187: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2754 - accuracy: 0.9141 - val_loss: 0.2521 - val_accuracy: 0.9198 - 19s/epoch - 43ms/step\n",
            "Epoch 188/1000\n",
            "\n",
            "Epoch 188: val_accuracy did not improve from 0.95000\n",
            "429/429 - 18s - loss: 0.2710 - accuracy: 0.9139 - val_loss: 0.2404 - val_accuracy: 0.9292 - 18s/epoch - 42ms/step\n",
            "Epoch 189/1000\n",
            "\n",
            "Epoch 189: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2780 - accuracy: 0.9127 - val_loss: 0.1889 - val_accuracy: 0.9498 - 19s/epoch - 44ms/step\n",
            "Epoch 190/1000\n",
            "\n",
            "Epoch 190: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2723 - accuracy: 0.9151 - val_loss: 0.2308 - val_accuracy: 0.9320 - 19s/epoch - 45ms/step\n",
            "Epoch 191/1000\n",
            "\n",
            "Epoch 191: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2753 - accuracy: 0.9141 - val_loss: 0.2140 - val_accuracy: 0.9396 - 19s/epoch - 45ms/step\n",
            "Epoch 192/1000\n",
            "\n",
            "Epoch 192: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2737 - accuracy: 0.9164 - val_loss: 0.1863 - val_accuracy: 0.9462 - 19s/epoch - 44ms/step\n",
            "Epoch 193/1000\n",
            "\n",
            "Epoch 193: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2735 - accuracy: 0.9154 - val_loss: 0.2136 - val_accuracy: 0.9354 - 19s/epoch - 45ms/step\n",
            "Epoch 194/1000\n",
            "\n",
            "Epoch 194: val_accuracy did not improve from 0.95000\n",
            "429/429 - 21s - loss: 0.2726 - accuracy: 0.9154 - val_loss: 0.2079 - val_accuracy: 0.9378 - 21s/epoch - 48ms/step\n",
            "Epoch 195/1000\n",
            "\n",
            "Epoch 195: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2736 - accuracy: 0.9162 - val_loss: 0.2275 - val_accuracy: 0.9286 - 19s/epoch - 45ms/step\n",
            "Epoch 196/1000\n",
            "\n",
            "Epoch 196: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2738 - accuracy: 0.9136 - val_loss: 0.2023 - val_accuracy: 0.9434 - 19s/epoch - 45ms/step\n",
            "Epoch 197/1000\n",
            "\n",
            "Epoch 197: val_accuracy did not improve from 0.95000\n",
            "429/429 - 18s - loss: 0.2750 - accuracy: 0.9134 - val_loss: 0.2122 - val_accuracy: 0.9356 - 18s/epoch - 43ms/step\n",
            "Epoch 198/1000\n",
            "\n",
            "Epoch 198: val_accuracy did not improve from 0.95000\n",
            "429/429 - 18s - loss: 0.2712 - accuracy: 0.9154 - val_loss: 0.2160 - val_accuracy: 0.9360 - 18s/epoch - 42ms/step\n",
            "Epoch 199/1000\n",
            "\n",
            "Epoch 199: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2704 - accuracy: 0.9164 - val_loss: 0.2153 - val_accuracy: 0.9362 - 19s/epoch - 45ms/step\n",
            "Epoch 200/1000\n",
            "\n",
            "Epoch 200: val_accuracy did not improve from 0.95000\n",
            "429/429 - 18s - loss: 0.2736 - accuracy: 0.9140 - val_loss: 0.1980 - val_accuracy: 0.9408 - 18s/epoch - 42ms/step\n",
            "Epoch 201/1000\n",
            "\n",
            "Epoch 201: val_accuracy did not improve from 0.95000\n",
            "429/429 - 18s - loss: 0.2752 - accuracy: 0.9137 - val_loss: 0.2103 - val_accuracy: 0.9388 - 18s/epoch - 42ms/step\n",
            "Epoch 202/1000\n",
            "\n",
            "Epoch 202: val_accuracy did not improve from 0.95000\n",
            "429/429 - 18s - loss: 0.2755 - accuracy: 0.9152 - val_loss: 0.1868 - val_accuracy: 0.9476 - 18s/epoch - 42ms/step\n",
            "Epoch 203/1000\n",
            "\n",
            "Epoch 203: val_accuracy did not improve from 0.95000\n",
            "429/429 - 18s - loss: 0.2742 - accuracy: 0.9141 - val_loss: 0.2123 - val_accuracy: 0.9362 - 18s/epoch - 43ms/step\n",
            "Epoch 204/1000\n",
            "\n",
            "Epoch 204: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2692 - accuracy: 0.9162 - val_loss: 0.2087 - val_accuracy: 0.9418 - 19s/epoch - 43ms/step\n",
            "Epoch 205/1000\n",
            "\n",
            "Epoch 205: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2694 - accuracy: 0.9156 - val_loss: 0.2186 - val_accuracy: 0.9368 - 19s/epoch - 43ms/step\n",
            "Epoch 206/1000\n",
            "\n",
            "Epoch 206: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2708 - accuracy: 0.9160 - val_loss: 0.2180 - val_accuracy: 0.9338 - 19s/epoch - 43ms/step\n",
            "Epoch 207/1000\n",
            "\n",
            "Epoch 207: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2742 - accuracy: 0.9149 - val_loss: 0.2060 - val_accuracy: 0.9404 - 19s/epoch - 45ms/step\n",
            "Epoch 208/1000\n",
            "\n",
            "Epoch 208: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2680 - accuracy: 0.9161 - val_loss: 0.1831 - val_accuracy: 0.9498 - 19s/epoch - 45ms/step\n",
            "Epoch 209/1000\n",
            "\n",
            "Epoch 209: val_accuracy did not improve from 0.95000\n",
            "429/429 - 20s - loss: 0.2680 - accuracy: 0.9169 - val_loss: 0.2129 - val_accuracy: 0.9372 - 20s/epoch - 47ms/step\n",
            "Epoch 210/1000\n",
            "\n",
            "Epoch 210: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2704 - accuracy: 0.9151 - val_loss: 0.2266 - val_accuracy: 0.9304 - 19s/epoch - 44ms/step\n",
            "Epoch 211/1000\n",
            "\n",
            "Epoch 211: val_accuracy did not improve from 0.95000\n",
            "429/429 - 21s - loss: 0.2697 - accuracy: 0.9157 - val_loss: 0.2132 - val_accuracy: 0.9336 - 21s/epoch - 49ms/step\n",
            "Epoch 212/1000\n",
            "\n",
            "Epoch 212: val_accuracy did not improve from 0.95000\n",
            "429/429 - 20s - loss: 0.2689 - accuracy: 0.9177 - val_loss: 0.2239 - val_accuracy: 0.9332 - 20s/epoch - 47ms/step\n",
            "Epoch 213/1000\n",
            "\n",
            "Epoch 213: val_accuracy did not improve from 0.95000\n",
            "429/429 - 19s - loss: 0.2691 - accuracy: 0.9156 - val_loss: 0.2131 - val_accuracy: 0.9368 - 19s/epoch - 45ms/step\n",
            "Epoch 214/1000\n",
            "\n",
            "Epoch 214: val_accuracy did not improve from 0.95000\n",
            "429/429 - 17s - loss: 0.2735 - accuracy: 0.9147 - val_loss: 0.2259 - val_accuracy: 0.9312 - 17s/epoch - 39ms/step\n",
            "Epoch 215/1000\n",
            "\n",
            "Epoch 215: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2733 - accuracy: 0.9145 - val_loss: 0.1866 - val_accuracy: 0.9456 - 16s/epoch - 38ms/step\n",
            "Epoch 216/1000\n",
            "\n",
            "Epoch 216: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2696 - accuracy: 0.9162 - val_loss: 0.2173 - val_accuracy: 0.9356 - 16s/epoch - 38ms/step\n",
            "Epoch 217/1000\n",
            "\n",
            "Epoch 217: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2676 - accuracy: 0.9161 - val_loss: 0.1975 - val_accuracy: 0.9404 - 16s/epoch - 38ms/step\n",
            "Epoch 218/1000\n",
            "\n",
            "Epoch 218: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2684 - accuracy: 0.9160 - val_loss: 0.1932 - val_accuracy: 0.9404 - 16s/epoch - 38ms/step\n",
            "Epoch 219/1000\n",
            "\n",
            "Epoch 219: val_accuracy did not improve from 0.95000\n",
            "429/429 - 17s - loss: 0.2728 - accuracy: 0.9158 - val_loss: 0.2049 - val_accuracy: 0.9404 - 17s/epoch - 39ms/step\n",
            "Epoch 220/1000\n",
            "\n",
            "Epoch 220: val_accuracy did not improve from 0.95000\n",
            "429/429 - 17s - loss: 0.2711 - accuracy: 0.9161 - val_loss: 0.2017 - val_accuracy: 0.9406 - 17s/epoch - 39ms/step\n",
            "Epoch 221/1000\n",
            "\n",
            "Epoch 221: val_accuracy did not improve from 0.95000\n",
            "429/429 - 17s - loss: 0.2713 - accuracy: 0.9148 - val_loss: 0.2149 - val_accuracy: 0.9346 - 17s/epoch - 40ms/step\n",
            "Epoch 222/1000\n",
            "\n",
            "Epoch 222: val_accuracy did not improve from 0.95000\n",
            "429/429 - 17s - loss: 0.2721 - accuracy: 0.9148 - val_loss: 0.1992 - val_accuracy: 0.9464 - 17s/epoch - 40ms/step\n",
            "Epoch 223/1000\n",
            "\n",
            "Epoch 223: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2706 - accuracy: 0.9162 - val_loss: 0.2080 - val_accuracy: 0.9370 - 16s/epoch - 38ms/step\n",
            "Epoch 224/1000\n",
            "\n",
            "Epoch 224: val_accuracy did not improve from 0.95000\n",
            "429/429 - 17s - loss: 0.2671 - accuracy: 0.9157 - val_loss: 0.1917 - val_accuracy: 0.9470 - 17s/epoch - 39ms/step\n",
            "Epoch 225/1000\n",
            "\n",
            "Epoch 225: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2679 - accuracy: 0.9170 - val_loss: 0.2180 - val_accuracy: 0.9364 - 16s/epoch - 37ms/step\n",
            "Epoch 226/1000\n",
            "\n",
            "Epoch 226: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2709 - accuracy: 0.9158 - val_loss: 0.2099 - val_accuracy: 0.9378 - 16s/epoch - 38ms/step\n",
            "Epoch 227/1000\n",
            "\n",
            "Epoch 227: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2704 - accuracy: 0.9155 - val_loss: 0.1932 - val_accuracy: 0.9446 - 16s/epoch - 38ms/step\n",
            "Epoch 228/1000\n",
            "\n",
            "Epoch 228: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2690 - accuracy: 0.9156 - val_loss: 0.2068 - val_accuracy: 0.9412 - 16s/epoch - 37ms/step\n",
            "Epoch 229/1000\n",
            "\n",
            "Epoch 229: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2709 - accuracy: 0.9157 - val_loss: 0.2082 - val_accuracy: 0.9372 - 16s/epoch - 37ms/step\n",
            "Epoch 230/1000\n",
            "\n",
            "Epoch 230: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2714 - accuracy: 0.9165 - val_loss: 0.2113 - val_accuracy: 0.9380 - 16s/epoch - 37ms/step\n",
            "Epoch 231/1000\n",
            "\n",
            "Epoch 231: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2685 - accuracy: 0.9175 - val_loss: 0.2010 - val_accuracy: 0.9420 - 16s/epoch - 37ms/step\n",
            "Epoch 232/1000\n",
            "\n",
            "Epoch 232: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2720 - accuracy: 0.9150 - val_loss: 0.2428 - val_accuracy: 0.9318 - 16s/epoch - 37ms/step\n",
            "Epoch 233/1000\n",
            "\n",
            "Epoch 233: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2681 - accuracy: 0.9178 - val_loss: 0.2024 - val_accuracy: 0.9438 - 16s/epoch - 38ms/step\n",
            "Epoch 234/1000\n",
            "\n",
            "Epoch 234: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2704 - accuracy: 0.9156 - val_loss: 0.2035 - val_accuracy: 0.9382 - 16s/epoch - 38ms/step\n",
            "Epoch 235/1000\n",
            "\n",
            "Epoch 235: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2702 - accuracy: 0.9168 - val_loss: 0.2279 - val_accuracy: 0.9308 - 16s/epoch - 38ms/step\n",
            "Epoch 236/1000\n",
            "\n",
            "Epoch 236: val_accuracy did not improve from 0.95000\n",
            "429/429 - 17s - loss: 0.2674 - accuracy: 0.9170 - val_loss: 0.2246 - val_accuracy: 0.9340 - 17s/epoch - 41ms/step\n",
            "Epoch 237/1000\n",
            "\n",
            "Epoch 237: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2700 - accuracy: 0.9165 - val_loss: 0.2332 - val_accuracy: 0.9278 - 16s/epoch - 38ms/step\n",
            "Epoch 238/1000\n",
            "\n",
            "Epoch 238: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2706 - accuracy: 0.9169 - val_loss: 0.1941 - val_accuracy: 0.9430 - 16s/epoch - 37ms/step\n",
            "Epoch 239/1000\n",
            "\n",
            "Epoch 239: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2670 - accuracy: 0.9169 - val_loss: 0.1901 - val_accuracy: 0.9492 - 16s/epoch - 38ms/step\n",
            "Epoch 240/1000\n",
            "\n",
            "Epoch 240: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2676 - accuracy: 0.9172 - val_loss: 0.1957 - val_accuracy: 0.9424 - 16s/epoch - 37ms/step\n",
            "Epoch 241/1000\n",
            "\n",
            "Epoch 241: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2732 - accuracy: 0.9156 - val_loss: 0.1799 - val_accuracy: 0.9492 - 16s/epoch - 37ms/step\n",
            "Epoch 242/1000\n",
            "\n",
            "Epoch 242: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2671 - accuracy: 0.9191 - val_loss: 0.1844 - val_accuracy: 0.9446 - 16s/epoch - 37ms/step\n",
            "Epoch 243/1000\n",
            "\n",
            "Epoch 243: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2703 - accuracy: 0.9159 - val_loss: 0.2320 - val_accuracy: 0.9356 - 16s/epoch - 37ms/step\n",
            "Epoch 244/1000\n",
            "\n",
            "Epoch 244: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2680 - accuracy: 0.9178 - val_loss: 0.2051 - val_accuracy: 0.9404 - 16s/epoch - 37ms/step\n",
            "Epoch 245/1000\n",
            "\n",
            "Epoch 245: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2645 - accuracy: 0.9182 - val_loss: 0.1869 - val_accuracy: 0.9494 - 16s/epoch - 37ms/step\n",
            "Epoch 246/1000\n",
            "\n",
            "Epoch 246: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2709 - accuracy: 0.9165 - val_loss: 0.2049 - val_accuracy: 0.9396 - 16s/epoch - 38ms/step\n",
            "Epoch 247/1000\n",
            "\n",
            "Epoch 247: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2726 - accuracy: 0.9152 - val_loss: 0.2142 - val_accuracy: 0.9346 - 16s/epoch - 38ms/step\n",
            "Epoch 248/1000\n",
            "\n",
            "Epoch 248: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2689 - accuracy: 0.9156 - val_loss: 0.2228 - val_accuracy: 0.9312 - 16s/epoch - 38ms/step\n",
            "Epoch 249/1000\n",
            "\n",
            "Epoch 249: val_accuracy did not improve from 0.95000\n",
            "429/429 - 17s - loss: 0.2666 - accuracy: 0.9172 - val_loss: 0.2054 - val_accuracy: 0.9418 - 17s/epoch - 39ms/step\n",
            "Epoch 250/1000\n",
            "\n",
            "Epoch 250: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2732 - accuracy: 0.9151 - val_loss: 0.2332 - val_accuracy: 0.9258 - 16s/epoch - 38ms/step\n",
            "Epoch 251/1000\n",
            "\n",
            "Epoch 251: val_accuracy did not improve from 0.95000\n",
            "429/429 - 17s - loss: 0.2686 - accuracy: 0.9165 - val_loss: 0.1955 - val_accuracy: 0.9402 - 17s/epoch - 39ms/step\n",
            "Epoch 252/1000\n",
            "\n",
            "Epoch 252: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2640 - accuracy: 0.9181 - val_loss: 0.1931 - val_accuracy: 0.9424 - 16s/epoch - 38ms/step\n",
            "Epoch 253/1000\n",
            "\n",
            "Epoch 253: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2698 - accuracy: 0.9164 - val_loss: 0.1924 - val_accuracy: 0.9428 - 16s/epoch - 37ms/step\n",
            "Epoch 254/1000\n",
            "\n",
            "Epoch 254: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2664 - accuracy: 0.9180 - val_loss: 0.1837 - val_accuracy: 0.9464 - 16s/epoch - 38ms/step\n",
            "Epoch 255/1000\n",
            "\n",
            "Epoch 255: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2693 - accuracy: 0.9168 - val_loss: 0.1913 - val_accuracy: 0.9450 - 16s/epoch - 37ms/step\n",
            "Epoch 256/1000\n",
            "\n",
            "Epoch 256: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2689 - accuracy: 0.9180 - val_loss: 0.1977 - val_accuracy: 0.9408 - 16s/epoch - 37ms/step\n",
            "Epoch 257/1000\n",
            "\n",
            "Epoch 257: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2694 - accuracy: 0.9164 - val_loss: 0.2112 - val_accuracy: 0.9398 - 16s/epoch - 37ms/step\n",
            "Epoch 258/1000\n",
            "\n",
            "Epoch 258: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2696 - accuracy: 0.9165 - val_loss: 0.2066 - val_accuracy: 0.9422 - 16s/epoch - 37ms/step\n",
            "Epoch 259/1000\n",
            "\n",
            "Epoch 259: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2653 - accuracy: 0.9166 - val_loss: 0.2017 - val_accuracy: 0.9398 - 16s/epoch - 37ms/step\n",
            "Epoch 260/1000\n",
            "\n",
            "Epoch 260: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2662 - accuracy: 0.9164 - val_loss: 0.2007 - val_accuracy: 0.9418 - 16s/epoch - 37ms/step\n",
            "Epoch 261/1000\n",
            "\n",
            "Epoch 261: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2712 - accuracy: 0.9159 - val_loss: 0.2083 - val_accuracy: 0.9386 - 16s/epoch - 38ms/step\n",
            "Epoch 262/1000\n",
            "\n",
            "Epoch 262: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2674 - accuracy: 0.9162 - val_loss: 0.2028 - val_accuracy: 0.9418 - 16s/epoch - 38ms/step\n",
            "Epoch 263/1000\n",
            "\n",
            "Epoch 263: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2676 - accuracy: 0.9168 - val_loss: 0.2070 - val_accuracy: 0.9374 - 16s/epoch - 38ms/step\n",
            "Epoch 264/1000\n",
            "\n",
            "Epoch 264: val_accuracy did not improve from 0.95000\n",
            "429/429 - 17s - loss: 0.2715 - accuracy: 0.9154 - val_loss: 0.1977 - val_accuracy: 0.9400 - 17s/epoch - 40ms/step\n",
            "Epoch 265/1000\n",
            "\n",
            "Epoch 265: val_accuracy did not improve from 0.95000\n",
            "429/429 - 17s - loss: 0.2696 - accuracy: 0.9164 - val_loss: 0.1999 - val_accuracy: 0.9444 - 17s/epoch - 39ms/step\n",
            "Epoch 266/1000\n",
            "\n",
            "Epoch 266: val_accuracy did not improve from 0.95000\n",
            "429/429 - 17s - loss: 0.2690 - accuracy: 0.9163 - val_loss: 0.2174 - val_accuracy: 0.9370 - 17s/epoch - 39ms/step\n",
            "Epoch 267/1000\n",
            "\n",
            "Epoch 267: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2703 - accuracy: 0.9167 - val_loss: 0.2007 - val_accuracy: 0.9426 - 16s/epoch - 37ms/step\n",
            "Epoch 268/1000\n",
            "\n",
            "Epoch 268: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2697 - accuracy: 0.9161 - val_loss: 0.2150 - val_accuracy: 0.9374 - 16s/epoch - 38ms/step\n",
            "Epoch 269/1000\n",
            "\n",
            "Epoch 269: val_accuracy did not improve from 0.95000\n",
            "429/429 - 17s - loss: 0.2704 - accuracy: 0.9159 - val_loss: 0.2193 - val_accuracy: 0.9364 - 17s/epoch - 39ms/step\n",
            "Epoch 270/1000\n",
            "\n",
            "Epoch 270: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2673 - accuracy: 0.9181 - val_loss: 0.2108 - val_accuracy: 0.9390 - 16s/epoch - 37ms/step\n",
            "Epoch 271/1000\n",
            "\n",
            "Epoch 271: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2710 - accuracy: 0.9158 - val_loss: 0.1902 - val_accuracy: 0.9470 - 16s/epoch - 37ms/step\n",
            "Epoch 272/1000\n",
            "\n",
            "Epoch 272: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2653 - accuracy: 0.9169 - val_loss: 0.2260 - val_accuracy: 0.9334 - 16s/epoch - 37ms/step\n",
            "Epoch 273/1000\n",
            "\n",
            "Epoch 273: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2667 - accuracy: 0.9162 - val_loss: 0.1852 - val_accuracy: 0.9494 - 16s/epoch - 38ms/step\n",
            "Epoch 274/1000\n",
            "\n",
            "Epoch 274: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2681 - accuracy: 0.9161 - val_loss: 0.2095 - val_accuracy: 0.9392 - 16s/epoch - 38ms/step\n",
            "Epoch 275/1000\n",
            "\n",
            "Epoch 275: val_accuracy did not improve from 0.95000\n",
            "429/429 - 17s - loss: 0.2673 - accuracy: 0.9175 - val_loss: 0.2453 - val_accuracy: 0.9248 - 17s/epoch - 39ms/step\n",
            "Epoch 276/1000\n",
            "\n",
            "Epoch 276: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2712 - accuracy: 0.9162 - val_loss: 0.2073 - val_accuracy: 0.9384 - 16s/epoch - 38ms/step\n",
            "Epoch 277/1000\n",
            "\n",
            "Epoch 277: val_accuracy did not improve from 0.95000\n",
            "429/429 - 17s - loss: 0.2666 - accuracy: 0.9172 - val_loss: 0.2320 - val_accuracy: 0.9306 - 17s/epoch - 39ms/step\n",
            "Epoch 278/1000\n",
            "\n",
            "Epoch 278: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2693 - accuracy: 0.9148 - val_loss: 0.1994 - val_accuracy: 0.9418 - 16s/epoch - 38ms/step\n",
            "Epoch 279/1000\n",
            "\n",
            "Epoch 279: val_accuracy did not improve from 0.95000\n",
            "429/429 - 17s - loss: 0.2637 - accuracy: 0.9181 - val_loss: 0.2268 - val_accuracy: 0.9294 - 17s/epoch - 39ms/step\n",
            "Epoch 280/1000\n",
            "\n",
            "Epoch 280: val_accuracy did not improve from 0.95000\n",
            "429/429 - 17s - loss: 0.2624 - accuracy: 0.9188 - val_loss: 0.2152 - val_accuracy: 0.9394 - 17s/epoch - 39ms/step\n",
            "Epoch 281/1000\n",
            "\n",
            "Epoch 281: val_accuracy did not improve from 0.95000\n",
            "429/429 - 16s - loss: 0.2690 - accuracy: 0.9162 - val_loss: 0.2442 - val_accuracy: 0.9306 - 16s/epoch - 37ms/step\n",
            "Epoch 282/1000\n",
            "\n",
            "Epoch 282: val_accuracy improved from 0.95000 to 0.95320, saving model to best_model,keras\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "429/429 - 18s - loss: 0.2681 - accuracy: 0.9169 - val_loss: 0.1855 - val_accuracy: 0.9532 - 18s/epoch - 42ms/step\n",
            "Epoch 283/1000\n",
            "\n",
            "Epoch 283: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2644 - accuracy: 0.9175 - val_loss: 0.2083 - val_accuracy: 0.9424 - 16s/epoch - 38ms/step\n",
            "Epoch 284/1000\n",
            "\n",
            "Epoch 284: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2699 - accuracy: 0.9155 - val_loss: 0.1930 - val_accuracy: 0.9482 - 16s/epoch - 37ms/step\n",
            "Epoch 285/1000\n",
            "\n",
            "Epoch 285: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2697 - accuracy: 0.9159 - val_loss: 0.2032 - val_accuracy: 0.9448 - 16s/epoch - 37ms/step\n",
            "Epoch 286/1000\n",
            "\n",
            "Epoch 286: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2680 - accuracy: 0.9176 - val_loss: 0.2483 - val_accuracy: 0.9228 - 16s/epoch - 38ms/step\n",
            "Epoch 287/1000\n",
            "\n",
            "Epoch 287: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2710 - accuracy: 0.9159 - val_loss: 0.1856 - val_accuracy: 0.9490 - 17s/epoch - 39ms/step\n",
            "Epoch 288/1000\n",
            "\n",
            "Epoch 288: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2682 - accuracy: 0.9167 - val_loss: 0.1868 - val_accuracy: 0.9476 - 16s/epoch - 37ms/step\n",
            "Epoch 289/1000\n",
            "\n",
            "Epoch 289: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2667 - accuracy: 0.9179 - val_loss: 0.2173 - val_accuracy: 0.9418 - 16s/epoch - 38ms/step\n",
            "Epoch 290/1000\n",
            "\n",
            "Epoch 290: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2663 - accuracy: 0.9174 - val_loss: 0.2264 - val_accuracy: 0.9278 - 16s/epoch - 38ms/step\n",
            "Epoch 291/1000\n",
            "\n",
            "Epoch 291: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2658 - accuracy: 0.9175 - val_loss: 0.2089 - val_accuracy: 0.9412 - 17s/epoch - 39ms/step\n",
            "Epoch 292/1000\n",
            "\n",
            "Epoch 292: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2683 - accuracy: 0.9167 - val_loss: 0.1954 - val_accuracy: 0.9448 - 17s/epoch - 39ms/step\n",
            "Epoch 293/1000\n",
            "\n",
            "Epoch 293: val_accuracy did not improve from 0.95320\n",
            "429/429 - 19s - loss: 0.2645 - accuracy: 0.9181 - val_loss: 0.1968 - val_accuracy: 0.9450 - 19s/epoch - 44ms/step\n",
            "Epoch 294/1000\n",
            "\n",
            "Epoch 294: val_accuracy did not improve from 0.95320\n",
            "429/429 - 19s - loss: 0.2666 - accuracy: 0.9165 - val_loss: 0.2780 - val_accuracy: 0.9166 - 19s/epoch - 45ms/step\n",
            "Epoch 295/1000\n",
            "\n",
            "Epoch 295: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2688 - accuracy: 0.9159 - val_loss: 0.1946 - val_accuracy: 0.9446 - 18s/epoch - 41ms/step\n",
            "Epoch 296/1000\n",
            "\n",
            "Epoch 296: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2695 - accuracy: 0.9162 - val_loss: 0.2014 - val_accuracy: 0.9456 - 18s/epoch - 42ms/step\n",
            "Epoch 297/1000\n",
            "\n",
            "Epoch 297: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2684 - accuracy: 0.9176 - val_loss: 0.1924 - val_accuracy: 0.9440 - 17s/epoch - 41ms/step\n",
            "Epoch 298/1000\n",
            "\n",
            "Epoch 298: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2701 - accuracy: 0.9159 - val_loss: 0.1986 - val_accuracy: 0.9466 - 17s/epoch - 40ms/step\n",
            "Epoch 299/1000\n",
            "\n",
            "Epoch 299: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2695 - accuracy: 0.9158 - val_loss: 0.2450 - val_accuracy: 0.9260 - 18s/epoch - 41ms/step\n",
            "Epoch 300/1000\n",
            "\n",
            "Epoch 300: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2634 - accuracy: 0.9183 - val_loss: 0.2028 - val_accuracy: 0.9372 - 17s/epoch - 41ms/step\n",
            "Epoch 301/1000\n",
            "\n",
            "Epoch 301: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2665 - accuracy: 0.9178 - val_loss: 0.2057 - val_accuracy: 0.9428 - 18s/epoch - 42ms/step\n",
            "Epoch 302/1000\n",
            "\n",
            "Epoch 302: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2648 - accuracy: 0.9177 - val_loss: 0.2061 - val_accuracy: 0.9426 - 18s/epoch - 41ms/step\n",
            "Epoch 303/1000\n",
            "\n",
            "Epoch 303: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2710 - accuracy: 0.9155 - val_loss: 0.2076 - val_accuracy: 0.9406 - 18s/epoch - 43ms/step\n",
            "Epoch 304/1000\n",
            "\n",
            "Epoch 304: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2661 - accuracy: 0.9166 - val_loss: 0.1959 - val_accuracy: 0.9426 - 18s/epoch - 41ms/step\n",
            "Epoch 305/1000\n",
            "\n",
            "Epoch 305: val_accuracy did not improve from 0.95320\n",
            "429/429 - 19s - loss: 0.2647 - accuracy: 0.9183 - val_loss: 0.2100 - val_accuracy: 0.9328 - 19s/epoch - 44ms/step\n",
            "Epoch 306/1000\n",
            "\n",
            "Epoch 306: val_accuracy did not improve from 0.95320\n",
            "429/429 - 20s - loss: 0.2657 - accuracy: 0.9177 - val_loss: 0.1980 - val_accuracy: 0.9424 - 20s/epoch - 47ms/step\n",
            "Epoch 307/1000\n",
            "\n",
            "Epoch 307: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2642 - accuracy: 0.9186 - val_loss: 0.2040 - val_accuracy: 0.9390 - 18s/epoch - 43ms/step\n",
            "Epoch 308/1000\n",
            "\n",
            "Epoch 308: val_accuracy did not improve from 0.95320\n",
            "429/429 - 19s - loss: 0.2677 - accuracy: 0.9166 - val_loss: 0.1956 - val_accuracy: 0.9460 - 19s/epoch - 43ms/step\n",
            "Epoch 309/1000\n",
            "\n",
            "Epoch 309: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2686 - accuracy: 0.9172 - val_loss: 0.2109 - val_accuracy: 0.9392 - 18s/epoch - 42ms/step\n",
            "Epoch 310/1000\n",
            "\n",
            "Epoch 310: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2615 - accuracy: 0.9192 - val_loss: 0.1992 - val_accuracy: 0.9442 - 17s/epoch - 40ms/step\n",
            "Epoch 311/1000\n",
            "\n",
            "Epoch 311: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2646 - accuracy: 0.9179 - val_loss: 0.1923 - val_accuracy: 0.9442 - 18s/epoch - 42ms/step\n",
            "Epoch 312/1000\n",
            "\n",
            "Epoch 312: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2640 - accuracy: 0.9188 - val_loss: 0.2225 - val_accuracy: 0.9352 - 17s/epoch - 40ms/step\n",
            "Epoch 313/1000\n",
            "\n",
            "Epoch 313: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2691 - accuracy: 0.9158 - val_loss: 0.2149 - val_accuracy: 0.9382 - 17s/epoch - 41ms/step\n",
            "Epoch 314/1000\n",
            "\n",
            "Epoch 314: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2664 - accuracy: 0.9173 - val_loss: 0.2097 - val_accuracy: 0.9374 - 18s/epoch - 41ms/step\n",
            "Epoch 315/1000\n",
            "\n",
            "Epoch 315: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2685 - accuracy: 0.9172 - val_loss: 0.2227 - val_accuracy: 0.9312 - 18s/epoch - 42ms/step\n",
            "Epoch 316/1000\n",
            "\n",
            "Epoch 316: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2662 - accuracy: 0.9178 - val_loss: 0.2000 - val_accuracy: 0.9382 - 18s/epoch - 43ms/step\n",
            "Epoch 317/1000\n",
            "\n",
            "Epoch 317: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2634 - accuracy: 0.9180 - val_loss: 0.2198 - val_accuracy: 0.9344 - 18s/epoch - 43ms/step\n",
            "Epoch 318/1000\n",
            "\n",
            "Epoch 318: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2659 - accuracy: 0.9177 - val_loss: 0.2104 - val_accuracy: 0.9362 - 18s/epoch - 42ms/step\n",
            "Epoch 319/1000\n",
            "\n",
            "Epoch 319: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2649 - accuracy: 0.9176 - val_loss: 0.2341 - val_accuracy: 0.9298 - 18s/epoch - 43ms/step\n",
            "Epoch 320/1000\n",
            "\n",
            "Epoch 320: val_accuracy did not improve from 0.95320\n",
            "429/429 - 19s - loss: 0.2669 - accuracy: 0.9163 - val_loss: 0.2176 - val_accuracy: 0.9348 - 19s/epoch - 44ms/step\n",
            "Epoch 321/1000\n",
            "\n",
            "Epoch 321: val_accuracy did not improve from 0.95320\n",
            "429/429 - 20s - loss: 0.2663 - accuracy: 0.9181 - val_loss: 0.1847 - val_accuracy: 0.9464 - 20s/epoch - 47ms/step\n",
            "Epoch 322/1000\n",
            "\n",
            "Epoch 322: val_accuracy did not improve from 0.95320\n",
            "429/429 - 19s - loss: 0.2676 - accuracy: 0.9172 - val_loss: 0.1968 - val_accuracy: 0.9446 - 19s/epoch - 43ms/step\n",
            "Epoch 323/1000\n",
            "\n",
            "Epoch 323: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2681 - accuracy: 0.9182 - val_loss: 0.2038 - val_accuracy: 0.9380 - 17s/epoch - 41ms/step\n",
            "Epoch 324/1000\n",
            "\n",
            "Epoch 324: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2675 - accuracy: 0.9164 - val_loss: 0.1977 - val_accuracy: 0.9450 - 16s/epoch - 38ms/step\n",
            "Epoch 325/1000\n",
            "\n",
            "Epoch 325: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2631 - accuracy: 0.9168 - val_loss: 0.1965 - val_accuracy: 0.9378 - 16s/epoch - 38ms/step\n",
            "Epoch 326/1000\n",
            "\n",
            "Epoch 326: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2611 - accuracy: 0.9185 - val_loss: 0.2200 - val_accuracy: 0.9368 - 16s/epoch - 38ms/step\n",
            "Epoch 327/1000\n",
            "\n",
            "Epoch 327: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2649 - accuracy: 0.9168 - val_loss: 0.1840 - val_accuracy: 0.9468 - 16s/epoch - 38ms/step\n",
            "Epoch 328/1000\n",
            "\n",
            "Epoch 328: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2624 - accuracy: 0.9184 - val_loss: 0.2019 - val_accuracy: 0.9428 - 16s/epoch - 38ms/step\n",
            "Epoch 329/1000\n",
            "\n",
            "Epoch 329: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2676 - accuracy: 0.9162 - val_loss: 0.1959 - val_accuracy: 0.9426 - 17s/epoch - 39ms/step\n",
            "Epoch 330/1000\n",
            "\n",
            "Epoch 330: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2612 - accuracy: 0.9189 - val_loss: 0.2092 - val_accuracy: 0.9362 - 17s/epoch - 39ms/step\n",
            "Epoch 331/1000\n",
            "\n",
            "Epoch 331: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2646 - accuracy: 0.9175 - val_loss: 0.1921 - val_accuracy: 0.9460 - 17s/epoch - 39ms/step\n",
            "Epoch 332/1000\n",
            "\n",
            "Epoch 332: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2678 - accuracy: 0.9152 - val_loss: 0.2067 - val_accuracy: 0.9404 - 17s/epoch - 39ms/step\n",
            "Epoch 333/1000\n",
            "\n",
            "Epoch 333: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2662 - accuracy: 0.9176 - val_loss: 0.1886 - val_accuracy: 0.9456 - 17s/epoch - 40ms/step\n",
            "Epoch 334/1000\n",
            "\n",
            "Epoch 334: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2654 - accuracy: 0.9172 - val_loss: 0.2022 - val_accuracy: 0.9446 - 18s/epoch - 42ms/step\n",
            "Epoch 335/1000\n",
            "\n",
            "Epoch 335: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2660 - accuracy: 0.9188 - val_loss: 0.1989 - val_accuracy: 0.9418 - 18s/epoch - 41ms/step\n",
            "Epoch 336/1000\n",
            "\n",
            "Epoch 336: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2643 - accuracy: 0.9190 - val_loss: 0.2027 - val_accuracy: 0.9416 - 17s/epoch - 40ms/step\n",
            "Epoch 337/1000\n",
            "\n",
            "Epoch 337: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2660 - accuracy: 0.9172 - val_loss: 0.1915 - val_accuracy: 0.9442 - 16s/epoch - 38ms/step\n",
            "Epoch 338/1000\n",
            "\n",
            "Epoch 338: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2671 - accuracy: 0.9172 - val_loss: 0.1977 - val_accuracy: 0.9418 - 16s/epoch - 38ms/step\n",
            "Epoch 339/1000\n",
            "\n",
            "Epoch 339: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2624 - accuracy: 0.9178 - val_loss: 0.2148 - val_accuracy: 0.9358 - 16s/epoch - 38ms/step\n",
            "Epoch 340/1000\n",
            "\n",
            "Epoch 340: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2677 - accuracy: 0.9178 - val_loss: 0.1948 - val_accuracy: 0.9442 - 16s/epoch - 38ms/step\n",
            "Epoch 341/1000\n",
            "\n",
            "Epoch 341: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2675 - accuracy: 0.9183 - val_loss: 0.1913 - val_accuracy: 0.9472 - 18s/epoch - 42ms/step\n",
            "Epoch 342/1000\n",
            "\n",
            "Epoch 342: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2638 - accuracy: 0.9190 - val_loss: 0.2113 - val_accuracy: 0.9408 - 18s/epoch - 42ms/step\n",
            "Epoch 343/1000\n",
            "\n",
            "Epoch 343: val_accuracy did not improve from 0.95320\n",
            "429/429 - 19s - loss: 0.2639 - accuracy: 0.9186 - val_loss: 0.1891 - val_accuracy: 0.9498 - 19s/epoch - 43ms/step\n",
            "Epoch 344/1000\n",
            "\n",
            "Epoch 344: val_accuracy did not improve from 0.95320\n",
            "429/429 - 19s - loss: 0.2640 - accuracy: 0.9184 - val_loss: 0.2022 - val_accuracy: 0.9400 - 19s/epoch - 43ms/step\n",
            "Epoch 345/1000\n",
            "\n",
            "Epoch 345: val_accuracy did not improve from 0.95320\n",
            "429/429 - 20s - loss: 0.2633 - accuracy: 0.9182 - val_loss: 0.1982 - val_accuracy: 0.9428 - 20s/epoch - 46ms/step\n",
            "Epoch 346/1000\n",
            "\n",
            "Epoch 346: val_accuracy did not improve from 0.95320\n",
            "429/429 - 19s - loss: 0.2673 - accuracy: 0.9170 - val_loss: 0.1937 - val_accuracy: 0.9444 - 19s/epoch - 45ms/step\n",
            "Epoch 347/1000\n",
            "\n",
            "Epoch 347: val_accuracy did not improve from 0.95320\n",
            "429/429 - 19s - loss: 0.2664 - accuracy: 0.9179 - val_loss: 0.1952 - val_accuracy: 0.9402 - 19s/epoch - 45ms/step\n",
            "Epoch 348/1000\n",
            "\n",
            "Epoch 348: val_accuracy did not improve from 0.95320\n",
            "429/429 - 19s - loss: 0.2644 - accuracy: 0.9179 - val_loss: 0.1829 - val_accuracy: 0.9484 - 19s/epoch - 44ms/step\n",
            "Epoch 349/1000\n",
            "\n",
            "Epoch 349: val_accuracy did not improve from 0.95320\n",
            "429/429 - 21s - loss: 0.2631 - accuracy: 0.9198 - val_loss: 0.1801 - val_accuracy: 0.9520 - 21s/epoch - 49ms/step\n",
            "Epoch 350/1000\n",
            "\n",
            "Epoch 350: val_accuracy did not improve from 0.95320\n",
            "429/429 - 19s - loss: 0.2658 - accuracy: 0.9186 - val_loss: 0.2020 - val_accuracy: 0.9414 - 19s/epoch - 44ms/step\n",
            "Epoch 351/1000\n",
            "\n",
            "Epoch 351: val_accuracy did not improve from 0.95320\n",
            "429/429 - 19s - loss: 0.2625 - accuracy: 0.9180 - val_loss: 0.2039 - val_accuracy: 0.9394 - 19s/epoch - 44ms/step\n",
            "Epoch 352/1000\n",
            "\n",
            "Epoch 352: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2629 - accuracy: 0.9173 - val_loss: 0.1796 - val_accuracy: 0.9504 - 18s/epoch - 43ms/step\n",
            "Epoch 353/1000\n",
            "\n",
            "Epoch 353: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2658 - accuracy: 0.9182 - val_loss: 0.2133 - val_accuracy: 0.9408 - 18s/epoch - 42ms/step\n",
            "Epoch 354/1000\n",
            "\n",
            "Epoch 354: val_accuracy did not improve from 0.95320\n",
            "429/429 - 19s - loss: 0.2663 - accuracy: 0.9172 - val_loss: 0.1957 - val_accuracy: 0.9454 - 19s/epoch - 44ms/step\n",
            "Epoch 355/1000\n",
            "\n",
            "Epoch 355: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2656 - accuracy: 0.9174 - val_loss: 0.2065 - val_accuracy: 0.9400 - 18s/epoch - 43ms/step\n",
            "Epoch 356/1000\n",
            "\n",
            "Epoch 356: val_accuracy did not improve from 0.95320\n",
            "429/429 - 19s - loss: 0.2667 - accuracy: 0.9194 - val_loss: 0.1849 - val_accuracy: 0.9480 - 19s/epoch - 43ms/step\n",
            "Epoch 357/1000\n",
            "\n",
            "Epoch 357: val_accuracy did not improve from 0.95320\n",
            "429/429 - 19s - loss: 0.2664 - accuracy: 0.9168 - val_loss: 0.2061 - val_accuracy: 0.9380 - 19s/epoch - 43ms/step\n",
            "Epoch 358/1000\n",
            "\n",
            "Epoch 358: val_accuracy did not improve from 0.95320\n",
            "429/429 - 19s - loss: 0.2654 - accuracy: 0.9177 - val_loss: 0.2072 - val_accuracy: 0.9398 - 19s/epoch - 45ms/step\n",
            "Epoch 359/1000\n",
            "\n",
            "Epoch 359: val_accuracy did not improve from 0.95320\n",
            "429/429 - 20s - loss: 0.2642 - accuracy: 0.9178 - val_loss: 0.2112 - val_accuracy: 0.9352 - 20s/epoch - 46ms/step\n",
            "Epoch 360/1000\n",
            "\n",
            "Epoch 360: val_accuracy did not improve from 0.95320\n",
            "429/429 - 19s - loss: 0.2622 - accuracy: 0.9177 - val_loss: 0.2158 - val_accuracy: 0.9388 - 19s/epoch - 44ms/step\n",
            "Epoch 361/1000\n",
            "\n",
            "Epoch 361: val_accuracy did not improve from 0.95320\n",
            "429/429 - 20s - loss: 0.2630 - accuracy: 0.9192 - val_loss: 0.1927 - val_accuracy: 0.9446 - 20s/epoch - 47ms/step\n",
            "Epoch 362/1000\n",
            "\n",
            "Epoch 362: val_accuracy did not improve from 0.95320\n",
            "429/429 - 19s - loss: 0.2620 - accuracy: 0.9181 - val_loss: 0.1893 - val_accuracy: 0.9458 - 19s/epoch - 45ms/step\n",
            "Epoch 363/1000\n",
            "\n",
            "Epoch 363: val_accuracy did not improve from 0.95320\n",
            "429/429 - 22s - loss: 0.2631 - accuracy: 0.9196 - val_loss: 0.1887 - val_accuracy: 0.9478 - 22s/epoch - 52ms/step\n",
            "Epoch 364/1000\n",
            "\n",
            "Epoch 364: val_accuracy did not improve from 0.95320\n",
            "429/429 - 21s - loss: 0.2621 - accuracy: 0.9192 - val_loss: 0.1867 - val_accuracy: 0.9478 - 21s/epoch - 50ms/step\n",
            "Epoch 365/1000\n",
            "\n",
            "Epoch 365: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2617 - accuracy: 0.9190 - val_loss: 0.2110 - val_accuracy: 0.9388 - 18s/epoch - 42ms/step\n",
            "Epoch 366/1000\n",
            "\n",
            "Epoch 366: val_accuracy did not improve from 0.95320\n",
            "429/429 - 20s - loss: 0.2665 - accuracy: 0.9173 - val_loss: 0.1853 - val_accuracy: 0.9488 - 20s/epoch - 46ms/step\n",
            "Epoch 367/1000\n",
            "\n",
            "Epoch 367: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2636 - accuracy: 0.9177 - val_loss: 0.2086 - val_accuracy: 0.9384 - 18s/epoch - 41ms/step\n",
            "Epoch 368/1000\n",
            "\n",
            "Epoch 368: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2651 - accuracy: 0.9174 - val_loss: 0.2069 - val_accuracy: 0.9402 - 18s/epoch - 42ms/step\n",
            "Epoch 369/1000\n",
            "\n",
            "Epoch 369: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2656 - accuracy: 0.9184 - val_loss: 0.2247 - val_accuracy: 0.9282 - 18s/epoch - 43ms/step\n",
            "Epoch 370/1000\n",
            "\n",
            "Epoch 370: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2600 - accuracy: 0.9194 - val_loss: 0.2349 - val_accuracy: 0.9300 - 18s/epoch - 41ms/step\n",
            "Epoch 371/1000\n",
            "\n",
            "Epoch 371: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2657 - accuracy: 0.9184 - val_loss: 0.1853 - val_accuracy: 0.9458 - 18s/epoch - 43ms/step\n",
            "Epoch 372/1000\n",
            "\n",
            "Epoch 372: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2640 - accuracy: 0.9162 - val_loss: 0.1778 - val_accuracy: 0.9512 - 18s/epoch - 42ms/step\n",
            "Epoch 373/1000\n",
            "\n",
            "Epoch 373: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2697 - accuracy: 0.9165 - val_loss: 0.1979 - val_accuracy: 0.9438 - 18s/epoch - 42ms/step\n",
            "Epoch 374/1000\n",
            "\n",
            "Epoch 374: val_accuracy did not improve from 0.95320\n",
            "429/429 - 19s - loss: 0.2655 - accuracy: 0.9178 - val_loss: 0.1999 - val_accuracy: 0.9390 - 19s/epoch - 44ms/step\n",
            "Epoch 375/1000\n",
            "\n",
            "Epoch 375: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2632 - accuracy: 0.9191 - val_loss: 0.2102 - val_accuracy: 0.9328 - 18s/epoch - 42ms/step\n",
            "Epoch 376/1000\n",
            "\n",
            "Epoch 376: val_accuracy did not improve from 0.95320\n",
            "429/429 - 20s - loss: 0.2647 - accuracy: 0.9170 - val_loss: 0.1986 - val_accuracy: 0.9434 - 20s/epoch - 45ms/step\n",
            "Epoch 377/1000\n",
            "\n",
            "Epoch 377: val_accuracy did not improve from 0.95320\n",
            "429/429 - 42s - loss: 0.2653 - accuracy: 0.9179 - val_loss: 0.1865 - val_accuracy: 0.9478 - 42s/epoch - 98ms/step\n",
            "Epoch 378/1000\n",
            "\n",
            "Epoch 378: val_accuracy did not improve from 0.95320\n",
            "429/429 - 20s - loss: 0.2641 - accuracy: 0.9174 - val_loss: 0.1935 - val_accuracy: 0.9438 - 20s/epoch - 47ms/step\n",
            "Epoch 379/1000\n",
            "\n",
            "Epoch 379: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2608 - accuracy: 0.9181 - val_loss: 0.1929 - val_accuracy: 0.9458 - 17s/epoch - 40ms/step\n",
            "Epoch 380/1000\n",
            "\n",
            "Epoch 380: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2665 - accuracy: 0.9185 - val_loss: 0.1895 - val_accuracy: 0.9456 - 18s/epoch - 41ms/step\n",
            "Epoch 381/1000\n",
            "\n",
            "Epoch 381: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2644 - accuracy: 0.9171 - val_loss: 0.1811 - val_accuracy: 0.9484 - 17s/epoch - 40ms/step\n",
            "Epoch 382/1000\n",
            "\n",
            "Epoch 382: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2608 - accuracy: 0.9193 - val_loss: 0.2024 - val_accuracy: 0.9420 - 17s/epoch - 39ms/step\n",
            "Epoch 383/1000\n",
            "\n",
            "Epoch 383: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2598 - accuracy: 0.9190 - val_loss: 0.2269 - val_accuracy: 0.9282 - 17s/epoch - 39ms/step\n",
            "Epoch 384/1000\n",
            "\n",
            "Epoch 384: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2658 - accuracy: 0.9179 - val_loss: 0.2211 - val_accuracy: 0.9356 - 16s/epoch - 38ms/step\n",
            "Epoch 385/1000\n",
            "\n",
            "Epoch 385: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2636 - accuracy: 0.9182 - val_loss: 0.1889 - val_accuracy: 0.9456 - 17s/epoch - 40ms/step\n",
            "Epoch 386/1000\n",
            "\n",
            "Epoch 386: val_accuracy did not improve from 0.95320\n",
            "429/429 - 21s - loss: 0.2635 - accuracy: 0.9189 - val_loss: 0.1996 - val_accuracy: 0.9412 - 21s/epoch - 50ms/step\n",
            "Epoch 387/1000\n",
            "\n",
            "Epoch 387: val_accuracy did not improve from 0.95320\n",
            "429/429 - 27s - loss: 0.2637 - accuracy: 0.9191 - val_loss: 0.1906 - val_accuracy: 0.9512 - 27s/epoch - 64ms/step\n",
            "Epoch 388/1000\n",
            "\n",
            "Epoch 388: val_accuracy did not improve from 0.95320\n",
            "429/429 - 21s - loss: 0.2631 - accuracy: 0.9168 - val_loss: 0.2121 - val_accuracy: 0.9382 - 21s/epoch - 48ms/step\n",
            "Epoch 389/1000\n",
            "\n",
            "Epoch 389: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2632 - accuracy: 0.9186 - val_loss: 0.2047 - val_accuracy: 0.9442 - 17s/epoch - 39ms/step\n",
            "Epoch 390/1000\n",
            "\n",
            "Epoch 390: val_accuracy did not improve from 0.95320\n",
            "429/429 - 19s - loss: 0.2662 - accuracy: 0.9169 - val_loss: 0.2060 - val_accuracy: 0.9430 - 19s/epoch - 43ms/step\n",
            "Epoch 391/1000\n",
            "\n",
            "Epoch 391: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2628 - accuracy: 0.9201 - val_loss: 0.2011 - val_accuracy: 0.9406 - 17s/epoch - 39ms/step\n",
            "Epoch 392/1000\n",
            "\n",
            "Epoch 392: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2645 - accuracy: 0.9181 - val_loss: 0.1956 - val_accuracy: 0.9462 - 16s/epoch - 37ms/step\n",
            "Epoch 393/1000\n",
            "\n",
            "Epoch 393: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2642 - accuracy: 0.9187 - val_loss: 0.1857 - val_accuracy: 0.9478 - 16s/epoch - 37ms/step\n",
            "Epoch 394/1000\n",
            "\n",
            "Epoch 394: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2618 - accuracy: 0.9192 - val_loss: 0.1758 - val_accuracy: 0.9490 - 16s/epoch - 36ms/step\n",
            "Epoch 395/1000\n",
            "\n",
            "Epoch 395: val_accuracy did not improve from 0.95320\n",
            "429/429 - 15s - loss: 0.2640 - accuracy: 0.9192 - val_loss: 0.2216 - val_accuracy: 0.9342 - 15s/epoch - 36ms/step\n",
            "Epoch 396/1000\n",
            "\n",
            "Epoch 396: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2653 - accuracy: 0.9174 - val_loss: 0.2024 - val_accuracy: 0.9426 - 16s/epoch - 36ms/step\n",
            "Epoch 397/1000\n",
            "\n",
            "Epoch 397: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2637 - accuracy: 0.9177 - val_loss: 0.1770 - val_accuracy: 0.9510 - 16s/epoch - 37ms/step\n",
            "Epoch 398/1000\n",
            "\n",
            "Epoch 398: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2666 - accuracy: 0.9182 - val_loss: 0.2203 - val_accuracy: 0.9322 - 16s/epoch - 36ms/step\n",
            "Epoch 399/1000\n",
            "\n",
            "Epoch 399: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2637 - accuracy: 0.9192 - val_loss: 0.1976 - val_accuracy: 0.9420 - 16s/epoch - 38ms/step\n",
            "Epoch 400/1000\n",
            "\n",
            "Epoch 400: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2627 - accuracy: 0.9181 - val_loss: 0.2045 - val_accuracy: 0.9420 - 16s/epoch - 38ms/step\n",
            "Epoch 401/1000\n",
            "\n",
            "Epoch 401: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2660 - accuracy: 0.9181 - val_loss: 0.2110 - val_accuracy: 0.9400 - 16s/epoch - 37ms/step\n",
            "Epoch 402/1000\n",
            "\n",
            "Epoch 402: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2642 - accuracy: 0.9174 - val_loss: 0.2013 - val_accuracy: 0.9406 - 17s/epoch - 39ms/step\n",
            "Epoch 403/1000\n",
            "\n",
            "Epoch 403: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2641 - accuracy: 0.9191 - val_loss: 0.2082 - val_accuracy: 0.9374 - 17s/epoch - 39ms/step\n",
            "Epoch 404/1000\n",
            "\n",
            "Epoch 404: val_accuracy did not improve from 0.95320\n",
            "429/429 - 19s - loss: 0.2627 - accuracy: 0.9187 - val_loss: 0.2284 - val_accuracy: 0.9334 - 19s/epoch - 45ms/step\n",
            "Epoch 405/1000\n",
            "\n",
            "Epoch 405: val_accuracy did not improve from 0.95320\n",
            "429/429 - 20s - loss: 0.2629 - accuracy: 0.9181 - val_loss: 0.2223 - val_accuracy: 0.9340 - 20s/epoch - 47ms/step\n",
            "Epoch 406/1000\n",
            "\n",
            "Epoch 406: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2622 - accuracy: 0.9195 - val_loss: 0.1977 - val_accuracy: 0.9428 - 16s/epoch - 37ms/step\n",
            "Epoch 407/1000\n",
            "\n",
            "Epoch 407: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2598 - accuracy: 0.9192 - val_loss: 0.1866 - val_accuracy: 0.9474 - 16s/epoch - 36ms/step\n",
            "Epoch 408/1000\n",
            "\n",
            "Epoch 408: val_accuracy did not improve from 0.95320\n",
            "429/429 - 15s - loss: 0.2636 - accuracy: 0.9195 - val_loss: 0.1934 - val_accuracy: 0.9462 - 15s/epoch - 36ms/step\n",
            "Epoch 409/1000\n",
            "\n",
            "Epoch 409: val_accuracy did not improve from 0.95320\n",
            "429/429 - 15s - loss: 0.2654 - accuracy: 0.9183 - val_loss: 0.1905 - val_accuracy: 0.9454 - 15s/epoch - 36ms/step\n",
            "Epoch 410/1000\n",
            "\n",
            "Epoch 410: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2652 - accuracy: 0.9177 - val_loss: 0.2054 - val_accuracy: 0.9410 - 16s/epoch - 37ms/step\n",
            "Epoch 411/1000\n",
            "\n",
            "Epoch 411: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2642 - accuracy: 0.9184 - val_loss: 0.2097 - val_accuracy: 0.9398 - 16s/epoch - 36ms/step\n",
            "Epoch 412/1000\n",
            "\n",
            "Epoch 412: val_accuracy did not improve from 0.95320\n",
            "429/429 - 15s - loss: 0.2624 - accuracy: 0.9176 - val_loss: 0.2058 - val_accuracy: 0.9436 - 15s/epoch - 36ms/step\n",
            "Epoch 413/1000\n",
            "\n",
            "Epoch 413: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2611 - accuracy: 0.9171 - val_loss: 0.1907 - val_accuracy: 0.9456 - 16s/epoch - 37ms/step\n",
            "Epoch 414/1000\n",
            "\n",
            "Epoch 414: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2676 - accuracy: 0.9161 - val_loss: 0.2249 - val_accuracy: 0.9318 - 17s/epoch - 39ms/step\n",
            "Epoch 415/1000\n",
            "\n",
            "Epoch 415: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2649 - accuracy: 0.9160 - val_loss: 0.2146 - val_accuracy: 0.9368 - 16s/epoch - 37ms/step\n",
            "Epoch 416/1000\n",
            "\n",
            "Epoch 416: val_accuracy did not improve from 0.95320\n",
            "429/429 - 19s - loss: 0.2630 - accuracy: 0.9188 - val_loss: 0.1964 - val_accuracy: 0.9382 - 19s/epoch - 45ms/step\n",
            "Epoch 417/1000\n",
            "\n",
            "Epoch 417: val_accuracy did not improve from 0.95320\n",
            "429/429 - 20s - loss: 0.2641 - accuracy: 0.9184 - val_loss: 0.1832 - val_accuracy: 0.9462 - 20s/epoch - 47ms/step\n",
            "Epoch 418/1000\n",
            "\n",
            "Epoch 418: val_accuracy did not improve from 0.95320\n",
            "429/429 - 19s - loss: 0.2609 - accuracy: 0.9198 - val_loss: 0.1885 - val_accuracy: 0.9454 - 19s/epoch - 44ms/step\n",
            "Epoch 419/1000\n",
            "\n",
            "Epoch 419: val_accuracy did not improve from 0.95320\n",
            "429/429 - 23s - loss: 0.2638 - accuracy: 0.9171 - val_loss: 0.2015 - val_accuracy: 0.9444 - 23s/epoch - 53ms/step\n",
            "Epoch 420/1000\n",
            "\n",
            "Epoch 420: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2590 - accuracy: 0.9190 - val_loss: 0.1862 - val_accuracy: 0.9464 - 17s/epoch - 41ms/step\n",
            "Epoch 421/1000\n",
            "\n",
            "Epoch 421: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2640 - accuracy: 0.9187 - val_loss: 0.2059 - val_accuracy: 0.9392 - 17s/epoch - 40ms/step\n",
            "Epoch 422/1000\n",
            "\n",
            "Epoch 422: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2653 - accuracy: 0.9174 - val_loss: 0.2086 - val_accuracy: 0.9366 - 17s/epoch - 40ms/step\n",
            "Epoch 423/1000\n",
            "\n",
            "Epoch 423: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2594 - accuracy: 0.9210 - val_loss: 0.2057 - val_accuracy: 0.9412 - 17s/epoch - 39ms/step\n",
            "Epoch 424/1000\n",
            "\n",
            "Epoch 424: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2639 - accuracy: 0.9185 - val_loss: 0.1812 - val_accuracy: 0.9522 - 17s/epoch - 39ms/step\n",
            "Epoch 425/1000\n",
            "\n",
            "Epoch 425: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2651 - accuracy: 0.9185 - val_loss: 0.1996 - val_accuracy: 0.9384 - 17s/epoch - 39ms/step\n",
            "Epoch 426/1000\n",
            "\n",
            "Epoch 426: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2600 - accuracy: 0.9206 - val_loss: 0.2079 - val_accuracy: 0.9386 - 17s/epoch - 39ms/step\n",
            "Epoch 427/1000\n",
            "\n",
            "Epoch 427: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2601 - accuracy: 0.9198 - val_loss: 0.1954 - val_accuracy: 0.9444 - 17s/epoch - 40ms/step\n",
            "Epoch 428/1000\n",
            "\n",
            "Epoch 428: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2631 - accuracy: 0.9183 - val_loss: 0.2288 - val_accuracy: 0.9312 - 17s/epoch - 41ms/step\n",
            "Epoch 429/1000\n",
            "\n",
            "Epoch 429: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2614 - accuracy: 0.9193 - val_loss: 0.2066 - val_accuracy: 0.9424 - 17s/epoch - 40ms/step\n",
            "Epoch 430/1000\n",
            "\n",
            "Epoch 430: val_accuracy did not improve from 0.95320\n",
            "429/429 - 19s - loss: 0.2608 - accuracy: 0.9186 - val_loss: 0.2057 - val_accuracy: 0.9430 - 19s/epoch - 44ms/step\n",
            "Epoch 431/1000\n",
            "\n",
            "Epoch 431: val_accuracy did not improve from 0.95320\n",
            "429/429 - 22s - loss: 0.2623 - accuracy: 0.9182 - val_loss: 0.2081 - val_accuracy: 0.9432 - 22s/epoch - 51ms/step\n",
            "Epoch 432/1000\n",
            "\n",
            "Epoch 432: val_accuracy did not improve from 0.95320\n",
            "429/429 - 20s - loss: 0.2627 - accuracy: 0.9197 - val_loss: 0.2145 - val_accuracy: 0.9382 - 20s/epoch - 46ms/step\n",
            "Epoch 433/1000\n",
            "\n",
            "Epoch 433: val_accuracy did not improve from 0.95320\n",
            "429/429 - 19s - loss: 0.2626 - accuracy: 0.9181 - val_loss: 0.1882 - val_accuracy: 0.9470 - 19s/epoch - 43ms/step\n",
            "Epoch 434/1000\n",
            "\n",
            "Epoch 434: val_accuracy did not improve from 0.95320\n",
            "429/429 - 20s - loss: 0.2619 - accuracy: 0.9190 - val_loss: 0.2101 - val_accuracy: 0.9376 - 20s/epoch - 47ms/step\n",
            "Epoch 435/1000\n",
            "\n",
            "Epoch 435: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2651 - accuracy: 0.9182 - val_loss: 0.2076 - val_accuracy: 0.9366 - 16s/epoch - 38ms/step\n",
            "Epoch 436/1000\n",
            "\n",
            "Epoch 436: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2612 - accuracy: 0.9185 - val_loss: 0.2186 - val_accuracy: 0.9344 - 16s/epoch - 37ms/step\n",
            "Epoch 437/1000\n",
            "\n",
            "Epoch 437: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2611 - accuracy: 0.9192 - val_loss: 0.1966 - val_accuracy: 0.9442 - 16s/epoch - 37ms/step\n",
            "Epoch 438/1000\n",
            "\n",
            "Epoch 438: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2634 - accuracy: 0.9183 - val_loss: 0.2046 - val_accuracy: 0.9384 - 16s/epoch - 37ms/step\n",
            "Epoch 439/1000\n",
            "\n",
            "Epoch 439: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2633 - accuracy: 0.9183 - val_loss: 0.1859 - val_accuracy: 0.9488 - 16s/epoch - 37ms/step\n",
            "Epoch 440/1000\n",
            "\n",
            "Epoch 440: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2581 - accuracy: 0.9213 - val_loss: 0.2008 - val_accuracy: 0.9434 - 16s/epoch - 37ms/step\n",
            "Epoch 441/1000\n",
            "\n",
            "Epoch 441: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2656 - accuracy: 0.9179 - val_loss: 0.2098 - val_accuracy: 0.9358 - 16s/epoch - 38ms/step\n",
            "Epoch 442/1000\n",
            "\n",
            "Epoch 442: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2606 - accuracy: 0.9180 - val_loss: 0.1909 - val_accuracy: 0.9468 - 16s/epoch - 37ms/step\n",
            "Epoch 443/1000\n",
            "\n",
            "Epoch 443: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2636 - accuracy: 0.9181 - val_loss: 0.1886 - val_accuracy: 0.9476 - 16s/epoch - 38ms/step\n",
            "Epoch 444/1000\n",
            "\n",
            "Epoch 444: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2627 - accuracy: 0.9178 - val_loss: 0.2113 - val_accuracy: 0.9414 - 16s/epoch - 38ms/step\n",
            "Epoch 445/1000\n",
            "\n",
            "Epoch 445: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2644 - accuracy: 0.9179 - val_loss: 0.2131 - val_accuracy: 0.9330 - 16s/epoch - 38ms/step\n",
            "Epoch 446/1000\n",
            "\n",
            "Epoch 446: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2636 - accuracy: 0.9184 - val_loss: 0.2108 - val_accuracy: 0.9332 - 17s/epoch - 40ms/step\n",
            "Epoch 447/1000\n",
            "\n",
            "Epoch 447: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2618 - accuracy: 0.9186 - val_loss: 0.2018 - val_accuracy: 0.9396 - 16s/epoch - 38ms/step\n",
            "Epoch 448/1000\n",
            "\n",
            "Epoch 448: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2599 - accuracy: 0.9183 - val_loss: 0.1848 - val_accuracy: 0.9460 - 17s/epoch - 39ms/step\n",
            "Epoch 449/1000\n",
            "\n",
            "Epoch 449: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2574 - accuracy: 0.9200 - val_loss: 0.1959 - val_accuracy: 0.9418 - 16s/epoch - 38ms/step\n",
            "Epoch 450/1000\n",
            "\n",
            "Epoch 450: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2641 - accuracy: 0.9185 - val_loss: 0.2002 - val_accuracy: 0.9404 - 16s/epoch - 37ms/step\n",
            "Epoch 451/1000\n",
            "\n",
            "Epoch 451: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2636 - accuracy: 0.9182 - val_loss: 0.1916 - val_accuracy: 0.9428 - 16s/epoch - 38ms/step\n",
            "Epoch 452/1000\n",
            "\n",
            "Epoch 452: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2598 - accuracy: 0.9194 - val_loss: 0.2069 - val_accuracy: 0.9392 - 17s/epoch - 39ms/step\n",
            "Epoch 453/1000\n",
            "\n",
            "Epoch 453: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2652 - accuracy: 0.9176 - val_loss: 0.1924 - val_accuracy: 0.9386 - 17s/epoch - 40ms/step\n",
            "Epoch 454/1000\n",
            "\n",
            "Epoch 454: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2646 - accuracy: 0.9193 - val_loss: 0.1868 - val_accuracy: 0.9470 - 16s/epoch - 37ms/step\n",
            "Epoch 455/1000\n",
            "\n",
            "Epoch 455: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2640 - accuracy: 0.9175 - val_loss: 0.2061 - val_accuracy: 0.9376 - 16s/epoch - 38ms/step\n",
            "Epoch 456/1000\n",
            "\n",
            "Epoch 456: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2631 - accuracy: 0.9197 - val_loss: 0.2108 - val_accuracy: 0.9382 - 17s/epoch - 39ms/step\n",
            "Epoch 457/1000\n",
            "\n",
            "Epoch 457: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2651 - accuracy: 0.9192 - val_loss: 0.2216 - val_accuracy: 0.9306 - 17s/epoch - 39ms/step\n",
            "Epoch 458/1000\n",
            "\n",
            "Epoch 458: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2634 - accuracy: 0.9182 - val_loss: 0.2019 - val_accuracy: 0.9388 - 17s/epoch - 39ms/step\n",
            "Epoch 459/1000\n",
            "\n",
            "Epoch 459: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2635 - accuracy: 0.9177 - val_loss: 0.1829 - val_accuracy: 0.9472 - 17s/epoch - 40ms/step\n",
            "Epoch 460/1000\n",
            "\n",
            "Epoch 460: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2606 - accuracy: 0.9197 - val_loss: 0.1914 - val_accuracy: 0.9498 - 17s/epoch - 41ms/step\n",
            "Epoch 461/1000\n",
            "\n",
            "Epoch 461: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2589 - accuracy: 0.9202 - val_loss: 0.2433 - val_accuracy: 0.9250 - 18s/epoch - 42ms/step\n",
            "Epoch 462/1000\n",
            "\n",
            "Epoch 462: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2591 - accuracy: 0.9198 - val_loss: 0.2068 - val_accuracy: 0.9396 - 16s/epoch - 37ms/step\n",
            "Epoch 463/1000\n",
            "\n",
            "Epoch 463: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2628 - accuracy: 0.9186 - val_loss: 0.1911 - val_accuracy: 0.9454 - 16s/epoch - 38ms/step\n",
            "Epoch 464/1000\n",
            "\n",
            "Epoch 464: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2624 - accuracy: 0.9201 - val_loss: 0.1962 - val_accuracy: 0.9428 - 16s/epoch - 38ms/step\n",
            "Epoch 465/1000\n",
            "\n",
            "Epoch 465: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2655 - accuracy: 0.9186 - val_loss: 0.1959 - val_accuracy: 0.9446 - 16s/epoch - 37ms/step\n",
            "Epoch 466/1000\n",
            "\n",
            "Epoch 466: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2605 - accuracy: 0.9206 - val_loss: 0.1804 - val_accuracy: 0.9490 - 16s/epoch - 37ms/step\n",
            "Epoch 467/1000\n",
            "\n",
            "Epoch 467: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2620 - accuracy: 0.9188 - val_loss: 0.1886 - val_accuracy: 0.9490 - 16s/epoch - 37ms/step\n",
            "Epoch 468/1000\n",
            "\n",
            "Epoch 468: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2632 - accuracy: 0.9190 - val_loss: 0.1822 - val_accuracy: 0.9468 - 16s/epoch - 36ms/step\n",
            "Epoch 469/1000\n",
            "\n",
            "Epoch 469: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2597 - accuracy: 0.9196 - val_loss: 0.1978 - val_accuracy: 0.9448 - 17s/epoch - 40ms/step\n",
            "Epoch 470/1000\n",
            "\n",
            "Epoch 470: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2618 - accuracy: 0.9205 - val_loss: 0.2119 - val_accuracy: 0.9408 - 16s/epoch - 38ms/step\n",
            "Epoch 471/1000\n",
            "\n",
            "Epoch 471: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2645 - accuracy: 0.9183 - val_loss: 0.2005 - val_accuracy: 0.9444 - 17s/epoch - 40ms/step\n",
            "Epoch 472/1000\n",
            "\n",
            "Epoch 472: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2626 - accuracy: 0.9184 - val_loss: 0.1920 - val_accuracy: 0.9476 - 17s/epoch - 40ms/step\n",
            "Epoch 473/1000\n",
            "\n",
            "Epoch 473: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2618 - accuracy: 0.9187 - val_loss: 0.1794 - val_accuracy: 0.9508 - 17s/epoch - 39ms/step\n",
            "Epoch 474/1000\n",
            "\n",
            "Epoch 474: val_accuracy did not improve from 0.95320\n",
            "429/429 - 20s - loss: 0.2632 - accuracy: 0.9196 - val_loss: 0.1925 - val_accuracy: 0.9444 - 20s/epoch - 46ms/step\n",
            "Epoch 475/1000\n",
            "\n",
            "Epoch 475: val_accuracy did not improve from 0.95320\n",
            "429/429 - 23s - loss: 0.2643 - accuracy: 0.9180 - val_loss: 0.1971 - val_accuracy: 0.9448 - 23s/epoch - 53ms/step\n",
            "Epoch 476/1000\n",
            "\n",
            "Epoch 476: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2626 - accuracy: 0.9184 - val_loss: 0.2227 - val_accuracy: 0.9308 - 17s/epoch - 40ms/step\n",
            "Epoch 477/1000\n",
            "\n",
            "Epoch 477: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2647 - accuracy: 0.9186 - val_loss: 0.1987 - val_accuracy: 0.9424 - 16s/epoch - 37ms/step\n",
            "Epoch 478/1000\n",
            "\n",
            "Epoch 478: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2669 - accuracy: 0.9167 - val_loss: 0.1869 - val_accuracy: 0.9464 - 16s/epoch - 37ms/step\n",
            "Epoch 479/1000\n",
            "\n",
            "Epoch 479: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2631 - accuracy: 0.9188 - val_loss: 0.2142 - val_accuracy: 0.9422 - 16s/epoch - 37ms/step\n",
            "Epoch 480/1000\n",
            "\n",
            "Epoch 480: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2635 - accuracy: 0.9170 - val_loss: 0.2338 - val_accuracy: 0.9336 - 16s/epoch - 37ms/step\n",
            "Epoch 481/1000\n",
            "\n",
            "Epoch 481: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2613 - accuracy: 0.9190 - val_loss: 0.2050 - val_accuracy: 0.9376 - 16s/epoch - 38ms/step\n",
            "Epoch 482/1000\n",
            "\n",
            "Epoch 482: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2576 - accuracy: 0.9208 - val_loss: 0.1829 - val_accuracy: 0.9454 - 16s/epoch - 38ms/step\n",
            "Epoch 483/1000\n",
            "\n",
            "Epoch 483: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2625 - accuracy: 0.9177 - val_loss: 0.1933 - val_accuracy: 0.9456 - 16s/epoch - 38ms/step\n",
            "Epoch 484/1000\n",
            "\n",
            "Epoch 484: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2607 - accuracy: 0.9195 - val_loss: 0.1927 - val_accuracy: 0.9440 - 17s/epoch - 39ms/step\n",
            "Epoch 485/1000\n",
            "\n",
            "Epoch 485: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2619 - accuracy: 0.9191 - val_loss: 0.1972 - val_accuracy: 0.9434 - 16s/epoch - 38ms/step\n",
            "Epoch 486/1000\n",
            "\n",
            "Epoch 486: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2648 - accuracy: 0.9188 - val_loss: 0.1832 - val_accuracy: 0.9496 - 17s/epoch - 39ms/step\n",
            "Epoch 487/1000\n",
            "\n",
            "Epoch 487: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2620 - accuracy: 0.9198 - val_loss: 0.1935 - val_accuracy: 0.9454 - 17s/epoch - 39ms/step\n",
            "Epoch 488/1000\n",
            "\n",
            "Epoch 488: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2624 - accuracy: 0.9187 - val_loss: 0.2035 - val_accuracy: 0.9408 - 17s/epoch - 40ms/step\n",
            "Epoch 489/1000\n",
            "\n",
            "Epoch 489: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2640 - accuracy: 0.9185 - val_loss: 0.1897 - val_accuracy: 0.9474 - 18s/epoch - 42ms/step\n",
            "Epoch 490/1000\n",
            "\n",
            "Epoch 490: val_accuracy did not improve from 0.95320\n",
            "429/429 - 19s - loss: 0.2651 - accuracy: 0.9171 - val_loss: 0.1990 - val_accuracy: 0.9428 - 19s/epoch - 43ms/step\n",
            "Epoch 491/1000\n",
            "\n",
            "Epoch 491: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2614 - accuracy: 0.9189 - val_loss: 0.1875 - val_accuracy: 0.9480 - 16s/epoch - 37ms/step\n",
            "Epoch 492/1000\n",
            "\n",
            "Epoch 492: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2613 - accuracy: 0.9200 - val_loss: 0.1949 - val_accuracy: 0.9438 - 16s/epoch - 38ms/step\n",
            "Epoch 493/1000\n",
            "\n",
            "Epoch 493: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2627 - accuracy: 0.9183 - val_loss: 0.1923 - val_accuracy: 0.9450 - 16s/epoch - 38ms/step\n",
            "Epoch 494/1000\n",
            "\n",
            "Epoch 494: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2616 - accuracy: 0.9182 - val_loss: 0.1949 - val_accuracy: 0.9428 - 16s/epoch - 37ms/step\n",
            "Epoch 495/1000\n",
            "\n",
            "Epoch 495: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2600 - accuracy: 0.9194 - val_loss: 0.1922 - val_accuracy: 0.9454 - 16s/epoch - 37ms/step\n",
            "Epoch 496/1000\n",
            "\n",
            "Epoch 496: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2646 - accuracy: 0.9188 - val_loss: 0.1923 - val_accuracy: 0.9462 - 16s/epoch - 37ms/step\n",
            "Epoch 497/1000\n",
            "\n",
            "Epoch 497: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2660 - accuracy: 0.9180 - val_loss: 0.2060 - val_accuracy: 0.9410 - 16s/epoch - 38ms/step\n",
            "Epoch 498/1000\n",
            "\n",
            "Epoch 498: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2619 - accuracy: 0.9179 - val_loss: 0.2102 - val_accuracy: 0.9396 - 17s/epoch - 39ms/step\n",
            "Epoch 499/1000\n",
            "\n",
            "Epoch 499: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2624 - accuracy: 0.9181 - val_loss: 0.1959 - val_accuracy: 0.9420 - 16s/epoch - 38ms/step\n",
            "Epoch 500/1000\n",
            "\n",
            "Epoch 500: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2610 - accuracy: 0.9196 - val_loss: 0.1867 - val_accuracy: 0.9474 - 16s/epoch - 38ms/step\n",
            "Epoch 501/1000\n",
            "\n",
            "Epoch 501: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2581 - accuracy: 0.9201 - val_loss: 0.1822 - val_accuracy: 0.9454 - 17s/epoch - 40ms/step\n",
            "Epoch 502/1000\n",
            "\n",
            "Epoch 502: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2610 - accuracy: 0.9180 - val_loss: 0.2077 - val_accuracy: 0.9406 - 18s/epoch - 41ms/step\n",
            "Epoch 503/1000\n",
            "\n",
            "Epoch 503: val_accuracy did not improve from 0.95320\n",
            "429/429 - 19s - loss: 0.2606 - accuracy: 0.9186 - val_loss: 0.1954 - val_accuracy: 0.9460 - 19s/epoch - 44ms/step\n",
            "Epoch 504/1000\n",
            "\n",
            "Epoch 504: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2647 - accuracy: 0.9179 - val_loss: 0.1990 - val_accuracy: 0.9430 - 17s/epoch - 39ms/step\n",
            "Epoch 505/1000\n",
            "\n",
            "Epoch 505: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2585 - accuracy: 0.9184 - val_loss: 0.2035 - val_accuracy: 0.9396 - 17s/epoch - 39ms/step\n",
            "Epoch 506/1000\n",
            "\n",
            "Epoch 506: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2666 - accuracy: 0.9178 - val_loss: 0.2203 - val_accuracy: 0.9322 - 16s/epoch - 37ms/step\n",
            "Epoch 507/1000\n",
            "\n",
            "Epoch 507: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2672 - accuracy: 0.9173 - val_loss: 0.2034 - val_accuracy: 0.9422 - 16s/epoch - 37ms/step\n",
            "Epoch 508/1000\n",
            "\n",
            "Epoch 508: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2632 - accuracy: 0.9185 - val_loss: 0.2074 - val_accuracy: 0.9368 - 16s/epoch - 37ms/step\n",
            "Epoch 509/1000\n",
            "\n",
            "Epoch 509: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2643 - accuracy: 0.9179 - val_loss: 0.2007 - val_accuracy: 0.9404 - 16s/epoch - 38ms/step\n",
            "Epoch 510/1000\n",
            "\n",
            "Epoch 510: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2632 - accuracy: 0.9170 - val_loss: 0.2250 - val_accuracy: 0.9314 - 16s/epoch - 37ms/step\n",
            "Epoch 511/1000\n",
            "\n",
            "Epoch 511: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2615 - accuracy: 0.9177 - val_loss: 0.1770 - val_accuracy: 0.9492 - 16s/epoch - 38ms/step\n",
            "Epoch 512/1000\n",
            "\n",
            "Epoch 512: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2630 - accuracy: 0.9187 - val_loss: 0.2054 - val_accuracy: 0.9428 - 17s/epoch - 40ms/step\n",
            "Epoch 513/1000\n",
            "\n",
            "Epoch 513: val_accuracy did not improve from 0.95320\n",
            "429/429 - 19s - loss: 0.2657 - accuracy: 0.9178 - val_loss: 0.1989 - val_accuracy: 0.9402 - 19s/epoch - 44ms/step\n",
            "Epoch 514/1000\n",
            "\n",
            "Epoch 514: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2626 - accuracy: 0.9188 - val_loss: 0.1928 - val_accuracy: 0.9456 - 18s/epoch - 42ms/step\n",
            "Epoch 515/1000\n",
            "\n",
            "Epoch 515: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2644 - accuracy: 0.9182 - val_loss: 0.1794 - val_accuracy: 0.9480 - 17s/epoch - 39ms/step\n",
            "Epoch 516/1000\n",
            "\n",
            "Epoch 516: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2619 - accuracy: 0.9197 - val_loss: 0.1837 - val_accuracy: 0.9468 - 17s/epoch - 41ms/step\n",
            "Epoch 517/1000\n",
            "\n",
            "Epoch 517: val_accuracy did not improve from 0.95320\n",
            "429/429 - 20s - loss: 0.2603 - accuracy: 0.9191 - val_loss: 0.1992 - val_accuracy: 0.9456 - 20s/epoch - 47ms/step\n",
            "Epoch 518/1000\n",
            "\n",
            "Epoch 518: val_accuracy did not improve from 0.95320\n",
            "429/429 - 19s - loss: 0.2613 - accuracy: 0.9186 - val_loss: 0.1921 - val_accuracy: 0.9460 - 19s/epoch - 44ms/step\n",
            "Epoch 519/1000\n",
            "\n",
            "Epoch 519: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2652 - accuracy: 0.9167 - val_loss: 0.1804 - val_accuracy: 0.9476 - 17s/epoch - 40ms/step\n",
            "Epoch 520/1000\n",
            "\n",
            "Epoch 520: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2611 - accuracy: 0.9196 - val_loss: 0.2060 - val_accuracy: 0.9414 - 16s/epoch - 38ms/step\n",
            "Epoch 521/1000\n",
            "\n",
            "Epoch 521: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2630 - accuracy: 0.9183 - val_loss: 0.1910 - val_accuracy: 0.9448 - 16s/epoch - 38ms/step\n",
            "Epoch 522/1000\n",
            "\n",
            "Epoch 522: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2624 - accuracy: 0.9186 - val_loss: 0.1973 - val_accuracy: 0.9430 - 16s/epoch - 37ms/step\n",
            "Epoch 523/1000\n",
            "\n",
            "Epoch 523: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2629 - accuracy: 0.9190 - val_loss: 0.1959 - val_accuracy: 0.9446 - 16s/epoch - 38ms/step\n",
            "Epoch 524/1000\n",
            "\n",
            "Epoch 524: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2638 - accuracy: 0.9185 - val_loss: 0.1880 - val_accuracy: 0.9462 - 16s/epoch - 37ms/step\n",
            "Epoch 525/1000\n",
            "\n",
            "Epoch 525: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2637 - accuracy: 0.9189 - val_loss: 0.1900 - val_accuracy: 0.9456 - 17s/epoch - 39ms/step\n",
            "Epoch 526/1000\n",
            "\n",
            "Epoch 526: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2595 - accuracy: 0.9186 - val_loss: 0.1949 - val_accuracy: 0.9416 - 16s/epoch - 38ms/step\n",
            "Epoch 527/1000\n",
            "\n",
            "Epoch 527: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2586 - accuracy: 0.9203 - val_loss: 0.1919 - val_accuracy: 0.9494 - 16s/epoch - 38ms/step\n",
            "Epoch 528/1000\n",
            "\n",
            "Epoch 528: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2614 - accuracy: 0.9191 - val_loss: 0.2045 - val_accuracy: 0.9372 - 17s/epoch - 39ms/step\n",
            "Epoch 529/1000\n",
            "\n",
            "Epoch 529: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2605 - accuracy: 0.9188 - val_loss: 0.1877 - val_accuracy: 0.9490 - 18s/epoch - 42ms/step\n",
            "Epoch 530/1000\n",
            "\n",
            "Epoch 530: val_accuracy did not improve from 0.95320\n",
            "429/429 - 20s - loss: 0.2620 - accuracy: 0.9195 - val_loss: 0.2054 - val_accuracy: 0.9404 - 20s/epoch - 47ms/step\n",
            "Epoch 531/1000\n",
            "\n",
            "Epoch 531: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2631 - accuracy: 0.9181 - val_loss: 0.2127 - val_accuracy: 0.9392 - 18s/epoch - 41ms/step\n",
            "Epoch 532/1000\n",
            "\n",
            "Epoch 532: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2628 - accuracy: 0.9196 - val_loss: 0.1919 - val_accuracy: 0.9488 - 16s/epoch - 38ms/step\n",
            "Epoch 533/1000\n",
            "\n",
            "Epoch 533: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2634 - accuracy: 0.9178 - val_loss: 0.1927 - val_accuracy: 0.9472 - 17s/epoch - 39ms/step\n",
            "Epoch 534/1000\n",
            "\n",
            "Epoch 534: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2618 - accuracy: 0.9184 - val_loss: 0.1925 - val_accuracy: 0.9414 - 16s/epoch - 37ms/step\n",
            "Epoch 535/1000\n",
            "\n",
            "Epoch 535: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2605 - accuracy: 0.9206 - val_loss: 0.2010 - val_accuracy: 0.9384 - 16s/epoch - 37ms/step\n",
            "Epoch 536/1000\n",
            "\n",
            "Epoch 536: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2640 - accuracy: 0.9184 - val_loss: 0.1930 - val_accuracy: 0.9426 - 16s/epoch - 37ms/step\n",
            "Epoch 537/1000\n",
            "\n",
            "Epoch 537: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2587 - accuracy: 0.9201 - val_loss: 0.2033 - val_accuracy: 0.9418 - 16s/epoch - 37ms/step\n",
            "Epoch 538/1000\n",
            "\n",
            "Epoch 538: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2632 - accuracy: 0.9179 - val_loss: 0.2026 - val_accuracy: 0.9440 - 16s/epoch - 37ms/step\n",
            "Epoch 539/1000\n",
            "\n",
            "Epoch 539: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2637 - accuracy: 0.9184 - val_loss: 0.2119 - val_accuracy: 0.9438 - 16s/epoch - 37ms/step\n",
            "Epoch 540/1000\n",
            "\n",
            "Epoch 540: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2574 - accuracy: 0.9206 - val_loss: 0.2054 - val_accuracy: 0.9426 - 17s/epoch - 39ms/step\n",
            "Epoch 541/1000\n",
            "\n",
            "Epoch 541: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2631 - accuracy: 0.9181 - val_loss: 0.2030 - val_accuracy: 0.9406 - 17s/epoch - 39ms/step\n",
            "Epoch 542/1000\n",
            "\n",
            "Epoch 542: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2590 - accuracy: 0.9199 - val_loss: 0.1931 - val_accuracy: 0.9426 - 16s/epoch - 38ms/step\n",
            "Epoch 543/1000\n",
            "\n",
            "Epoch 543: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2615 - accuracy: 0.9199 - val_loss: 0.2178 - val_accuracy: 0.9388 - 18s/epoch - 41ms/step\n",
            "Epoch 544/1000\n",
            "\n",
            "Epoch 544: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2617 - accuracy: 0.9186 - val_loss: 0.2047 - val_accuracy: 0.9394 - 17s/epoch - 40ms/step\n",
            "Epoch 545/1000\n",
            "\n",
            "Epoch 545: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2679 - accuracy: 0.9171 - val_loss: 0.1955 - val_accuracy: 0.9428 - 18s/epoch - 42ms/step\n",
            "Epoch 546/1000\n",
            "\n",
            "Epoch 546: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2616 - accuracy: 0.9193 - val_loss: 0.1892 - val_accuracy: 0.9506 - 18s/epoch - 41ms/step\n",
            "Epoch 547/1000\n",
            "\n",
            "Epoch 547: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2637 - accuracy: 0.9186 - val_loss: 0.2308 - val_accuracy: 0.9332 - 16s/epoch - 38ms/step\n",
            "Epoch 548/1000\n",
            "\n",
            "Epoch 548: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2614 - accuracy: 0.9184 - val_loss: 0.2032 - val_accuracy: 0.9430 - 16s/epoch - 38ms/step\n",
            "Epoch 549/1000\n",
            "\n",
            "Epoch 549: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2601 - accuracy: 0.9197 - val_loss: 0.1846 - val_accuracy: 0.9482 - 16s/epoch - 37ms/step\n",
            "Epoch 550/1000\n",
            "\n",
            "Epoch 550: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2617 - accuracy: 0.9187 - val_loss: 0.2274 - val_accuracy: 0.9326 - 16s/epoch - 37ms/step\n",
            "Epoch 551/1000\n",
            "\n",
            "Epoch 551: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2599 - accuracy: 0.9195 - val_loss: 0.1979 - val_accuracy: 0.9430 - 16s/epoch - 37ms/step\n",
            "Epoch 552/1000\n",
            "\n",
            "Epoch 552: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2607 - accuracy: 0.9196 - val_loss: 0.1932 - val_accuracy: 0.9444 - 16s/epoch - 38ms/step\n",
            "Epoch 553/1000\n",
            "\n",
            "Epoch 553: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2615 - accuracy: 0.9194 - val_loss: 0.1863 - val_accuracy: 0.9488 - 16s/epoch - 38ms/step\n",
            "Epoch 554/1000\n",
            "\n",
            "Epoch 554: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2606 - accuracy: 0.9193 - val_loss: 0.1888 - val_accuracy: 0.9424 - 16s/epoch - 38ms/step\n",
            "Epoch 555/1000\n",
            "\n",
            "Epoch 555: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2586 - accuracy: 0.9190 - val_loss: 0.1855 - val_accuracy: 0.9468 - 16s/epoch - 37ms/step\n",
            "Epoch 556/1000\n",
            "\n",
            "Epoch 556: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2629 - accuracy: 0.9193 - val_loss: 0.1835 - val_accuracy: 0.9460 - 16s/epoch - 38ms/step\n",
            "Epoch 557/1000\n",
            "\n",
            "Epoch 557: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2620 - accuracy: 0.9191 - val_loss: 0.2033 - val_accuracy: 0.9366 - 17s/epoch - 39ms/step\n",
            "Epoch 558/1000\n",
            "\n",
            "Epoch 558: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2589 - accuracy: 0.9193 - val_loss: 0.1981 - val_accuracy: 0.9402 - 17s/epoch - 39ms/step\n",
            "Epoch 559/1000\n",
            "\n",
            "Epoch 559: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2632 - accuracy: 0.9175 - val_loss: 0.1957 - val_accuracy: 0.9462 - 17s/epoch - 40ms/step\n",
            "Epoch 560/1000\n",
            "\n",
            "Epoch 560: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2596 - accuracy: 0.9191 - val_loss: 0.1819 - val_accuracy: 0.9470 - 18s/epoch - 41ms/step\n",
            "Epoch 561/1000\n",
            "\n",
            "Epoch 561: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2652 - accuracy: 0.9189 - val_loss: 0.2042 - val_accuracy: 0.9428 - 17s/epoch - 40ms/step\n",
            "Epoch 562/1000\n",
            "\n",
            "Epoch 562: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2654 - accuracy: 0.9172 - val_loss: 0.2035 - val_accuracy: 0.9382 - 16s/epoch - 37ms/step\n",
            "Epoch 563/1000\n",
            "\n",
            "Epoch 563: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2601 - accuracy: 0.9199 - val_loss: 0.2286 - val_accuracy: 0.9332 - 16s/epoch - 37ms/step\n",
            "Epoch 564/1000\n",
            "\n",
            "Epoch 564: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2613 - accuracy: 0.9190 - val_loss: 0.2124 - val_accuracy: 0.9372 - 16s/epoch - 37ms/step\n",
            "Epoch 565/1000\n",
            "\n",
            "Epoch 565: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2619 - accuracy: 0.9195 - val_loss: 0.2175 - val_accuracy: 0.9346 - 16s/epoch - 37ms/step\n",
            "Epoch 566/1000\n",
            "\n",
            "Epoch 566: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2627 - accuracy: 0.9199 - val_loss: 0.2244 - val_accuracy: 0.9338 - 16s/epoch - 38ms/step\n",
            "Epoch 567/1000\n",
            "\n",
            "Epoch 567: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2625 - accuracy: 0.9182 - val_loss: 0.2077 - val_accuracy: 0.9398 - 16s/epoch - 37ms/step\n",
            "Epoch 568/1000\n",
            "\n",
            "Epoch 568: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2654 - accuracy: 0.9182 - val_loss: 0.2058 - val_accuracy: 0.9392 - 16s/epoch - 38ms/step\n",
            "Epoch 569/1000\n",
            "\n",
            "Epoch 569: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2622 - accuracy: 0.9195 - val_loss: 0.1909 - val_accuracy: 0.9460 - 16s/epoch - 38ms/step\n",
            "Epoch 570/1000\n",
            "\n",
            "Epoch 570: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2640 - accuracy: 0.9196 - val_loss: 0.1946 - val_accuracy: 0.9460 - 17s/epoch - 39ms/step\n",
            "Epoch 571/1000\n",
            "\n",
            "Epoch 571: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2568 - accuracy: 0.9211 - val_loss: 0.1977 - val_accuracy: 0.9450 - 17s/epoch - 40ms/step\n",
            "Epoch 572/1000\n",
            "\n",
            "Epoch 572: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2625 - accuracy: 0.9182 - val_loss: 0.1901 - val_accuracy: 0.9460 - 17s/epoch - 41ms/step\n",
            "Epoch 573/1000\n",
            "\n",
            "Epoch 573: val_accuracy did not improve from 0.95320\n",
            "429/429 - 22s - loss: 0.2610 - accuracy: 0.9200 - val_loss: 0.1898 - val_accuracy: 0.9462 - 22s/epoch - 51ms/step\n",
            "Epoch 574/1000\n",
            "\n",
            "Epoch 574: val_accuracy did not improve from 0.95320\n",
            "429/429 - 19s - loss: 0.2619 - accuracy: 0.9188 - val_loss: 0.1898 - val_accuracy: 0.9476 - 19s/epoch - 44ms/step\n",
            "Epoch 575/1000\n",
            "\n",
            "Epoch 575: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2608 - accuracy: 0.9195 - val_loss: 0.1916 - val_accuracy: 0.9452 - 16s/epoch - 37ms/step\n",
            "Epoch 576/1000\n",
            "\n",
            "Epoch 576: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2636 - accuracy: 0.9192 - val_loss: 0.2248 - val_accuracy: 0.9316 - 16s/epoch - 38ms/step\n",
            "Epoch 577/1000\n",
            "\n",
            "Epoch 577: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2608 - accuracy: 0.9203 - val_loss: 0.2099 - val_accuracy: 0.9424 - 16s/epoch - 36ms/step\n",
            "Epoch 578/1000\n",
            "\n",
            "Epoch 578: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2588 - accuracy: 0.9191 - val_loss: 0.1898 - val_accuracy: 0.9458 - 16s/epoch - 37ms/step\n",
            "Epoch 579/1000\n",
            "\n",
            "Epoch 579: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2583 - accuracy: 0.9208 - val_loss: 0.2107 - val_accuracy: 0.9420 - 16s/epoch - 37ms/step\n",
            "Epoch 580/1000\n",
            "\n",
            "Epoch 580: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2616 - accuracy: 0.9201 - val_loss: 0.1838 - val_accuracy: 0.9468 - 16s/epoch - 37ms/step\n",
            "Epoch 581/1000\n",
            "\n",
            "Epoch 581: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2613 - accuracy: 0.9191 - val_loss: 0.2128 - val_accuracy: 0.9356 - 16s/epoch - 38ms/step\n",
            "Epoch 582/1000\n",
            "\n",
            "Epoch 582: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2626 - accuracy: 0.9202 - val_loss: 0.1849 - val_accuracy: 0.9482 - 16s/epoch - 38ms/step\n",
            "Epoch 583/1000\n",
            "\n",
            "Epoch 583: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2583 - accuracy: 0.9203 - val_loss: 0.2239 - val_accuracy: 0.9338 - 16s/epoch - 37ms/step\n",
            "Epoch 584/1000\n",
            "\n",
            "Epoch 584: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2618 - accuracy: 0.9196 - val_loss: 0.1857 - val_accuracy: 0.9474 - 16s/epoch - 38ms/step\n",
            "Epoch 585/1000\n",
            "\n",
            "Epoch 585: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2604 - accuracy: 0.9193 - val_loss: 0.2014 - val_accuracy: 0.9398 - 16s/epoch - 37ms/step\n",
            "Epoch 586/1000\n",
            "\n",
            "Epoch 586: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2598 - accuracy: 0.9199 - val_loss: 0.1908 - val_accuracy: 0.9442 - 17s/epoch - 40ms/step\n",
            "Epoch 587/1000\n",
            "\n",
            "Epoch 587: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2626 - accuracy: 0.9189 - val_loss: 0.1969 - val_accuracy: 0.9420 - 17s/epoch - 39ms/step\n",
            "Epoch 588/1000\n",
            "\n",
            "Epoch 588: val_accuracy did not improve from 0.95320\n",
            "429/429 - 19s - loss: 0.2626 - accuracy: 0.9185 - val_loss: 0.2099 - val_accuracy: 0.9390 - 19s/epoch - 43ms/step\n",
            "Epoch 589/1000\n",
            "\n",
            "Epoch 589: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2577 - accuracy: 0.9195 - val_loss: 0.2033 - val_accuracy: 0.9430 - 17s/epoch - 39ms/step\n",
            "Epoch 590/1000\n",
            "\n",
            "Epoch 590: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2574 - accuracy: 0.9201 - val_loss: 0.2021 - val_accuracy: 0.9452 - 16s/epoch - 37ms/step\n",
            "Epoch 591/1000\n",
            "\n",
            "Epoch 591: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2616 - accuracy: 0.9190 - val_loss: 0.2243 - val_accuracy: 0.9320 - 16s/epoch - 37ms/step\n",
            "Epoch 592/1000\n",
            "\n",
            "Epoch 592: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2605 - accuracy: 0.9202 - val_loss: 0.1952 - val_accuracy: 0.9414 - 17s/epoch - 40ms/step\n",
            "Epoch 593/1000\n",
            "\n",
            "Epoch 593: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2607 - accuracy: 0.9199 - val_loss: 0.1909 - val_accuracy: 0.9446 - 16s/epoch - 37ms/step\n",
            "Epoch 594/1000\n",
            "\n",
            "Epoch 594: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2601 - accuracy: 0.9191 - val_loss: 0.2027 - val_accuracy: 0.9430 - 16s/epoch - 38ms/step\n",
            "Epoch 595/1000\n",
            "\n",
            "Epoch 595: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2647 - accuracy: 0.9189 - val_loss: 0.1956 - val_accuracy: 0.9452 - 16s/epoch - 38ms/step\n",
            "Epoch 596/1000\n",
            "\n",
            "Epoch 596: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2606 - accuracy: 0.9195 - val_loss: 0.1969 - val_accuracy: 0.9442 - 16s/epoch - 37ms/step\n",
            "Epoch 597/1000\n",
            "\n",
            "Epoch 597: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2628 - accuracy: 0.9179 - val_loss: 0.1951 - val_accuracy: 0.9424 - 16s/epoch - 38ms/step\n",
            "Epoch 598/1000\n",
            "\n",
            "Epoch 598: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2596 - accuracy: 0.9189 - val_loss: 0.2090 - val_accuracy: 0.9404 - 16s/epoch - 38ms/step\n",
            "Epoch 599/1000\n",
            "\n",
            "Epoch 599: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2602 - accuracy: 0.9196 - val_loss: 0.1940 - val_accuracy: 0.9428 - 17s/epoch - 39ms/step\n",
            "Epoch 600/1000\n",
            "\n",
            "Epoch 600: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2619 - accuracy: 0.9190 - val_loss: 0.2228 - val_accuracy: 0.9326 - 17s/epoch - 39ms/step\n",
            "Epoch 601/1000\n",
            "\n",
            "Epoch 601: val_accuracy did not improve from 0.95320\n",
            "429/429 - 21s - loss: 0.2618 - accuracy: 0.9179 - val_loss: 0.1901 - val_accuracy: 0.9422 - 21s/epoch - 49ms/step\n",
            "Epoch 602/1000\n",
            "\n",
            "Epoch 602: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2595 - accuracy: 0.9196 - val_loss: 0.2082 - val_accuracy: 0.9396 - 17s/epoch - 40ms/step\n",
            "Epoch 603/1000\n",
            "\n",
            "Epoch 603: val_accuracy did not improve from 0.95320\n",
            "429/429 - 20s - loss: 0.2581 - accuracy: 0.9206 - val_loss: 0.2013 - val_accuracy: 0.9374 - 20s/epoch - 47ms/step\n",
            "Epoch 604/1000\n",
            "\n",
            "Epoch 604: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2618 - accuracy: 0.9184 - val_loss: 0.1805 - val_accuracy: 0.9492 - 16s/epoch - 38ms/step\n",
            "Epoch 605/1000\n",
            "\n",
            "Epoch 605: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2618 - accuracy: 0.9200 - val_loss: 0.1984 - val_accuracy: 0.9390 - 16s/epoch - 37ms/step\n",
            "Epoch 606/1000\n",
            "\n",
            "Epoch 606: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2589 - accuracy: 0.9198 - val_loss: 0.1964 - val_accuracy: 0.9422 - 16s/epoch - 38ms/step\n",
            "Epoch 607/1000\n",
            "\n",
            "Epoch 607: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2626 - accuracy: 0.9199 - val_loss: 0.1984 - val_accuracy: 0.9394 - 16s/epoch - 38ms/step\n",
            "Epoch 608/1000\n",
            "\n",
            "Epoch 608: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2625 - accuracy: 0.9189 - val_loss: 0.2016 - val_accuracy: 0.9448 - 16s/epoch - 37ms/step\n",
            "Epoch 609/1000\n",
            "\n",
            "Epoch 609: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2625 - accuracy: 0.9197 - val_loss: 0.1821 - val_accuracy: 0.9458 - 16s/epoch - 37ms/step\n",
            "Epoch 610/1000\n",
            "\n",
            "Epoch 610: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2603 - accuracy: 0.9190 - val_loss: 0.2036 - val_accuracy: 0.9420 - 16s/epoch - 38ms/step\n",
            "Epoch 611/1000\n",
            "\n",
            "Epoch 611: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2632 - accuracy: 0.9191 - val_loss: 0.1995 - val_accuracy: 0.9402 - 16s/epoch - 38ms/step\n",
            "Epoch 612/1000\n",
            "\n",
            "Epoch 612: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2592 - accuracy: 0.9204 - val_loss: 0.1870 - val_accuracy: 0.9486 - 17s/epoch - 40ms/step\n",
            "Epoch 613/1000\n",
            "\n",
            "Epoch 613: val_accuracy did not improve from 0.95320\n",
            "429/429 - 19s - loss: 0.2590 - accuracy: 0.9187 - val_loss: 0.1834 - val_accuracy: 0.9448 - 19s/epoch - 43ms/step\n",
            "Epoch 614/1000\n",
            "\n",
            "Epoch 614: val_accuracy did not improve from 0.95320\n",
            "429/429 - 19s - loss: 0.2617 - accuracy: 0.9176 - val_loss: 0.1838 - val_accuracy: 0.9444 - 19s/epoch - 43ms/step\n",
            "Epoch 615/1000\n",
            "\n",
            "Epoch 615: val_accuracy did not improve from 0.95320\n",
            "429/429 - 28s - loss: 0.2595 - accuracy: 0.9192 - val_loss: 0.1900 - val_accuracy: 0.9456 - 28s/epoch - 64ms/step\n",
            "Epoch 616/1000\n",
            "\n",
            "Epoch 616: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2622 - accuracy: 0.9187 - val_loss: 0.2149 - val_accuracy: 0.9328 - 18s/epoch - 42ms/step\n",
            "Epoch 617/1000\n",
            "\n",
            "Epoch 617: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2627 - accuracy: 0.9186 - val_loss: 0.1854 - val_accuracy: 0.9482 - 16s/epoch - 38ms/step\n",
            "Epoch 618/1000\n",
            "\n",
            "Epoch 618: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2626 - accuracy: 0.9190 - val_loss: 0.2006 - val_accuracy: 0.9386 - 16s/epoch - 38ms/step\n",
            "Epoch 619/1000\n",
            "\n",
            "Epoch 619: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2646 - accuracy: 0.9177 - val_loss: 0.1864 - val_accuracy: 0.9460 - 16s/epoch - 37ms/step\n",
            "Epoch 620/1000\n",
            "\n",
            "Epoch 620: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2625 - accuracy: 0.9198 - val_loss: 0.1937 - val_accuracy: 0.9438 - 16s/epoch - 37ms/step\n",
            "Epoch 621/1000\n",
            "\n",
            "Epoch 621: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2606 - accuracy: 0.9187 - val_loss: 0.2283 - val_accuracy: 0.9302 - 16s/epoch - 37ms/step\n",
            "Epoch 622/1000\n",
            "\n",
            "Epoch 622: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2608 - accuracy: 0.9190 - val_loss: 0.1960 - val_accuracy: 0.9420 - 16s/epoch - 37ms/step\n",
            "Epoch 623/1000\n",
            "\n",
            "Epoch 623: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2620 - accuracy: 0.9190 - val_loss: 0.1708 - val_accuracy: 0.9506 - 16s/epoch - 38ms/step\n",
            "Epoch 624/1000\n",
            "\n",
            "Epoch 624: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2605 - accuracy: 0.9194 - val_loss: 0.2036 - val_accuracy: 0.9374 - 16s/epoch - 38ms/step\n",
            "Epoch 625/1000\n",
            "\n",
            "Epoch 625: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2596 - accuracy: 0.9188 - val_loss: 0.1975 - val_accuracy: 0.9402 - 16s/epoch - 37ms/step\n",
            "Epoch 626/1000\n",
            "\n",
            "Epoch 626: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2620 - accuracy: 0.9190 - val_loss: 0.1913 - val_accuracy: 0.9480 - 17s/epoch - 40ms/step\n",
            "Epoch 627/1000\n",
            "\n",
            "Epoch 627: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2633 - accuracy: 0.9178 - val_loss: 0.1954 - val_accuracy: 0.9466 - 17s/epoch - 40ms/step\n",
            "Epoch 628/1000\n",
            "\n",
            "Epoch 628: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2617 - accuracy: 0.9187 - val_loss: 0.1979 - val_accuracy: 0.9464 - 18s/epoch - 42ms/step\n",
            "Epoch 629/1000\n",
            "\n",
            "Epoch 629: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2588 - accuracy: 0.9205 - val_loss: 0.1856 - val_accuracy: 0.9474 - 18s/epoch - 42ms/step\n",
            "Epoch 630/1000\n",
            "\n",
            "Epoch 630: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2629 - accuracy: 0.9186 - val_loss: 0.1951 - val_accuracy: 0.9420 - 16s/epoch - 38ms/step\n",
            "Epoch 631/1000\n",
            "\n",
            "Epoch 631: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2607 - accuracy: 0.9190 - val_loss: 0.2785 - val_accuracy: 0.9130 - 17s/epoch - 39ms/step\n",
            "Epoch 632/1000\n",
            "\n",
            "Epoch 632: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2628 - accuracy: 0.9184 - val_loss: 0.2040 - val_accuracy: 0.9414 - 16s/epoch - 38ms/step\n",
            "Epoch 633/1000\n",
            "\n",
            "Epoch 633: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2610 - accuracy: 0.9198 - val_loss: 0.1989 - val_accuracy: 0.9426 - 16s/epoch - 37ms/step\n",
            "Epoch 634/1000\n",
            "\n",
            "Epoch 634: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2597 - accuracy: 0.9193 - val_loss: 0.1969 - val_accuracy: 0.9414 - 16s/epoch - 37ms/step\n",
            "Epoch 635/1000\n",
            "\n",
            "Epoch 635: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2601 - accuracy: 0.9202 - val_loss: 0.1806 - val_accuracy: 0.9490 - 16s/epoch - 37ms/step\n",
            "Epoch 636/1000\n",
            "\n",
            "Epoch 636: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2638 - accuracy: 0.9188 - val_loss: 0.2050 - val_accuracy: 0.9426 - 16s/epoch - 38ms/step\n",
            "Epoch 637/1000\n",
            "\n",
            "Epoch 637: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2618 - accuracy: 0.9187 - val_loss: 0.1968 - val_accuracy: 0.9402 - 17s/epoch - 40ms/step\n",
            "Epoch 638/1000\n",
            "\n",
            "Epoch 638: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2604 - accuracy: 0.9203 - val_loss: 0.1767 - val_accuracy: 0.9482 - 16s/epoch - 38ms/step\n",
            "Epoch 639/1000\n",
            "\n",
            "Epoch 639: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2590 - accuracy: 0.9195 - val_loss: 0.1759 - val_accuracy: 0.9518 - 18s/epoch - 43ms/step\n",
            "Epoch 640/1000\n",
            "\n",
            "Epoch 640: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2662 - accuracy: 0.9179 - val_loss: 0.1973 - val_accuracy: 0.9458 - 17s/epoch - 40ms/step\n",
            "Epoch 641/1000\n",
            "\n",
            "Epoch 641: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2613 - accuracy: 0.9191 - val_loss: 0.1920 - val_accuracy: 0.9452 - 18s/epoch - 42ms/step\n",
            "Epoch 642/1000\n",
            "\n",
            "Epoch 642: val_accuracy did not improve from 0.95320\n",
            "429/429 - 19s - loss: 0.2634 - accuracy: 0.9194 - val_loss: 0.1889 - val_accuracy: 0.9458 - 19s/epoch - 44ms/step\n",
            "Epoch 643/1000\n",
            "\n",
            "Epoch 643: val_accuracy did not improve from 0.95320\n",
            "429/429 - 21s - loss: 0.2583 - accuracy: 0.9207 - val_loss: 0.1978 - val_accuracy: 0.9422 - 21s/epoch - 49ms/step\n",
            "Epoch 644/1000\n",
            "\n",
            "Epoch 644: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2587 - accuracy: 0.9201 - val_loss: 0.1909 - val_accuracy: 0.9434 - 18s/epoch - 42ms/step\n",
            "Epoch 645/1000\n",
            "\n",
            "Epoch 645: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2580 - accuracy: 0.9196 - val_loss: 0.1865 - val_accuracy: 0.9490 - 16s/epoch - 38ms/step\n",
            "Epoch 646/1000\n",
            "\n",
            "Epoch 646: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2618 - accuracy: 0.9196 - val_loss: 0.1873 - val_accuracy: 0.9460 - 16s/epoch - 38ms/step\n",
            "Epoch 647/1000\n",
            "\n",
            "Epoch 647: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2608 - accuracy: 0.9189 - val_loss: 0.1906 - val_accuracy: 0.9476 - 17s/epoch - 39ms/step\n",
            "Epoch 648/1000\n",
            "\n",
            "Epoch 648: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2593 - accuracy: 0.9201 - val_loss: 0.1920 - val_accuracy: 0.9480 - 16s/epoch - 38ms/step\n",
            "Epoch 649/1000\n",
            "\n",
            "Epoch 649: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2634 - accuracy: 0.9189 - val_loss: 0.1872 - val_accuracy: 0.9458 - 16s/epoch - 38ms/step\n",
            "Epoch 650/1000\n",
            "\n",
            "Epoch 650: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2579 - accuracy: 0.9205 - val_loss: 0.1976 - val_accuracy: 0.9456 - 16s/epoch - 38ms/step\n",
            "Epoch 651/1000\n",
            "\n",
            "Epoch 651: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2603 - accuracy: 0.9190 - val_loss: 0.1853 - val_accuracy: 0.9466 - 16s/epoch - 38ms/step\n",
            "Epoch 652/1000\n",
            "\n",
            "Epoch 652: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2614 - accuracy: 0.9193 - val_loss: 0.1755 - val_accuracy: 0.9506 - 17s/epoch - 39ms/step\n",
            "Epoch 653/1000\n",
            "\n",
            "Epoch 653: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2624 - accuracy: 0.9188 - val_loss: 0.2074 - val_accuracy: 0.9418 - 16s/epoch - 38ms/step\n",
            "Epoch 654/1000\n",
            "\n",
            "Epoch 654: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2630 - accuracy: 0.9188 - val_loss: 0.2083 - val_accuracy: 0.9412 - 17s/epoch - 40ms/step\n",
            "Epoch 655/1000\n",
            "\n",
            "Epoch 655: val_accuracy did not improve from 0.95320\n",
            "429/429 - 20s - loss: 0.2612 - accuracy: 0.9190 - val_loss: 0.1864 - val_accuracy: 0.9468 - 20s/epoch - 46ms/step\n",
            "Epoch 656/1000\n",
            "\n",
            "Epoch 656: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2619 - accuracy: 0.9186 - val_loss: 0.1717 - val_accuracy: 0.9512 - 18s/epoch - 42ms/step\n",
            "Epoch 657/1000\n",
            "\n",
            "Epoch 657: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2600 - accuracy: 0.9187 - val_loss: 0.1912 - val_accuracy: 0.9456 - 18s/epoch - 43ms/step\n",
            "Epoch 658/1000\n",
            "\n",
            "Epoch 658: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2599 - accuracy: 0.9201 - val_loss: 0.1959 - val_accuracy: 0.9458 - 18s/epoch - 41ms/step\n",
            "Epoch 659/1000\n",
            "\n",
            "Epoch 659: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2608 - accuracy: 0.9191 - val_loss: 0.1919 - val_accuracy: 0.9414 - 16s/epoch - 38ms/step\n",
            "Epoch 660/1000\n",
            "\n",
            "Epoch 660: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2608 - accuracy: 0.9202 - val_loss: 0.1831 - val_accuracy: 0.9476 - 17s/epoch - 39ms/step\n",
            "Epoch 661/1000\n",
            "\n",
            "Epoch 661: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2621 - accuracy: 0.9188 - val_loss: 0.2078 - val_accuracy: 0.9350 - 16s/epoch - 37ms/step\n",
            "Epoch 662/1000\n",
            "\n",
            "Epoch 662: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2582 - accuracy: 0.9215 - val_loss: 0.2083 - val_accuracy: 0.9396 - 16s/epoch - 38ms/step\n",
            "Epoch 663/1000\n",
            "\n",
            "Epoch 663: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2563 - accuracy: 0.9218 - val_loss: 0.1946 - val_accuracy: 0.9408 - 16s/epoch - 38ms/step\n",
            "Epoch 664/1000\n",
            "\n",
            "Epoch 664: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2568 - accuracy: 0.9214 - val_loss: 0.2029 - val_accuracy: 0.9436 - 16s/epoch - 38ms/step\n",
            "Epoch 665/1000\n",
            "\n",
            "Epoch 665: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2616 - accuracy: 0.9190 - val_loss: 0.2171 - val_accuracy: 0.9370 - 17s/epoch - 41ms/step\n",
            "Epoch 666/1000\n",
            "\n",
            "Epoch 666: val_accuracy did not improve from 0.95320\n",
            "429/429 - 24s - loss: 0.2606 - accuracy: 0.9177 - val_loss: 0.1869 - val_accuracy: 0.9454 - 24s/epoch - 55ms/step\n",
            "Epoch 667/1000\n",
            "\n",
            "Epoch 667: val_accuracy did not improve from 0.95320\n",
            "429/429 - 19s - loss: 0.2647 - accuracy: 0.9186 - val_loss: 0.1961 - val_accuracy: 0.9448 - 19s/epoch - 45ms/step\n",
            "Epoch 668/1000\n",
            "\n",
            "Epoch 668: val_accuracy did not improve from 0.95320\n",
            "429/429 - 19s - loss: 0.2599 - accuracy: 0.9201 - val_loss: 0.1910 - val_accuracy: 0.9438 - 19s/epoch - 45ms/step\n",
            "Epoch 669/1000\n",
            "\n",
            "Epoch 669: val_accuracy did not improve from 0.95320\n",
            "429/429 - 22s - loss: 0.2613 - accuracy: 0.9202 - val_loss: 0.2088 - val_accuracy: 0.9440 - 22s/epoch - 50ms/step\n",
            "Epoch 670/1000\n",
            "\n",
            "Epoch 670: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2611 - accuracy: 0.9188 - val_loss: 0.1970 - val_accuracy: 0.9436 - 17s/epoch - 40ms/step\n",
            "Epoch 671/1000\n",
            "\n",
            "Epoch 671: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2622 - accuracy: 0.9194 - val_loss: 0.2014 - val_accuracy: 0.9404 - 18s/epoch - 41ms/step\n",
            "Epoch 672/1000\n",
            "\n",
            "Epoch 672: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2572 - accuracy: 0.9211 - val_loss: 0.1937 - val_accuracy: 0.9430 - 16s/epoch - 38ms/step\n",
            "Epoch 673/1000\n",
            "\n",
            "Epoch 673: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2626 - accuracy: 0.9186 - val_loss: 0.1913 - val_accuracy: 0.9484 - 16s/epoch - 38ms/step\n",
            "Epoch 674/1000\n",
            "\n",
            "Epoch 674: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2628 - accuracy: 0.9188 - val_loss: 0.1933 - val_accuracy: 0.9440 - 16s/epoch - 37ms/step\n",
            "Epoch 675/1000\n",
            "\n",
            "Epoch 675: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2584 - accuracy: 0.9216 - val_loss: 0.2046 - val_accuracy: 0.9420 - 16s/epoch - 37ms/step\n",
            "Epoch 676/1000\n",
            "\n",
            "Epoch 676: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2587 - accuracy: 0.9209 - val_loss: 0.2348 - val_accuracy: 0.9312 - 16s/epoch - 38ms/step\n",
            "Epoch 677/1000\n",
            "\n",
            "Epoch 677: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2615 - accuracy: 0.9193 - val_loss: 0.2187 - val_accuracy: 0.9358 - 16s/epoch - 38ms/step\n",
            "Epoch 678/1000\n",
            "\n",
            "Epoch 678: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2592 - accuracy: 0.9200 - val_loss: 0.2085 - val_accuracy: 0.9374 - 16s/epoch - 38ms/step\n",
            "Epoch 679/1000\n",
            "\n",
            "Epoch 679: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2609 - accuracy: 0.9203 - val_loss: 0.1936 - val_accuracy: 0.9482 - 17s/epoch - 39ms/step\n",
            "Epoch 680/1000\n",
            "\n",
            "Epoch 680: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2600 - accuracy: 0.9207 - val_loss: 0.1923 - val_accuracy: 0.9446 - 16s/epoch - 38ms/step\n",
            "Epoch 681/1000\n",
            "\n",
            "Epoch 681: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2620 - accuracy: 0.9183 - val_loss: 0.1929 - val_accuracy: 0.9462 - 16s/epoch - 38ms/step\n",
            "Epoch 682/1000\n",
            "\n",
            "Epoch 682: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2578 - accuracy: 0.9199 - val_loss: 0.1948 - val_accuracy: 0.9456 - 17s/epoch - 39ms/step\n",
            "Epoch 683/1000\n",
            "\n",
            "Epoch 683: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2625 - accuracy: 0.9178 - val_loss: 0.1787 - val_accuracy: 0.9532 - 16s/epoch - 38ms/step\n",
            "Epoch 684/1000\n",
            "\n",
            "Epoch 684: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2614 - accuracy: 0.9186 - val_loss: 0.1940 - val_accuracy: 0.9444 - 17s/epoch - 41ms/step\n",
            "Epoch 685/1000\n",
            "\n",
            "Epoch 685: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2618 - accuracy: 0.9186 - val_loss: 0.1976 - val_accuracy: 0.9430 - 17s/epoch - 40ms/step\n",
            "Epoch 686/1000\n",
            "\n",
            "Epoch 686: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2635 - accuracy: 0.9187 - val_loss: 0.1902 - val_accuracy: 0.9450 - 17s/epoch - 39ms/step\n",
            "Epoch 687/1000\n",
            "\n",
            "Epoch 687: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2578 - accuracy: 0.9198 - val_loss: 0.1783 - val_accuracy: 0.9528 - 16s/epoch - 37ms/step\n",
            "Epoch 688/1000\n",
            "\n",
            "Epoch 688: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2580 - accuracy: 0.9195 - val_loss: 0.1942 - val_accuracy: 0.9430 - 16s/epoch - 37ms/step\n",
            "Epoch 689/1000\n",
            "\n",
            "Epoch 689: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2585 - accuracy: 0.9205 - val_loss: 0.1964 - val_accuracy: 0.9436 - 16s/epoch - 37ms/step\n",
            "Epoch 690/1000\n",
            "\n",
            "Epoch 690: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2595 - accuracy: 0.9205 - val_loss: 0.2295 - val_accuracy: 0.9310 - 16s/epoch - 37ms/step\n",
            "Epoch 691/1000\n",
            "\n",
            "Epoch 691: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2597 - accuracy: 0.9198 - val_loss: 0.2273 - val_accuracy: 0.9366 - 16s/epoch - 37ms/step\n",
            "Epoch 692/1000\n",
            "\n",
            "Epoch 692: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2587 - accuracy: 0.9196 - val_loss: 0.2064 - val_accuracy: 0.9392 - 16s/epoch - 37ms/step\n",
            "Epoch 693/1000\n",
            "\n",
            "Epoch 693: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2568 - accuracy: 0.9196 - val_loss: 0.1985 - val_accuracy: 0.9420 - 16s/epoch - 37ms/step\n",
            "Epoch 694/1000\n",
            "\n",
            "Epoch 694: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2592 - accuracy: 0.9198 - val_loss: 0.1812 - val_accuracy: 0.9508 - 16s/epoch - 37ms/step\n",
            "Epoch 695/1000\n",
            "\n",
            "Epoch 695: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2609 - accuracy: 0.9204 - val_loss: 0.1969 - val_accuracy: 0.9416 - 16s/epoch - 38ms/step\n",
            "Epoch 696/1000\n",
            "\n",
            "Epoch 696: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2594 - accuracy: 0.9194 - val_loss: 0.1938 - val_accuracy: 0.9460 - 16s/epoch - 37ms/step\n",
            "Epoch 697/1000\n",
            "\n",
            "Epoch 697: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2567 - accuracy: 0.9207 - val_loss: 0.1962 - val_accuracy: 0.9402 - 17s/epoch - 38ms/step\n",
            "Epoch 698/1000\n",
            "\n",
            "Epoch 698: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2622 - accuracy: 0.9191 - val_loss: 0.2166 - val_accuracy: 0.9360 - 16s/epoch - 38ms/step\n",
            "Epoch 699/1000\n",
            "\n",
            "Epoch 699: val_accuracy did not improve from 0.95320\n",
            "429/429 - 18s - loss: 0.2594 - accuracy: 0.9201 - val_loss: 0.1946 - val_accuracy: 0.9452 - 18s/epoch - 41ms/step\n",
            "Epoch 700/1000\n",
            "\n",
            "Epoch 700: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2605 - accuracy: 0.9190 - val_loss: 0.2225 - val_accuracy: 0.9336 - 17s/epoch - 40ms/step\n",
            "Epoch 701/1000\n",
            "\n",
            "Epoch 701: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2592 - accuracy: 0.9194 - val_loss: 0.1986 - val_accuracy: 0.9434 - 16s/epoch - 37ms/step\n",
            "Epoch 702/1000\n",
            "\n",
            "Epoch 702: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2580 - accuracy: 0.9203 - val_loss: 0.1774 - val_accuracy: 0.9508 - 16s/epoch - 37ms/step\n",
            "Epoch 703/1000\n",
            "\n",
            "Epoch 703: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2611 - accuracy: 0.9192 - val_loss: 0.2011 - val_accuracy: 0.9420 - 16s/epoch - 36ms/step\n",
            "Epoch 704/1000\n",
            "\n",
            "Epoch 704: val_accuracy did not improve from 0.95320\n",
            "429/429 - 15s - loss: 0.2590 - accuracy: 0.9208 - val_loss: 0.2048 - val_accuracy: 0.9428 - 15s/epoch - 36ms/step\n",
            "Epoch 705/1000\n",
            "\n",
            "Epoch 705: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2618 - accuracy: 0.9192 - val_loss: 0.2006 - val_accuracy: 0.9406 - 16s/epoch - 37ms/step\n",
            "Epoch 706/1000\n",
            "\n",
            "Epoch 706: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2622 - accuracy: 0.9198 - val_loss: 0.1956 - val_accuracy: 0.9450 - 16s/epoch - 37ms/step\n",
            "Epoch 707/1000\n",
            "\n",
            "Epoch 707: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2602 - accuracy: 0.9196 - val_loss: 0.2044 - val_accuracy: 0.9396 - 16s/epoch - 37ms/step\n",
            "Epoch 708/1000\n",
            "\n",
            "Epoch 708: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2582 - accuracy: 0.9200 - val_loss: 0.1973 - val_accuracy: 0.9442 - 16s/epoch - 38ms/step\n",
            "Epoch 709/1000\n",
            "\n",
            "Epoch 709: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2635 - accuracy: 0.9181 - val_loss: 0.1893 - val_accuracy: 0.9460 - 16s/epoch - 37ms/step\n",
            "Epoch 710/1000\n",
            "\n",
            "Epoch 710: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2591 - accuracy: 0.9199 - val_loss: 0.2061 - val_accuracy: 0.9418 - 16s/epoch - 38ms/step\n",
            "Epoch 711/1000\n",
            "\n",
            "Epoch 711: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2580 - accuracy: 0.9198 - val_loss: 0.2252 - val_accuracy: 0.9336 - 17s/epoch - 41ms/step\n",
            "Epoch 712/1000\n",
            "\n",
            "Epoch 712: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2611 - accuracy: 0.9194 - val_loss: 0.1865 - val_accuracy: 0.9442 - 17s/epoch - 40ms/step\n",
            "Epoch 713/1000\n",
            "\n",
            "Epoch 713: val_accuracy did not improve from 0.95320\n",
            "429/429 - 19s - loss: 0.2612 - accuracy: 0.9205 - val_loss: 0.2092 - val_accuracy: 0.9378 - 19s/epoch - 45ms/step\n",
            "Epoch 714/1000\n",
            "\n",
            "Epoch 714: val_accuracy did not improve from 0.95320\n",
            "429/429 - 22s - loss: 0.2596 - accuracy: 0.9197 - val_loss: 0.2057 - val_accuracy: 0.9372 - 22s/epoch - 51ms/step\n",
            "Epoch 715/1000\n",
            "\n",
            "Epoch 715: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2595 - accuracy: 0.9198 - val_loss: 0.2032 - val_accuracy: 0.9398 - 17s/epoch - 39ms/step\n",
            "Epoch 716/1000\n",
            "\n",
            "Epoch 716: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2609 - accuracy: 0.9203 - val_loss: 0.2093 - val_accuracy: 0.9374 - 16s/epoch - 37ms/step\n",
            "Epoch 717/1000\n",
            "\n",
            "Epoch 717: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2576 - accuracy: 0.9205 - val_loss: 0.2058 - val_accuracy: 0.9400 - 16s/epoch - 37ms/step\n",
            "Epoch 718/1000\n",
            "\n",
            "Epoch 718: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2617 - accuracy: 0.9204 - val_loss: 0.1765 - val_accuracy: 0.9518 - 16s/epoch - 38ms/step\n",
            "Epoch 719/1000\n",
            "\n",
            "Epoch 719: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2581 - accuracy: 0.9203 - val_loss: 0.1836 - val_accuracy: 0.9480 - 16s/epoch - 37ms/step\n",
            "Epoch 720/1000\n",
            "\n",
            "Epoch 720: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2573 - accuracy: 0.9199 - val_loss: 0.2053 - val_accuracy: 0.9396 - 16s/epoch - 37ms/step\n",
            "Epoch 721/1000\n",
            "\n",
            "Epoch 721: val_accuracy did not improve from 0.95320\n",
            "429/429 - 17s - loss: 0.2610 - accuracy: 0.9181 - val_loss: 0.1974 - val_accuracy: 0.9476 - 17s/epoch - 39ms/step\n",
            "Epoch 722/1000\n",
            "\n",
            "Epoch 722: val_accuracy did not improve from 0.95320\n",
            "429/429 - 16s - loss: 0.2600 - accuracy: 0.9190 - val_loss: 0.1862 - val_accuracy: 0.9468 - 16s/epoch - 38ms/step\n",
            "Epoch 723/1000\n",
            "\n",
            "Epoch 723: val_accuracy did not improve from 0.95320\n",
            "429/429 - 19s - loss: 0.2613 - accuracy: 0.9190 - val_loss: 0.1817 - val_accuracy: 0.9476 - 19s/epoch - 44ms/step\n",
            "Epoch 724/1000\n",
            "\n",
            "Epoch 724: val_accuracy did not improve from 0.95320\n",
            "429/429 - 19s - loss: 0.2597 - accuracy: 0.9200 - val_loss: 0.1901 - val_accuracy: 0.9464 - 19s/epoch - 44ms/step\n",
            "Epoch 725/1000\n",
            "\n",
            "Epoch 725: val_accuracy did not improve from 0.95320\n",
            "429/429 - 21s - loss: 0.2628 - accuracy: 0.9203 - val_loss: 0.1888 - val_accuracy: 0.9450 - 21s/epoch - 50ms/step\n",
            "Epoch 726/1000\n",
            "\n",
            "Epoch 726: val_accuracy did not improve from 0.95320\n",
            "429/429 - 23s - loss: 0.2618 - accuracy: 0.9193 - val_loss: 0.1892 - val_accuracy: 0.9404 - 23s/epoch - 54ms/step\n",
            "Epoch 727/1000\n",
            "\n",
            "Epoch 727: val_accuracy did not improve from 0.95320\n",
            "429/429 - 24s - loss: 0.2620 - accuracy: 0.9180 - val_loss: 0.1819 - val_accuracy: 0.9504 - 24s/epoch - 56ms/step\n",
            "Epoch 728/1000\n",
            "\n",
            "Epoch 728: val_accuracy did not improve from 0.95320\n",
            "429/429 - 23s - loss: 0.2620 - accuracy: 0.9180 - val_loss: 0.2095 - val_accuracy: 0.9384 - 23s/epoch - 53ms/step\n",
            "Epoch 729/1000\n",
            "\n",
            "Epoch 729: val_accuracy improved from 0.95320 to 0.95380, saving model to best_model,keras\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "429/429 - 21s - loss: 0.2571 - accuracy: 0.9216 - val_loss: 0.1739 - val_accuracy: 0.9538 - 21s/epoch - 49ms/step\n",
            "Epoch 730/1000\n",
            "\n",
            "Epoch 730: val_accuracy did not improve from 0.95380\n",
            "429/429 - 17s - loss: 0.2601 - accuracy: 0.9189 - val_loss: 0.2101 - val_accuracy: 0.9370 - 17s/epoch - 40ms/step\n",
            "Epoch 731/1000\n",
            "\n",
            "Epoch 731: val_accuracy did not improve from 0.95380\n",
            "429/429 - 16s - loss: 0.2598 - accuracy: 0.9203 - val_loss: 0.1957 - val_accuracy: 0.9428 - 16s/epoch - 37ms/step\n",
            "Epoch 732/1000\n",
            "\n",
            "Epoch 732: val_accuracy did not improve from 0.95380\n",
            "429/429 - 16s - loss: 0.2596 - accuracy: 0.9194 - val_loss: 0.2059 - val_accuracy: 0.9376 - 16s/epoch - 38ms/step\n",
            "Epoch 733/1000\n",
            "\n",
            "Epoch 733: val_accuracy did not improve from 0.95380\n",
            "429/429 - 16s - loss: 0.2604 - accuracy: 0.9195 - val_loss: 0.1876 - val_accuracy: 0.9430 - 16s/epoch - 38ms/step\n",
            "Epoch 734/1000\n",
            "\n",
            "Epoch 734: val_accuracy did not improve from 0.95380\n",
            "429/429 - 17s - loss: 0.2586 - accuracy: 0.9212 - val_loss: 0.2093 - val_accuracy: 0.9386 - 17s/epoch - 39ms/step\n",
            "Epoch 735/1000\n",
            "\n",
            "Epoch 735: val_accuracy did not improve from 0.95380\n",
            "429/429 - 17s - loss: 0.2589 - accuracy: 0.9191 - val_loss: 0.1867 - val_accuracy: 0.9444 - 17s/epoch - 40ms/step\n",
            "Epoch 736/1000\n",
            "\n",
            "Epoch 736: val_accuracy did not improve from 0.95380\n",
            "429/429 - 19s - loss: 0.2570 - accuracy: 0.9205 - val_loss: 0.1854 - val_accuracy: 0.9480 - 19s/epoch - 43ms/step\n",
            "Epoch 737/1000\n",
            "\n",
            "Epoch 737: val_accuracy did not improve from 0.95380\n",
            "429/429 - 17s - loss: 0.2587 - accuracy: 0.9203 - val_loss: 0.1755 - val_accuracy: 0.9502 - 17s/epoch - 40ms/step\n",
            "Epoch 738/1000\n",
            "\n",
            "Epoch 738: val_accuracy did not improve from 0.95380\n",
            "429/429 - 16s - loss: 0.2637 - accuracy: 0.9184 - val_loss: 0.1888 - val_accuracy: 0.9478 - 16s/epoch - 38ms/step\n",
            "Epoch 739/1000\n",
            "\n",
            "Epoch 739: val_accuracy did not improve from 0.95380\n",
            "429/429 - 17s - loss: 0.2591 - accuracy: 0.9203 - val_loss: 0.1850 - val_accuracy: 0.9476 - 17s/epoch - 39ms/step\n",
            "Epoch 740/1000\n",
            "\n",
            "Epoch 740: val_accuracy did not improve from 0.95380\n",
            "429/429 - 17s - loss: 0.2578 - accuracy: 0.9200 - val_loss: 0.2064 - val_accuracy: 0.9396 - 17s/epoch - 40ms/step\n",
            "Epoch 741/1000\n",
            "\n",
            "Epoch 741: val_accuracy did not improve from 0.95380\n",
            "429/429 - 21s - loss: 0.2601 - accuracy: 0.9186 - val_loss: 0.1970 - val_accuracy: 0.9470 - 21s/epoch - 49ms/step\n",
            "Epoch 742/1000\n",
            "\n",
            "Epoch 742: val_accuracy did not improve from 0.95380\n",
            "429/429 - 16s - loss: 0.2598 - accuracy: 0.9200 - val_loss: 0.1834 - val_accuracy: 0.9480 - 16s/epoch - 38ms/step\n",
            "Epoch 743/1000\n",
            "\n",
            "Epoch 743: val_accuracy did not improve from 0.95380\n",
            "429/429 - 16s - loss: 0.2620 - accuracy: 0.9180 - val_loss: 0.1879 - val_accuracy: 0.9470 - 16s/epoch - 38ms/step\n",
            "Epoch 744/1000\n",
            "\n",
            "Epoch 744: val_accuracy did not improve from 0.95380\n",
            "429/429 - 16s - loss: 0.2615 - accuracy: 0.9187 - val_loss: 0.1929 - val_accuracy: 0.9448 - 16s/epoch - 38ms/step\n",
            "Epoch 745/1000\n",
            "\n",
            "Epoch 745: val_accuracy improved from 0.95380 to 0.95540, saving model to best_model,keras\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "429/429 - 17s - loss: 0.2599 - accuracy: 0.9201 - val_loss: 0.1696 - val_accuracy: 0.9554 - 17s/epoch - 40ms/step\n",
            "Epoch 746/1000\n",
            "\n",
            "Epoch 746: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2623 - accuracy: 0.9182 - val_loss: 0.1909 - val_accuracy: 0.9468 - 17s/epoch - 39ms/step\n",
            "Epoch 747/1000\n",
            "\n",
            "Epoch 747: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2583 - accuracy: 0.9201 - val_loss: 0.1924 - val_accuracy: 0.9476 - 16s/epoch - 37ms/step\n",
            "Epoch 748/1000\n",
            "\n",
            "Epoch 748: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2585 - accuracy: 0.9209 - val_loss: 0.1862 - val_accuracy: 0.9486 - 17s/epoch - 39ms/step\n",
            "Epoch 749/1000\n",
            "\n",
            "Epoch 749: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2603 - accuracy: 0.9203 - val_loss: 0.1923 - val_accuracy: 0.9424 - 16s/epoch - 37ms/step\n",
            "Epoch 750/1000\n",
            "\n",
            "Epoch 750: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2610 - accuracy: 0.9193 - val_loss: 0.2045 - val_accuracy: 0.9408 - 16s/epoch - 38ms/step\n",
            "Epoch 751/1000\n",
            "\n",
            "Epoch 751: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2602 - accuracy: 0.9202 - val_loss: 0.1982 - val_accuracy: 0.9410 - 17s/epoch - 39ms/step\n",
            "Epoch 752/1000\n",
            "\n",
            "Epoch 752: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2614 - accuracy: 0.9193 - val_loss: 0.1770 - val_accuracy: 0.9522 - 17s/epoch - 39ms/step\n",
            "Epoch 753/1000\n",
            "\n",
            "Epoch 753: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2580 - accuracy: 0.9192 - val_loss: 0.1885 - val_accuracy: 0.9468 - 17s/epoch - 39ms/step\n",
            "Epoch 754/1000\n",
            "\n",
            "Epoch 754: val_accuracy did not improve from 0.95540\n",
            "429/429 - 18s - loss: 0.2634 - accuracy: 0.9192 - val_loss: 0.1921 - val_accuracy: 0.9460 - 18s/epoch - 42ms/step\n",
            "Epoch 755/1000\n",
            "\n",
            "Epoch 755: val_accuracy did not improve from 0.95540\n",
            "429/429 - 21s - loss: 0.2582 - accuracy: 0.9211 - val_loss: 0.1970 - val_accuracy: 0.9430 - 21s/epoch - 50ms/step\n",
            "Epoch 756/1000\n",
            "\n",
            "Epoch 756: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2603 - accuracy: 0.9195 - val_loss: 0.1918 - val_accuracy: 0.9410 - 16s/epoch - 38ms/step\n",
            "Epoch 757/1000\n",
            "\n",
            "Epoch 757: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2610 - accuracy: 0.9192 - val_loss: 0.1931 - val_accuracy: 0.9450 - 16s/epoch - 37ms/step\n",
            "Epoch 758/1000\n",
            "\n",
            "Epoch 758: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2581 - accuracy: 0.9196 - val_loss: 0.1885 - val_accuracy: 0.9444 - 16s/epoch - 37ms/step\n",
            "Epoch 759/1000\n",
            "\n",
            "Epoch 759: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2579 - accuracy: 0.9202 - val_loss: 0.1911 - val_accuracy: 0.9440 - 16s/epoch - 38ms/step\n",
            "Epoch 760/1000\n",
            "\n",
            "Epoch 760: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2605 - accuracy: 0.9197 - val_loss: 0.1997 - val_accuracy: 0.9438 - 16s/epoch - 37ms/step\n",
            "Epoch 761/1000\n",
            "\n",
            "Epoch 761: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2607 - accuracy: 0.9196 - val_loss: 0.2020 - val_accuracy: 0.9434 - 16s/epoch - 37ms/step\n",
            "Epoch 762/1000\n",
            "\n",
            "Epoch 762: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2604 - accuracy: 0.9196 - val_loss: 0.1852 - val_accuracy: 0.9462 - 16s/epoch - 37ms/step\n",
            "Epoch 763/1000\n",
            "\n",
            "Epoch 763: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2595 - accuracy: 0.9192 - val_loss: 0.1929 - val_accuracy: 0.9454 - 16s/epoch - 38ms/step\n",
            "Epoch 764/1000\n",
            "\n",
            "Epoch 764: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2592 - accuracy: 0.9203 - val_loss: 0.1725 - val_accuracy: 0.9544 - 16s/epoch - 38ms/step\n",
            "Epoch 765/1000\n",
            "\n",
            "Epoch 765: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2594 - accuracy: 0.9207 - val_loss: 0.1784 - val_accuracy: 0.9480 - 16s/epoch - 38ms/step\n",
            "Epoch 766/1000\n",
            "\n",
            "Epoch 766: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2633 - accuracy: 0.9197 - val_loss: 0.1883 - val_accuracy: 0.9454 - 16s/epoch - 38ms/step\n",
            "Epoch 767/1000\n",
            "\n",
            "Epoch 767: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2588 - accuracy: 0.9201 - val_loss: 0.1859 - val_accuracy: 0.9464 - 17s/epoch - 38ms/step\n",
            "Epoch 768/1000\n",
            "\n",
            "Epoch 768: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2611 - accuracy: 0.9184 - val_loss: 0.1845 - val_accuracy: 0.9460 - 17s/epoch - 39ms/step\n",
            "Epoch 769/1000\n",
            "\n",
            "Epoch 769: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2606 - accuracy: 0.9187 - val_loss: 0.1870 - val_accuracy: 0.9448 - 17s/epoch - 39ms/step\n",
            "Epoch 770/1000\n",
            "\n",
            "Epoch 770: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2594 - accuracy: 0.9198 - val_loss: 0.2155 - val_accuracy: 0.9402 - 17s/epoch - 39ms/step\n",
            "Epoch 771/1000\n",
            "\n",
            "Epoch 771: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2605 - accuracy: 0.9197 - val_loss: 0.1932 - val_accuracy: 0.9456 - 16s/epoch - 38ms/step\n",
            "Epoch 772/1000\n",
            "\n",
            "Epoch 772: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2587 - accuracy: 0.9209 - val_loss: 0.2065 - val_accuracy: 0.9404 - 16s/epoch - 38ms/step\n",
            "Epoch 773/1000\n",
            "\n",
            "Epoch 773: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2617 - accuracy: 0.9201 - val_loss: 0.1810 - val_accuracy: 0.9484 - 16s/epoch - 37ms/step\n",
            "Epoch 774/1000\n",
            "\n",
            "Epoch 774: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2577 - accuracy: 0.9201 - val_loss: 0.1877 - val_accuracy: 0.9482 - 16s/epoch - 37ms/step\n",
            "Epoch 775/1000\n",
            "\n",
            "Epoch 775: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2603 - accuracy: 0.9196 - val_loss: 0.1968 - val_accuracy: 0.9464 - 16s/epoch - 37ms/step\n",
            "Epoch 776/1000\n",
            "\n",
            "Epoch 776: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2600 - accuracy: 0.9192 - val_loss: 0.2022 - val_accuracy: 0.9410 - 16s/epoch - 37ms/step\n",
            "Epoch 777/1000\n",
            "\n",
            "Epoch 777: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2594 - accuracy: 0.9200 - val_loss: 0.1719 - val_accuracy: 0.9520 - 16s/epoch - 37ms/step\n",
            "Epoch 778/1000\n",
            "\n",
            "Epoch 778: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2584 - accuracy: 0.9206 - val_loss: 0.1932 - val_accuracy: 0.9410 - 16s/epoch - 38ms/step\n",
            "Epoch 779/1000\n",
            "\n",
            "Epoch 779: val_accuracy did not improve from 0.95540\n",
            "429/429 - 18s - loss: 0.2604 - accuracy: 0.9190 - val_loss: 0.1968 - val_accuracy: 0.9428 - 18s/epoch - 42ms/step\n",
            "Epoch 780/1000\n",
            "\n",
            "Epoch 780: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2612 - accuracy: 0.9204 - val_loss: 0.2104 - val_accuracy: 0.9414 - 16s/epoch - 38ms/step\n",
            "Epoch 781/1000\n",
            "\n",
            "Epoch 781: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2600 - accuracy: 0.9203 - val_loss: 0.2082 - val_accuracy: 0.9402 - 17s/epoch - 40ms/step\n",
            "Epoch 782/1000\n",
            "\n",
            "Epoch 782: val_accuracy did not improve from 0.95540\n",
            "429/429 - 18s - loss: 0.2611 - accuracy: 0.9189 - val_loss: 0.2001 - val_accuracy: 0.9406 - 18s/epoch - 41ms/step\n",
            "Epoch 783/1000\n",
            "\n",
            "Epoch 783: val_accuracy did not improve from 0.95540\n",
            "429/429 - 18s - loss: 0.2586 - accuracy: 0.9201 - val_loss: 0.1979 - val_accuracy: 0.9468 - 18s/epoch - 42ms/step\n",
            "Epoch 784/1000\n",
            "\n",
            "Epoch 784: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2595 - accuracy: 0.9198 - val_loss: 0.2081 - val_accuracy: 0.9414 - 17s/epoch - 39ms/step\n",
            "Epoch 785/1000\n",
            "\n",
            "Epoch 785: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2613 - accuracy: 0.9184 - val_loss: 0.1941 - val_accuracy: 0.9436 - 16s/epoch - 38ms/step\n",
            "Epoch 786/1000\n",
            "\n",
            "Epoch 786: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2585 - accuracy: 0.9204 - val_loss: 0.2022 - val_accuracy: 0.9376 - 16s/epoch - 37ms/step\n",
            "Epoch 787/1000\n",
            "\n",
            "Epoch 787: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2621 - accuracy: 0.9184 - val_loss: 0.2131 - val_accuracy: 0.9360 - 16s/epoch - 37ms/step\n",
            "Epoch 788/1000\n",
            "\n",
            "Epoch 788: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2649 - accuracy: 0.9178 - val_loss: 0.2065 - val_accuracy: 0.9404 - 16s/epoch - 37ms/step\n",
            "Epoch 789/1000\n",
            "\n",
            "Epoch 789: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2589 - accuracy: 0.9197 - val_loss: 0.1988 - val_accuracy: 0.9420 - 16s/epoch - 37ms/step\n",
            "Epoch 790/1000\n",
            "\n",
            "Epoch 790: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2563 - accuracy: 0.9209 - val_loss: 0.2014 - val_accuracy: 0.9432 - 16s/epoch - 37ms/step\n",
            "Epoch 791/1000\n",
            "\n",
            "Epoch 791: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2578 - accuracy: 0.9210 - val_loss: 0.1854 - val_accuracy: 0.9484 - 16s/epoch - 38ms/step\n",
            "Epoch 792/1000\n",
            "\n",
            "Epoch 792: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2603 - accuracy: 0.9194 - val_loss: 0.2108 - val_accuracy: 0.9378 - 17s/epoch - 39ms/step\n",
            "Epoch 793/1000\n",
            "\n",
            "Epoch 793: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2590 - accuracy: 0.9189 - val_loss: 0.1885 - val_accuracy: 0.9442 - 16s/epoch - 38ms/step\n",
            "Epoch 794/1000\n",
            "\n",
            "Epoch 794: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2613 - accuracy: 0.9185 - val_loss: 0.1953 - val_accuracy: 0.9444 - 16s/epoch - 37ms/step\n",
            "Epoch 795/1000\n",
            "\n",
            "Epoch 795: val_accuracy did not improve from 0.95540\n",
            "429/429 - 18s - loss: 0.2574 - accuracy: 0.9201 - val_loss: 0.2376 - val_accuracy: 0.9278 - 18s/epoch - 41ms/step\n",
            "Epoch 796/1000\n",
            "\n",
            "Epoch 796: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2586 - accuracy: 0.9197 - val_loss: 0.1866 - val_accuracy: 0.9460 - 17s/epoch - 40ms/step\n",
            "Epoch 797/1000\n",
            "\n",
            "Epoch 797: val_accuracy did not improve from 0.95540\n",
            "429/429 - 19s - loss: 0.2609 - accuracy: 0.9193 - val_loss: 0.2256 - val_accuracy: 0.9346 - 19s/epoch - 45ms/step\n",
            "Epoch 798/1000\n",
            "\n",
            "Epoch 798: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2624 - accuracy: 0.9191 - val_loss: 0.1874 - val_accuracy: 0.9454 - 17s/epoch - 39ms/step\n",
            "Epoch 799/1000\n",
            "\n",
            "Epoch 799: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2604 - accuracy: 0.9205 - val_loss: 0.2073 - val_accuracy: 0.9412 - 16s/epoch - 37ms/step\n",
            "Epoch 800/1000\n",
            "\n",
            "Epoch 800: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2629 - accuracy: 0.9181 - val_loss: 0.1877 - val_accuracy: 0.9430 - 16s/epoch - 38ms/step\n",
            "Epoch 801/1000\n",
            "\n",
            "Epoch 801: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2605 - accuracy: 0.9196 - val_loss: 0.1959 - val_accuracy: 0.9436 - 16s/epoch - 38ms/step\n",
            "Epoch 802/1000\n",
            "\n",
            "Epoch 802: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2607 - accuracy: 0.9198 - val_loss: 0.1952 - val_accuracy: 0.9446 - 16s/epoch - 38ms/step\n",
            "Epoch 803/1000\n",
            "\n",
            "Epoch 803: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2601 - accuracy: 0.9202 - val_loss: 0.2162 - val_accuracy: 0.9350 - 16s/epoch - 38ms/step\n",
            "Epoch 804/1000\n",
            "\n",
            "Epoch 804: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2575 - accuracy: 0.9202 - val_loss: 0.1881 - val_accuracy: 0.9454 - 16s/epoch - 38ms/step\n",
            "Epoch 805/1000\n",
            "\n",
            "Epoch 805: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2621 - accuracy: 0.9195 - val_loss: 0.1920 - val_accuracy: 0.9472 - 17s/epoch - 39ms/step\n",
            "Epoch 806/1000\n",
            "\n",
            "Epoch 806: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2626 - accuracy: 0.9194 - val_loss: 0.2144 - val_accuracy: 0.9394 - 17s/epoch - 39ms/step\n",
            "Epoch 807/1000\n",
            "\n",
            "Epoch 807: val_accuracy did not improve from 0.95540\n",
            "429/429 - 19s - loss: 0.2599 - accuracy: 0.9187 - val_loss: 0.1980 - val_accuracy: 0.9424 - 19s/epoch - 43ms/step\n",
            "Epoch 808/1000\n",
            "\n",
            "Epoch 808: val_accuracy did not improve from 0.95540\n",
            "429/429 - 19s - loss: 0.2578 - accuracy: 0.9197 - val_loss: 0.2009 - val_accuracy: 0.9390 - 19s/epoch - 45ms/step\n",
            "Epoch 809/1000\n",
            "\n",
            "Epoch 809: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2625 - accuracy: 0.9194 - val_loss: 0.1952 - val_accuracy: 0.9460 - 17s/epoch - 40ms/step\n",
            "Epoch 810/1000\n",
            "\n",
            "Epoch 810: val_accuracy did not improve from 0.95540\n",
            "429/429 - 20s - loss: 0.2626 - accuracy: 0.9179 - val_loss: 0.1951 - val_accuracy: 0.9428 - 20s/epoch - 48ms/step\n",
            "Epoch 811/1000\n",
            "\n",
            "Epoch 811: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2564 - accuracy: 0.9201 - val_loss: 0.1942 - val_accuracy: 0.9454 - 17s/epoch - 40ms/step\n",
            "Epoch 812/1000\n",
            "\n",
            "Epoch 812: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2581 - accuracy: 0.9210 - val_loss: 0.2123 - val_accuracy: 0.9392 - 16s/epoch - 38ms/step\n",
            "Epoch 813/1000\n",
            "\n",
            "Epoch 813: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2644 - accuracy: 0.9183 - val_loss: 0.1884 - val_accuracy: 0.9488 - 17s/epoch - 39ms/step\n",
            "Epoch 814/1000\n",
            "\n",
            "Epoch 814: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2607 - accuracy: 0.9205 - val_loss: 0.2094 - val_accuracy: 0.9376 - 16s/epoch - 37ms/step\n",
            "Epoch 815/1000\n",
            "\n",
            "Epoch 815: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2602 - accuracy: 0.9197 - val_loss: 0.1857 - val_accuracy: 0.9480 - 16s/epoch - 37ms/step\n",
            "Epoch 816/1000\n",
            "\n",
            "Epoch 816: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2608 - accuracy: 0.9213 - val_loss: 0.1955 - val_accuracy: 0.9446 - 16s/epoch - 37ms/step\n",
            "Epoch 817/1000\n",
            "\n",
            "Epoch 817: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2598 - accuracy: 0.9204 - val_loss: 0.2026 - val_accuracy: 0.9394 - 16s/epoch - 38ms/step\n",
            "Epoch 818/1000\n",
            "\n",
            "Epoch 818: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2586 - accuracy: 0.9211 - val_loss: 0.2023 - val_accuracy: 0.9394 - 16s/epoch - 38ms/step\n",
            "Epoch 819/1000\n",
            "\n",
            "Epoch 819: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2590 - accuracy: 0.9200 - val_loss: 0.2065 - val_accuracy: 0.9408 - 16s/epoch - 37ms/step\n",
            "Epoch 820/1000\n",
            "\n",
            "Epoch 820: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2640 - accuracy: 0.9174 - val_loss: 0.1996 - val_accuracy: 0.9396 - 16s/epoch - 37ms/step\n",
            "Epoch 821/1000\n",
            "\n",
            "Epoch 821: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2582 - accuracy: 0.9205 - val_loss: 0.1894 - val_accuracy: 0.9450 - 16s/epoch - 38ms/step\n",
            "Epoch 822/1000\n",
            "\n",
            "Epoch 822: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2604 - accuracy: 0.9190 - val_loss: 0.1917 - val_accuracy: 0.9450 - 16s/epoch - 38ms/step\n",
            "Epoch 823/1000\n",
            "\n",
            "Epoch 823: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2590 - accuracy: 0.9197 - val_loss: 0.1960 - val_accuracy: 0.9472 - 17s/epoch - 39ms/step\n",
            "Epoch 824/1000\n",
            "\n",
            "Epoch 824: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2600 - accuracy: 0.9203 - val_loss: 0.1899 - val_accuracy: 0.9438 - 17s/epoch - 39ms/step\n",
            "Epoch 825/1000\n",
            "\n",
            "Epoch 825: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2593 - accuracy: 0.9194 - val_loss: 0.2184 - val_accuracy: 0.9390 - 17s/epoch - 39ms/step\n",
            "Epoch 826/1000\n",
            "\n",
            "Epoch 826: val_accuracy did not improve from 0.95540\n",
            "429/429 - 18s - loss: 0.2601 - accuracy: 0.9199 - val_loss: 0.1946 - val_accuracy: 0.9420 - 18s/epoch - 41ms/step\n",
            "Epoch 827/1000\n",
            "\n",
            "Epoch 827: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2607 - accuracy: 0.9193 - val_loss: 0.1926 - val_accuracy: 0.9444 - 16s/epoch - 37ms/step\n",
            "Epoch 828/1000\n",
            "\n",
            "Epoch 828: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2648 - accuracy: 0.9181 - val_loss: 0.2249 - val_accuracy: 0.9302 - 16s/epoch - 37ms/step\n",
            "Epoch 829/1000\n",
            "\n",
            "Epoch 829: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2568 - accuracy: 0.9212 - val_loss: 0.2064 - val_accuracy: 0.9394 - 16s/epoch - 38ms/step\n",
            "Epoch 830/1000\n",
            "\n",
            "Epoch 830: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2615 - accuracy: 0.9204 - val_loss: 0.2007 - val_accuracy: 0.9406 - 16s/epoch - 36ms/step\n",
            "Epoch 831/1000\n",
            "\n",
            "Epoch 831: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2583 - accuracy: 0.9190 - val_loss: 0.1933 - val_accuracy: 0.9470 - 16s/epoch - 38ms/step\n",
            "Epoch 832/1000\n",
            "\n",
            "Epoch 832: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2598 - accuracy: 0.9204 - val_loss: 0.1963 - val_accuracy: 0.9408 - 16s/epoch - 37ms/step\n",
            "Epoch 833/1000\n",
            "\n",
            "Epoch 833: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2589 - accuracy: 0.9188 - val_loss: 0.1748 - val_accuracy: 0.9500 - 16s/epoch - 37ms/step\n",
            "Epoch 834/1000\n",
            "\n",
            "Epoch 834: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2606 - accuracy: 0.9184 - val_loss: 0.2012 - val_accuracy: 0.9404 - 16s/epoch - 37ms/step\n",
            "Epoch 835/1000\n",
            "\n",
            "Epoch 835: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2567 - accuracy: 0.9209 - val_loss: 0.1888 - val_accuracy: 0.9454 - 16s/epoch - 38ms/step\n",
            "Epoch 836/1000\n",
            "\n",
            "Epoch 836: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2571 - accuracy: 0.9209 - val_loss: 0.2093 - val_accuracy: 0.9382 - 17s/epoch - 39ms/step\n",
            "Epoch 837/1000\n",
            "\n",
            "Epoch 837: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2604 - accuracy: 0.9199 - val_loss: 0.1870 - val_accuracy: 0.9446 - 17s/epoch - 40ms/step\n",
            "Epoch 838/1000\n",
            "\n",
            "Epoch 838: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2597 - accuracy: 0.9198 - val_loss: 0.2276 - val_accuracy: 0.9326 - 16s/epoch - 38ms/step\n",
            "Epoch 839/1000\n",
            "\n",
            "Epoch 839: val_accuracy did not improve from 0.95540\n",
            "429/429 - 19s - loss: 0.2597 - accuracy: 0.9202 - val_loss: 0.1924 - val_accuracy: 0.9440 - 19s/epoch - 44ms/step\n",
            "Epoch 840/1000\n",
            "\n",
            "Epoch 840: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2608 - accuracy: 0.9190 - val_loss: 0.1936 - val_accuracy: 0.9434 - 16s/epoch - 38ms/step\n",
            "Epoch 841/1000\n",
            "\n",
            "Epoch 841: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2569 - accuracy: 0.9208 - val_loss: 0.1868 - val_accuracy: 0.9468 - 16s/epoch - 37ms/step\n",
            "Epoch 842/1000\n",
            "\n",
            "Epoch 842: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2607 - accuracy: 0.9200 - val_loss: 0.1850 - val_accuracy: 0.9442 - 16s/epoch - 37ms/step\n",
            "Epoch 843/1000\n",
            "\n",
            "Epoch 843: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2597 - accuracy: 0.9200 - val_loss: 0.1916 - val_accuracy: 0.9478 - 16s/epoch - 36ms/step\n",
            "Epoch 844/1000\n",
            "\n",
            "Epoch 844: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2581 - accuracy: 0.9202 - val_loss: 0.2017 - val_accuracy: 0.9404 - 16s/epoch - 37ms/step\n",
            "Epoch 845/1000\n",
            "\n",
            "Epoch 845: val_accuracy did not improve from 0.95540\n",
            "429/429 - 15s - loss: 0.2596 - accuracy: 0.9183 - val_loss: 0.1982 - val_accuracy: 0.9412 - 15s/epoch - 36ms/step\n",
            "Epoch 846/1000\n",
            "\n",
            "Epoch 846: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2579 - accuracy: 0.9200 - val_loss: 0.1884 - val_accuracy: 0.9470 - 16s/epoch - 38ms/step\n",
            "Epoch 847/1000\n",
            "\n",
            "Epoch 847: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2597 - accuracy: 0.9190 - val_loss: 0.2250 - val_accuracy: 0.9288 - 16s/epoch - 37ms/step\n",
            "Epoch 848/1000\n",
            "\n",
            "Epoch 848: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2620 - accuracy: 0.9191 - val_loss: 0.1832 - val_accuracy: 0.9490 - 16s/epoch - 37ms/step\n",
            "Epoch 849/1000\n",
            "\n",
            "Epoch 849: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2662 - accuracy: 0.9165 - val_loss: 0.1913 - val_accuracy: 0.9460 - 16s/epoch - 38ms/step\n",
            "Epoch 850/1000\n",
            "\n",
            "Epoch 850: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2614 - accuracy: 0.9198 - val_loss: 0.1991 - val_accuracy: 0.9458 - 17s/epoch - 39ms/step\n",
            "Epoch 851/1000\n",
            "\n",
            "Epoch 851: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2576 - accuracy: 0.9198 - val_loss: 0.1891 - val_accuracy: 0.9468 - 16s/epoch - 38ms/step\n",
            "Epoch 852/1000\n",
            "\n",
            "Epoch 852: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2608 - accuracy: 0.9208 - val_loss: 0.1908 - val_accuracy: 0.9468 - 17s/epoch - 39ms/step\n",
            "Epoch 853/1000\n",
            "\n",
            "Epoch 853: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2619 - accuracy: 0.9193 - val_loss: 0.1950 - val_accuracy: 0.9416 - 17s/epoch - 40ms/step\n",
            "Epoch 854/1000\n",
            "\n",
            "Epoch 854: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2640 - accuracy: 0.9193 - val_loss: 0.1952 - val_accuracy: 0.9408 - 17s/epoch - 40ms/step\n",
            "Epoch 855/1000\n",
            "\n",
            "Epoch 855: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2622 - accuracy: 0.9184 - val_loss: 0.1835 - val_accuracy: 0.9462 - 16s/epoch - 38ms/step\n",
            "Epoch 856/1000\n",
            "\n",
            "Epoch 856: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2552 - accuracy: 0.9208 - val_loss: 0.1834 - val_accuracy: 0.9462 - 16s/epoch - 37ms/step\n",
            "Epoch 857/1000\n",
            "\n",
            "Epoch 857: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2600 - accuracy: 0.9188 - val_loss: 0.2137 - val_accuracy: 0.9408 - 16s/epoch - 37ms/step\n",
            "Epoch 858/1000\n",
            "\n",
            "Epoch 858: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2575 - accuracy: 0.9212 - val_loss: 0.1925 - val_accuracy: 0.9452 - 16s/epoch - 36ms/step\n",
            "Epoch 859/1000\n",
            "\n",
            "Epoch 859: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2594 - accuracy: 0.9200 - val_loss: 0.2450 - val_accuracy: 0.9280 - 17s/epoch - 38ms/step\n",
            "Epoch 860/1000\n",
            "\n",
            "Epoch 860: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2614 - accuracy: 0.9189 - val_loss: 0.1899 - val_accuracy: 0.9440 - 16s/epoch - 37ms/step\n",
            "Epoch 861/1000\n",
            "\n",
            "Epoch 861: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2595 - accuracy: 0.9197 - val_loss: 0.1711 - val_accuracy: 0.9518 - 16s/epoch - 38ms/step\n",
            "Epoch 862/1000\n",
            "\n",
            "Epoch 862: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2579 - accuracy: 0.9200 - val_loss: 0.1973 - val_accuracy: 0.9434 - 16s/epoch - 38ms/step\n",
            "Epoch 863/1000\n",
            "\n",
            "Epoch 863: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2570 - accuracy: 0.9200 - val_loss: 0.1877 - val_accuracy: 0.9432 - 16s/epoch - 38ms/step\n",
            "Epoch 864/1000\n",
            "\n",
            "Epoch 864: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2618 - accuracy: 0.9198 - val_loss: 0.1884 - val_accuracy: 0.9500 - 16s/epoch - 38ms/step\n",
            "Epoch 865/1000\n",
            "\n",
            "Epoch 865: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2592 - accuracy: 0.9202 - val_loss: 0.2163 - val_accuracy: 0.9372 - 16s/epoch - 38ms/step\n",
            "Epoch 866/1000\n",
            "\n",
            "Epoch 866: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2593 - accuracy: 0.9211 - val_loss: 0.2156 - val_accuracy: 0.9352 - 17s/epoch - 40ms/step\n",
            "Epoch 867/1000\n",
            "\n",
            "Epoch 867: val_accuracy did not improve from 0.95540\n",
            "429/429 - 22s - loss: 0.2598 - accuracy: 0.9201 - val_loss: 0.2098 - val_accuracy: 0.9410 - 22s/epoch - 50ms/step\n",
            "Epoch 868/1000\n",
            "\n",
            "Epoch 868: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2597 - accuracy: 0.9207 - val_loss: 0.2129 - val_accuracy: 0.9340 - 16s/epoch - 38ms/step\n",
            "Epoch 869/1000\n",
            "\n",
            "Epoch 869: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2563 - accuracy: 0.9209 - val_loss: 0.1965 - val_accuracy: 0.9444 - 16s/epoch - 37ms/step\n",
            "Epoch 870/1000\n",
            "\n",
            "Epoch 870: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2605 - accuracy: 0.9194 - val_loss: 0.1998 - val_accuracy: 0.9418 - 16s/epoch - 37ms/step\n",
            "Epoch 871/1000\n",
            "\n",
            "Epoch 871: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2563 - accuracy: 0.9204 - val_loss: 0.2033 - val_accuracy: 0.9378 - 16s/epoch - 37ms/step\n",
            "Epoch 872/1000\n",
            "\n",
            "Epoch 872: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2592 - accuracy: 0.9199 - val_loss: 0.1906 - val_accuracy: 0.9460 - 16s/epoch - 37ms/step\n",
            "Epoch 873/1000\n",
            "\n",
            "Epoch 873: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2601 - accuracy: 0.9192 - val_loss: 0.2002 - val_accuracy: 0.9430 - 16s/epoch - 36ms/step\n",
            "Epoch 874/1000\n",
            "\n",
            "Epoch 874: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2595 - accuracy: 0.9198 - val_loss: 0.1908 - val_accuracy: 0.9448 - 16s/epoch - 37ms/step\n",
            "Epoch 875/1000\n",
            "\n",
            "Epoch 875: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2588 - accuracy: 0.9203 - val_loss: 0.1780 - val_accuracy: 0.9496 - 16s/epoch - 37ms/step\n",
            "Epoch 876/1000\n",
            "\n",
            "Epoch 876: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2630 - accuracy: 0.9184 - val_loss: 0.1813 - val_accuracy: 0.9496 - 16s/epoch - 37ms/step\n",
            "Epoch 877/1000\n",
            "\n",
            "Epoch 877: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2599 - accuracy: 0.9209 - val_loss: 0.2015 - val_accuracy: 0.9434 - 16s/epoch - 37ms/step\n",
            "Epoch 878/1000\n",
            "\n",
            "Epoch 878: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2594 - accuracy: 0.9203 - val_loss: 0.2172 - val_accuracy: 0.9378 - 16s/epoch - 38ms/step\n",
            "Epoch 879/1000\n",
            "\n",
            "Epoch 879: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2600 - accuracy: 0.9199 - val_loss: 0.1950 - val_accuracy: 0.9444 - 17s/epoch - 39ms/step\n",
            "Epoch 880/1000\n",
            "\n",
            "Epoch 880: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2596 - accuracy: 0.9209 - val_loss: 0.1779 - val_accuracy: 0.9512 - 16s/epoch - 38ms/step\n",
            "Epoch 881/1000\n",
            "\n",
            "Epoch 881: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2611 - accuracy: 0.9201 - val_loss: 0.1996 - val_accuracy: 0.9392 - 17s/epoch - 40ms/step\n",
            "Epoch 882/1000\n",
            "\n",
            "Epoch 882: val_accuracy did not improve from 0.95540\n",
            "429/429 - 18s - loss: 0.2585 - accuracy: 0.9204 - val_loss: 0.1859 - val_accuracy: 0.9466 - 18s/epoch - 41ms/step\n",
            "Epoch 883/1000\n",
            "\n",
            "Epoch 883: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2590 - accuracy: 0.9192 - val_loss: 0.1956 - val_accuracy: 0.9418 - 16s/epoch - 36ms/step\n",
            "Epoch 884/1000\n",
            "\n",
            "Epoch 884: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2615 - accuracy: 0.9190 - val_loss: 0.2004 - val_accuracy: 0.9444 - 16s/epoch - 37ms/step\n",
            "Epoch 885/1000\n",
            "\n",
            "Epoch 885: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2601 - accuracy: 0.9208 - val_loss: 0.2043 - val_accuracy: 0.9388 - 16s/epoch - 37ms/step\n",
            "Epoch 886/1000\n",
            "\n",
            "Epoch 886: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2591 - accuracy: 0.9189 - val_loss: 0.1880 - val_accuracy: 0.9442 - 16s/epoch - 37ms/step\n",
            "Epoch 887/1000\n",
            "\n",
            "Epoch 887: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2610 - accuracy: 0.9206 - val_loss: 0.1890 - val_accuracy: 0.9468 - 16s/epoch - 37ms/step\n",
            "Epoch 888/1000\n",
            "\n",
            "Epoch 888: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2610 - accuracy: 0.9188 - val_loss: 0.2201 - val_accuracy: 0.9348 - 16s/epoch - 37ms/step\n",
            "Epoch 889/1000\n",
            "\n",
            "Epoch 889: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2581 - accuracy: 0.9202 - val_loss: 0.2038 - val_accuracy: 0.9410 - 17s/epoch - 39ms/step\n",
            "Epoch 890/1000\n",
            "\n",
            "Epoch 890: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2592 - accuracy: 0.9200 - val_loss: 0.2008 - val_accuracy: 0.9432 - 16s/epoch - 38ms/step\n",
            "Epoch 891/1000\n",
            "\n",
            "Epoch 891: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2643 - accuracy: 0.9179 - val_loss: 0.1908 - val_accuracy: 0.9444 - 17s/epoch - 40ms/step\n",
            "Epoch 892/1000\n",
            "\n",
            "Epoch 892: val_accuracy did not improve from 0.95540\n",
            "429/429 - 18s - loss: 0.2580 - accuracy: 0.9207 - val_loss: 0.1943 - val_accuracy: 0.9450 - 18s/epoch - 43ms/step\n",
            "Epoch 893/1000\n",
            "\n",
            "Epoch 893: val_accuracy did not improve from 0.95540\n",
            "429/429 - 18s - loss: 0.2587 - accuracy: 0.9188 - val_loss: 0.2010 - val_accuracy: 0.9448 - 18s/epoch - 41ms/step\n",
            "Epoch 894/1000\n",
            "\n",
            "Epoch 894: val_accuracy did not improve from 0.95540\n",
            "429/429 - 19s - loss: 0.2578 - accuracy: 0.9196 - val_loss: 0.1932 - val_accuracy: 0.9426 - 19s/epoch - 45ms/step\n",
            "Epoch 895/1000\n",
            "\n",
            "Epoch 895: val_accuracy did not improve from 0.95540\n",
            "429/429 - 22s - loss: 0.2585 - accuracy: 0.9192 - val_loss: 0.2151 - val_accuracy: 0.9362 - 22s/epoch - 52ms/step\n",
            "Epoch 896/1000\n",
            "\n",
            "Epoch 896: val_accuracy did not improve from 0.95540\n",
            "429/429 - 20s - loss: 0.2566 - accuracy: 0.9198 - val_loss: 0.1750 - val_accuracy: 0.9490 - 20s/epoch - 47ms/step\n",
            "Epoch 897/1000\n",
            "\n",
            "Epoch 897: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2621 - accuracy: 0.9181 - val_loss: 0.2057 - val_accuracy: 0.9392 - 16s/epoch - 38ms/step\n",
            "Epoch 898/1000\n",
            "\n",
            "Epoch 898: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2579 - accuracy: 0.9213 - val_loss: 0.1961 - val_accuracy: 0.9416 - 16s/epoch - 37ms/step\n",
            "Epoch 899/1000\n",
            "\n",
            "Epoch 899: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2576 - accuracy: 0.9215 - val_loss: 0.2128 - val_accuracy: 0.9368 - 16s/epoch - 36ms/step\n",
            "Epoch 900/1000\n",
            "\n",
            "Epoch 900: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2588 - accuracy: 0.9209 - val_loss: 0.1855 - val_accuracy: 0.9458 - 17s/epoch - 40ms/step\n",
            "Epoch 901/1000\n",
            "\n",
            "Epoch 901: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2587 - accuracy: 0.9203 - val_loss: 0.1883 - val_accuracy: 0.9468 - 16s/epoch - 38ms/step\n",
            "Epoch 902/1000\n",
            "\n",
            "Epoch 902: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2581 - accuracy: 0.9211 - val_loss: 0.1859 - val_accuracy: 0.9480 - 17s/epoch - 39ms/step\n",
            "Epoch 903/1000\n",
            "\n",
            "Epoch 903: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2597 - accuracy: 0.9195 - val_loss: 0.2002 - val_accuracy: 0.9440 - 17s/epoch - 39ms/step\n",
            "Epoch 904/1000\n",
            "\n",
            "Epoch 904: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2591 - accuracy: 0.9204 - val_loss: 0.2116 - val_accuracy: 0.9326 - 16s/epoch - 38ms/step\n",
            "Epoch 905/1000\n",
            "\n",
            "Epoch 905: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2600 - accuracy: 0.9189 - val_loss: 0.1904 - val_accuracy: 0.9472 - 17s/epoch - 39ms/step\n",
            "Epoch 906/1000\n",
            "\n",
            "Epoch 906: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2589 - accuracy: 0.9208 - val_loss: 0.2068 - val_accuracy: 0.9404 - 17s/epoch - 39ms/step\n",
            "Epoch 907/1000\n",
            "\n",
            "Epoch 907: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2621 - accuracy: 0.9186 - val_loss: 0.2084 - val_accuracy: 0.9402 - 17s/epoch - 39ms/step\n",
            "Epoch 908/1000\n",
            "\n",
            "Epoch 908: val_accuracy did not improve from 0.95540\n",
            "429/429 - 22s - loss: 0.2561 - accuracy: 0.9212 - val_loss: 0.2114 - val_accuracy: 0.9404 - 22s/epoch - 50ms/step\n",
            "Epoch 909/1000\n",
            "\n",
            "Epoch 909: val_accuracy did not improve from 0.95540\n",
            "429/429 - 22s - loss: 0.2567 - accuracy: 0.9197 - val_loss: 0.1833 - val_accuracy: 0.9490 - 22s/epoch - 51ms/step\n",
            "Epoch 910/1000\n",
            "\n",
            "Epoch 910: val_accuracy did not improve from 0.95540\n",
            "429/429 - 21s - loss: 0.2593 - accuracy: 0.9201 - val_loss: 0.2240 - val_accuracy: 0.9318 - 21s/epoch - 49ms/step\n",
            "Epoch 911/1000\n",
            "\n",
            "Epoch 911: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2621 - accuracy: 0.9198 - val_loss: 0.1887 - val_accuracy: 0.9468 - 16s/epoch - 38ms/step\n",
            "Epoch 912/1000\n",
            "\n",
            "Epoch 912: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2593 - accuracy: 0.9193 - val_loss: 0.1889 - val_accuracy: 0.9444 - 16s/epoch - 38ms/step\n",
            "Epoch 913/1000\n",
            "\n",
            "Epoch 913: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2600 - accuracy: 0.9199 - val_loss: 0.2026 - val_accuracy: 0.9412 - 16s/epoch - 38ms/step\n",
            "Epoch 914/1000\n",
            "\n",
            "Epoch 914: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2598 - accuracy: 0.9200 - val_loss: 0.1824 - val_accuracy: 0.9488 - 17s/epoch - 38ms/step\n",
            "Epoch 915/1000\n",
            "\n",
            "Epoch 915: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2618 - accuracy: 0.9186 - val_loss: 0.1822 - val_accuracy: 0.9504 - 16s/epoch - 38ms/step\n",
            "Epoch 916/1000\n",
            "\n",
            "Epoch 916: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2567 - accuracy: 0.9201 - val_loss: 0.1884 - val_accuracy: 0.9496 - 17s/epoch - 39ms/step\n",
            "Epoch 917/1000\n",
            "\n",
            "Epoch 917: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2585 - accuracy: 0.9195 - val_loss: 0.2085 - val_accuracy: 0.9406 - 17s/epoch - 39ms/step\n",
            "Epoch 918/1000\n",
            "\n",
            "Epoch 918: val_accuracy did not improve from 0.95540\n",
            "429/429 - 18s - loss: 0.2594 - accuracy: 0.9202 - val_loss: 0.1846 - val_accuracy: 0.9480 - 18s/epoch - 42ms/step\n",
            "Epoch 919/1000\n",
            "\n",
            "Epoch 919: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2558 - accuracy: 0.9212 - val_loss: 0.1974 - val_accuracy: 0.9424 - 16s/epoch - 38ms/step\n",
            "Epoch 920/1000\n",
            "\n",
            "Epoch 920: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2635 - accuracy: 0.9187 - val_loss: 0.1878 - val_accuracy: 0.9446 - 16s/epoch - 38ms/step\n",
            "Epoch 921/1000\n",
            "\n",
            "Epoch 921: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2586 - accuracy: 0.9197 - val_loss: 0.2052 - val_accuracy: 0.9394 - 17s/epoch - 39ms/step\n",
            "Epoch 922/1000\n",
            "\n",
            "Epoch 922: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2590 - accuracy: 0.9215 - val_loss: 0.1969 - val_accuracy: 0.9414 - 17s/epoch - 39ms/step\n",
            "Epoch 923/1000\n",
            "\n",
            "Epoch 923: val_accuracy did not improve from 0.95540\n",
            "429/429 - 18s - loss: 0.2562 - accuracy: 0.9206 - val_loss: 0.1925 - val_accuracy: 0.9422 - 18s/epoch - 43ms/step\n",
            "Epoch 924/1000\n",
            "\n",
            "Epoch 924: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2595 - accuracy: 0.9196 - val_loss: 0.2242 - val_accuracy: 0.9330 - 17s/epoch - 41ms/step\n",
            "Epoch 925/1000\n",
            "\n",
            "Epoch 925: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2575 - accuracy: 0.9197 - val_loss: 0.2011 - val_accuracy: 0.9404 - 17s/epoch - 39ms/step\n",
            "Epoch 926/1000\n",
            "\n",
            "Epoch 926: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2610 - accuracy: 0.9195 - val_loss: 0.1856 - val_accuracy: 0.9464 - 17s/epoch - 39ms/step\n",
            "Epoch 927/1000\n",
            "\n",
            "Epoch 927: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2605 - accuracy: 0.9202 - val_loss: 0.2165 - val_accuracy: 0.9368 - 16s/epoch - 37ms/step\n",
            "Epoch 928/1000\n",
            "\n",
            "Epoch 928: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2578 - accuracy: 0.9199 - val_loss: 0.2056 - val_accuracy: 0.9374 - 16s/epoch - 37ms/step\n",
            "Epoch 929/1000\n",
            "\n",
            "Epoch 929: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2595 - accuracy: 0.9208 - val_loss: 0.1857 - val_accuracy: 0.9504 - 16s/epoch - 38ms/step\n",
            "Epoch 930/1000\n",
            "\n",
            "Epoch 930: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2591 - accuracy: 0.9192 - val_loss: 0.1939 - val_accuracy: 0.9456 - 16s/epoch - 38ms/step\n",
            "Epoch 931/1000\n",
            "\n",
            "Epoch 931: val_accuracy did not improve from 0.95540\n",
            "429/429 - 21s - loss: 0.2586 - accuracy: 0.9198 - val_loss: 0.1891 - val_accuracy: 0.9470 - 21s/epoch - 50ms/step\n",
            "Epoch 932/1000\n",
            "\n",
            "Epoch 932: val_accuracy did not improve from 0.95540\n",
            "429/429 - 30s - loss: 0.2575 - accuracy: 0.9215 - val_loss: 0.2019 - val_accuracy: 0.9426 - 30s/epoch - 71ms/step\n",
            "Epoch 933/1000\n",
            "\n",
            "Epoch 933: val_accuracy did not improve from 0.95540\n",
            "429/429 - 18s - loss: 0.2573 - accuracy: 0.9209 - val_loss: 0.1907 - val_accuracy: 0.9470 - 18s/epoch - 43ms/step\n",
            "Epoch 934/1000\n",
            "\n",
            "Epoch 934: val_accuracy did not improve from 0.95540\n",
            "429/429 - 20s - loss: 0.2572 - accuracy: 0.9213 - val_loss: 0.1960 - val_accuracy: 0.9450 - 20s/epoch - 46ms/step\n",
            "Epoch 935/1000\n",
            "\n",
            "Epoch 935: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2568 - accuracy: 0.9211 - val_loss: 0.1936 - val_accuracy: 0.9470 - 17s/epoch - 40ms/step\n",
            "Epoch 936/1000\n",
            "\n",
            "Epoch 936: val_accuracy did not improve from 0.95540\n",
            "429/429 - 19s - loss: 0.2635 - accuracy: 0.9194 - val_loss: 0.1912 - val_accuracy: 0.9464 - 19s/epoch - 45ms/step\n",
            "Epoch 937/1000\n",
            "\n",
            "Epoch 937: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2579 - accuracy: 0.9205 - val_loss: 0.1815 - val_accuracy: 0.9454 - 17s/epoch - 39ms/step\n",
            "Epoch 938/1000\n",
            "\n",
            "Epoch 938: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2540 - accuracy: 0.9209 - val_loss: 0.2013 - val_accuracy: 0.9418 - 16s/epoch - 38ms/step\n",
            "Epoch 939/1000\n",
            "\n",
            "Epoch 939: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2573 - accuracy: 0.9206 - val_loss: 0.2082 - val_accuracy: 0.9390 - 17s/epoch - 39ms/step\n",
            "Epoch 940/1000\n",
            "\n",
            "Epoch 940: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2558 - accuracy: 0.9218 - val_loss: 0.1953 - val_accuracy: 0.9462 - 16s/epoch - 37ms/step\n",
            "Epoch 941/1000\n",
            "\n",
            "Epoch 941: val_accuracy did not improve from 0.95540\n",
            "429/429 - 18s - loss: 0.2574 - accuracy: 0.9212 - val_loss: 0.2027 - val_accuracy: 0.9438 - 18s/epoch - 41ms/step\n",
            "Epoch 942/1000\n",
            "\n",
            "Epoch 942: val_accuracy did not improve from 0.95540\n",
            "429/429 - 18s - loss: 0.2570 - accuracy: 0.9210 - val_loss: 0.2152 - val_accuracy: 0.9332 - 18s/epoch - 43ms/step\n",
            "Epoch 943/1000\n",
            "\n",
            "Epoch 943: val_accuracy did not improve from 0.95540\n",
            "429/429 - 18s - loss: 0.2633 - accuracy: 0.9184 - val_loss: 0.1906 - val_accuracy: 0.9444 - 18s/epoch - 43ms/step\n",
            "Epoch 944/1000\n",
            "\n",
            "Epoch 944: val_accuracy did not improve from 0.95540\n",
            "429/429 - 19s - loss: 0.2571 - accuracy: 0.9196 - val_loss: 0.2030 - val_accuracy: 0.9398 - 19s/epoch - 44ms/step\n",
            "Epoch 945/1000\n",
            "\n",
            "Epoch 945: val_accuracy did not improve from 0.95540\n",
            "429/429 - 19s - loss: 0.2603 - accuracy: 0.9202 - val_loss: 0.1814 - val_accuracy: 0.9504 - 19s/epoch - 45ms/step\n",
            "Epoch 946/1000\n",
            "\n",
            "Epoch 946: val_accuracy did not improve from 0.95540\n",
            "429/429 - 20s - loss: 0.2574 - accuracy: 0.9207 - val_loss: 0.1899 - val_accuracy: 0.9448 - 20s/epoch - 46ms/step\n",
            "Epoch 947/1000\n",
            "\n",
            "Epoch 947: val_accuracy did not improve from 0.95540\n",
            "429/429 - 19s - loss: 0.2588 - accuracy: 0.9202 - val_loss: 0.1877 - val_accuracy: 0.9480 - 19s/epoch - 45ms/step\n",
            "Epoch 948/1000\n",
            "\n",
            "Epoch 948: val_accuracy did not improve from 0.95540\n",
            "429/429 - 23s - loss: 0.2550 - accuracy: 0.9211 - val_loss: 0.1899 - val_accuracy: 0.9462 - 23s/epoch - 53ms/step\n",
            "Epoch 949/1000\n",
            "\n",
            "Epoch 949: val_accuracy did not improve from 0.95540\n",
            "429/429 - 23s - loss: 0.2598 - accuracy: 0.9196 - val_loss: 0.1882 - val_accuracy: 0.9472 - 23s/epoch - 53ms/step\n",
            "Epoch 950/1000\n",
            "\n",
            "Epoch 950: val_accuracy did not improve from 0.95540\n",
            "429/429 - 19s - loss: 0.2600 - accuracy: 0.9201 - val_loss: 0.2010 - val_accuracy: 0.9392 - 19s/epoch - 44ms/step\n",
            "Epoch 951/1000\n",
            "\n",
            "Epoch 951: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2595 - accuracy: 0.9194 - val_loss: 0.1967 - val_accuracy: 0.9402 - 16s/epoch - 38ms/step\n",
            "Epoch 952/1000\n",
            "\n",
            "Epoch 952: val_accuracy did not improve from 0.95540\n",
            "429/429 - 18s - loss: 0.2570 - accuracy: 0.9210 - val_loss: 0.1809 - val_accuracy: 0.9464 - 18s/epoch - 41ms/step\n",
            "Epoch 953/1000\n",
            "\n",
            "Epoch 953: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2610 - accuracy: 0.9197 - val_loss: 0.1725 - val_accuracy: 0.9530 - 16s/epoch - 37ms/step\n",
            "Epoch 954/1000\n",
            "\n",
            "Epoch 954: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2598 - accuracy: 0.9204 - val_loss: 0.2033 - val_accuracy: 0.9410 - 16s/epoch - 37ms/step\n",
            "Epoch 955/1000\n",
            "\n",
            "Epoch 955: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2633 - accuracy: 0.9202 - val_loss: 0.1889 - val_accuracy: 0.9486 - 16s/epoch - 37ms/step\n",
            "Epoch 956/1000\n",
            "\n",
            "Epoch 956: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2590 - accuracy: 0.9197 - val_loss: 0.2033 - val_accuracy: 0.9394 - 16s/epoch - 36ms/step\n",
            "Epoch 957/1000\n",
            "\n",
            "Epoch 957: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2585 - accuracy: 0.9210 - val_loss: 0.1740 - val_accuracy: 0.9540 - 16s/epoch - 37ms/step\n",
            "Epoch 958/1000\n",
            "\n",
            "Epoch 958: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2573 - accuracy: 0.9209 - val_loss: 0.1892 - val_accuracy: 0.9448 - 16s/epoch - 37ms/step\n",
            "Epoch 959/1000\n",
            "\n",
            "Epoch 959: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2612 - accuracy: 0.9200 - val_loss: 0.1788 - val_accuracy: 0.9508 - 17s/epoch - 39ms/step\n",
            "Epoch 960/1000\n",
            "\n",
            "Epoch 960: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2623 - accuracy: 0.9194 - val_loss: 0.1848 - val_accuracy: 0.9452 - 16s/epoch - 38ms/step\n",
            "Epoch 961/1000\n",
            "\n",
            "Epoch 961: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2607 - accuracy: 0.9194 - val_loss: 0.1882 - val_accuracy: 0.9460 - 16s/epoch - 37ms/step\n",
            "Epoch 962/1000\n",
            "\n",
            "Epoch 962: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2592 - accuracy: 0.9199 - val_loss: 0.1904 - val_accuracy: 0.9488 - 16s/epoch - 38ms/step\n",
            "Epoch 963/1000\n",
            "\n",
            "Epoch 963: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2602 - accuracy: 0.9195 - val_loss: 0.2058 - val_accuracy: 0.9452 - 16s/epoch - 38ms/step\n",
            "Epoch 964/1000\n",
            "\n",
            "Epoch 964: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2560 - accuracy: 0.9201 - val_loss: 0.1999 - val_accuracy: 0.9364 - 17s/epoch - 40ms/step\n",
            "Epoch 965/1000\n",
            "\n",
            "Epoch 965: val_accuracy did not improve from 0.95540\n",
            "429/429 - 19s - loss: 0.2607 - accuracy: 0.9186 - val_loss: 0.1897 - val_accuracy: 0.9462 - 19s/epoch - 45ms/step\n",
            "Epoch 966/1000\n",
            "\n",
            "Epoch 966: val_accuracy did not improve from 0.95540\n",
            "429/429 - 20s - loss: 0.2578 - accuracy: 0.9210 - val_loss: 0.2258 - val_accuracy: 0.9340 - 20s/epoch - 47ms/step\n",
            "Epoch 967/1000\n",
            "\n",
            "Epoch 967: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2587 - accuracy: 0.9210 - val_loss: 0.1859 - val_accuracy: 0.9498 - 16s/epoch - 37ms/step\n",
            "Epoch 968/1000\n",
            "\n",
            "Epoch 968: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2600 - accuracy: 0.9198 - val_loss: 0.1879 - val_accuracy: 0.9462 - 16s/epoch - 38ms/step\n",
            "Epoch 969/1000\n",
            "\n",
            "Epoch 969: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2603 - accuracy: 0.9204 - val_loss: 0.1843 - val_accuracy: 0.9490 - 16s/epoch - 37ms/step\n",
            "Epoch 970/1000\n",
            "\n",
            "Epoch 970: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2571 - accuracy: 0.9212 - val_loss: 0.1920 - val_accuracy: 0.9446 - 16s/epoch - 37ms/step\n",
            "Epoch 971/1000\n",
            "\n",
            "Epoch 971: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2583 - accuracy: 0.9191 - val_loss: 0.2244 - val_accuracy: 0.9372 - 16s/epoch - 36ms/step\n",
            "Epoch 972/1000\n",
            "\n",
            "Epoch 972: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2600 - accuracy: 0.9182 - val_loss: 0.1821 - val_accuracy: 0.9476 - 16s/epoch - 38ms/step\n",
            "Epoch 973/1000\n",
            "\n",
            "Epoch 973: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2607 - accuracy: 0.9203 - val_loss: 0.1892 - val_accuracy: 0.9458 - 16s/epoch - 36ms/step\n",
            "Epoch 974/1000\n",
            "\n",
            "Epoch 974: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2568 - accuracy: 0.9207 - val_loss: 0.1762 - val_accuracy: 0.9514 - 16s/epoch - 37ms/step\n",
            "Epoch 975/1000\n",
            "\n",
            "Epoch 975: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2628 - accuracy: 0.9186 - val_loss: 0.1893 - val_accuracy: 0.9496 - 16s/epoch - 37ms/step\n",
            "Epoch 976/1000\n",
            "\n",
            "Epoch 976: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2600 - accuracy: 0.9179 - val_loss: 0.1978 - val_accuracy: 0.9448 - 16s/epoch - 38ms/step\n",
            "Epoch 977/1000\n",
            "\n",
            "Epoch 977: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2586 - accuracy: 0.9205 - val_loss: 0.2114 - val_accuracy: 0.9362 - 17s/epoch - 40ms/step\n",
            "Epoch 978/1000\n",
            "\n",
            "Epoch 978: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2583 - accuracy: 0.9206 - val_loss: 0.1967 - val_accuracy: 0.9440 - 17s/epoch - 39ms/step\n",
            "Epoch 979/1000\n",
            "\n",
            "Epoch 979: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2587 - accuracy: 0.9197 - val_loss: 0.1795 - val_accuracy: 0.9518 - 17s/epoch - 40ms/step\n",
            "Epoch 980/1000\n",
            "\n",
            "Epoch 980: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2557 - accuracy: 0.9210 - val_loss: 0.1917 - val_accuracy: 0.9486 - 17s/epoch - 40ms/step\n",
            "Epoch 981/1000\n",
            "\n",
            "Epoch 981: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2590 - accuracy: 0.9200 - val_loss: 0.1813 - val_accuracy: 0.9516 - 16s/epoch - 38ms/step\n",
            "Epoch 982/1000\n",
            "\n",
            "Epoch 982: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2580 - accuracy: 0.9202 - val_loss: 0.1849 - val_accuracy: 0.9482 - 16s/epoch - 38ms/step\n",
            "Epoch 983/1000\n",
            "\n",
            "Epoch 983: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2596 - accuracy: 0.9196 - val_loss: 0.1892 - val_accuracy: 0.9484 - 16s/epoch - 37ms/step\n",
            "Epoch 984/1000\n",
            "\n",
            "Epoch 984: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2586 - accuracy: 0.9208 - val_loss: 0.1905 - val_accuracy: 0.9486 - 16s/epoch - 37ms/step\n",
            "Epoch 985/1000\n",
            "\n",
            "Epoch 985: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2603 - accuracy: 0.9196 - val_loss: 0.1853 - val_accuracy: 0.9518 - 17s/epoch - 39ms/step\n",
            "Epoch 986/1000\n",
            "\n",
            "Epoch 986: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2606 - accuracy: 0.9199 - val_loss: 0.1790 - val_accuracy: 0.9506 - 17s/epoch - 39ms/step\n",
            "Epoch 987/1000\n",
            "\n",
            "Epoch 987: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2585 - accuracy: 0.9196 - val_loss: 0.1946 - val_accuracy: 0.9482 - 16s/epoch - 38ms/step\n",
            "Epoch 988/1000\n",
            "\n",
            "Epoch 988: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2570 - accuracy: 0.9213 - val_loss: 0.2140 - val_accuracy: 0.9356 - 17s/epoch - 40ms/step\n",
            "Epoch 989/1000\n",
            "\n",
            "Epoch 989: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2596 - accuracy: 0.9198 - val_loss: 0.1890 - val_accuracy: 0.9464 - 16s/epoch - 38ms/step\n",
            "Epoch 990/1000\n",
            "\n",
            "Epoch 990: val_accuracy did not improve from 0.95540\n",
            "429/429 - 18s - loss: 0.2578 - accuracy: 0.9207 - val_loss: 0.1850 - val_accuracy: 0.9490 - 18s/epoch - 41ms/step\n",
            "Epoch 991/1000\n",
            "\n",
            "Epoch 991: val_accuracy did not improve from 0.95540\n",
            "429/429 - 18s - loss: 0.2595 - accuracy: 0.9207 - val_loss: 0.2070 - val_accuracy: 0.9420 - 18s/epoch - 42ms/step\n",
            "Epoch 992/1000\n",
            "\n",
            "Epoch 992: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2569 - accuracy: 0.9205 - val_loss: 0.2040 - val_accuracy: 0.9434 - 17s/epoch - 40ms/step\n",
            "Epoch 993/1000\n",
            "\n",
            "Epoch 993: val_accuracy did not improve from 0.95540\n",
            "429/429 - 18s - loss: 0.2608 - accuracy: 0.9200 - val_loss: 0.2175 - val_accuracy: 0.9330 - 18s/epoch - 43ms/step\n",
            "Epoch 994/1000\n",
            "\n",
            "Epoch 994: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2571 - accuracy: 0.9212 - val_loss: 0.2146 - val_accuracy: 0.9428 - 16s/epoch - 38ms/step\n",
            "Epoch 995/1000\n",
            "\n",
            "Epoch 995: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2561 - accuracy: 0.9206 - val_loss: 0.1899 - val_accuracy: 0.9442 - 16s/epoch - 38ms/step\n",
            "Epoch 996/1000\n",
            "\n",
            "Epoch 996: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2597 - accuracy: 0.9196 - val_loss: 0.2051 - val_accuracy: 0.9392 - 16s/epoch - 37ms/step\n",
            "Epoch 997/1000\n",
            "\n",
            "Epoch 997: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2586 - accuracy: 0.9216 - val_loss: 0.1967 - val_accuracy: 0.9448 - 16s/epoch - 38ms/step\n",
            "Epoch 998/1000\n",
            "\n",
            "Epoch 998: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2565 - accuracy: 0.9201 - val_loss: 0.2004 - val_accuracy: 0.9436 - 16s/epoch - 37ms/step\n",
            "Epoch 999/1000\n",
            "\n",
            "Epoch 999: val_accuracy did not improve from 0.95540\n",
            "429/429 - 16s - loss: 0.2624 - accuracy: 0.9187 - val_loss: 0.1841 - val_accuracy: 0.9490 - 16s/epoch - 37ms/step\n",
            "Epoch 1000/1000\n",
            "\n",
            "Epoch 1000: val_accuracy did not improve from 0.95540\n",
            "429/429 - 17s - loss: 0.2596 - accuracy: 0.9212 - val_loss: 0.1680 - val_accuracy: 0.9530 - 17s/epoch - 39ms/step\n"
          ]
        }
      ],
      "source": [
        "MnistHistory = MnistModel.fit(\n",
        "    datagen.flow(xTrainMnist, yTrainMnist, batch_size=128), \n",
        "    epochs=1000, \n",
        "    validation_data=(xValidMnist, yValidMnist), \n",
        "    callbacks=[CBCheckPoint], \n",
        "    verbose=2, \n",
        "    steps_per_epoch=xTrainMnist.shape[0] // 128\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### lets see accuracy of the Mnist model : \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "assure the 4 dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "xTestMnist = np.expand_dims(xTestMnist, axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " Evaluate the model on the test set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 3s 9ms/step - loss: 7.3798 - accuracy: 0.1135\n",
            "Pérdida en el conjunto de prueba: 737.98%\n",
            "Precisión en el conjunto de prueba: 11.35%\n"
          ]
        }
      ],
      "source": [
        "loss, acc = MnistModel.evaluate(xTestMnist, yTestMnist)\n",
        "print(f\"Pérdida en el conjunto de prueba: {loss * 100:.2f}%\")\n",
        "print(f\"Precisión en el conjunto de prueba: {acc* 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " error  of the model : 737.98%\n"
          ]
        }
      ],
      "source": [
        "plt.plot(MnistHistory.history['loss'], label='Loss (TRAIN)')\n",
        "plt.plot(MnistHistory.history['val_loss'], label='Loss (VALIDATION)')\n",
        "plt.xlabel('EPOCHS')\n",
        "plt.ylabel('LOSS')\n",
        "plt.title('Loss per epoch')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "print(f\" error  of the model : {loss*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAHHCAYAAABXx+fLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAACQfUlEQVR4nO2dB3hT1RvGv3RS9t577yF7q2wQERegAqKiqCiKiyHgRnGBiiL+BbcggigICIKIKHvvvfduS6Ez/+c96U1vbm7atE1zU/r+nifQ3NzcnHuSe897vnVsdrvdLoQQQgghOYggqxtACCGEEOJvKIAIIYQQkuOgACKEEEJIjoMCiBBCCCE5DgogQgghhOQ4KIAIIYQQkuOgACKEEEJIjoMCiBBCCCE5DgogQgghhOQ4KIAIIYS48Morr4jNZpPz589b3RRCsgwKIEIIIYTkOCiACCGEEJLjoAAihGQpV69elZxITj1vQrILFECEZDOOHDkiTzzxhNSoUUMiIiKkSJEics8998jhw4fd9r18+bI8++yzUrFiRQkPD5eyZcvKgAEDXGI7rl+/rmI+qlevLrly5ZJSpUrJnXfeKQcOHFCvL1++XMWD4H89+Dxs/+qrr5zbHnzwQcmbN696b/fu3SVfvnxy//33q9f++ecf1c7y5curtpQrV0617dq1a27t3r17t9x7771SrFgxdY4419GjR6vX/vrrL/W5v/zyi9v7fvjhB/XaqlWrPPYf2ot9VqxYIY899pjqv/z586t+uXTpktv+CxculLZt20qePHnU+fTo0UN27Njhsk9q5+2JEydOyEMPPSQlSpRQ/VGnTh2ZNm2ayz5a38+cOVNGjRolJUuWVO24/fbb5dixY27HnDVrljRu3Fj1WdGiReWBBx5Qn5Oe/jX+fnBuBQsWlAIFCsigQYMkJiYm1fMiJLsQYnUDCCHpY926dfLff/9J3759laCBEPnss8/k5ptvlp07d0ru3LnVftHR0Wrg3rVrlxpob7rpJiV8fvvtNzl+/LgaIBMTE+W2226TpUuXquMNGzZMoqKiZMmSJbJ9+3apUqVKutuXkJAgXbp0kTZt2sh7773nbA8GZwyejz/+uBIda9eulY8//li1Ba9pbN26VbU7NDRUHn30USXeICzmzZsnb775pjpPiKfvv/9eevfu7fLZ2IY2t2zZMs12Dh06VA3sEH979uxRfQhxqYkO8O2338rAgQPV+bzzzjuq/dgP57Zp0ybVtrTO24wzZ85IixYt1OegHRAiEFoPP/ywREZGyjPPPOOyP84b+7700kty9uxZmThxonTs2FE2b96sBIwm7CBQmjZtKuPHj1efMWnSJPn3339VW3Gu3vSvHoikSpUqqeNt3LhR/ve//0nx4sVVXxCS7bETQrIVMTExbttWrVplx+X8zTffOLeNHTtWbZszZ47b/klJSer/adOmqX0++OADj/v89ddfah/8r+fQoUNq+/Tp053bBg4cqLaNGDHCq3aPHz/ebrPZ7EeOHHFua9eunT1fvnwu2/TtASNHjrSHh4fbL1++7Nx29uxZe0hIiH3cuHH21EB70cbGjRvb4+LinNsnTJigtv/666/qeVRUlL1gwYL2wYMHu7z/9OnT9gIFCrhsT+28zXj44YftpUqVsp8/f95le9++fdWxtb7S+r5MmTL2yMhI534//fST2j5p0iT1HOdRvHhxe926de3Xrl1z7jd//ny1H34L6elf9CHe99BDD7ns07t3b3uRIkW8OkdCAh26wAjJZmgzfhAfHy8XLlyQqlWrqhk+Zukas2fPlgYNGrhZSYBm4cA+sAQ99dRTHvfJCLDypNZuxMfAGtWqVStMwpSFApw7d065pmCxgqvMU3vgroqNjZWff/7ZuQ1uIlhh4PbxBlg/YAXRtzkkJEQWLFignsMKBhdQv379VFu1R3BwsDRv3ly54rw5byM4X/R7z5491d/6Y8OCdOXKFZfvUTtfuNU07r77buWq1Nq6fv16ZRmCaxRuTA2462rWrCm///57uvpXY8iQIS7PYTnC7w1WKkKyO3SBEZLNQMwMXBLTp09X8R0YRDUweGrArXHXXXeleizsg/gPDPy+AseCa87I0aNHZezYscoFZ4y10dp98OBB9X/dunVT/QwM6nD1wOUFtxHA33ArQQx6Q7Vq1VyeI4YHokKLpdq3b5/6/9ZbbzV9P+KGvDlvIxAhEFZTp05VDzMgZlJrK8QKzlNrK1x3AN+lWV+tXLkyXf2rYRRJhQoVUv/j+zOePyHZDQogQrIZsNZA/CBOBLEuCE7FgIgYnqSkJJ9/nidLEOKHzEBAb1BQkNu+nTp1kosXL6o4FgzKCOaFgEOQbUbaDasIYpYQQwRr0OrVq+WTTz4RX6G1CXFACD42YhSNZued2nFhqUJ8kRn169eXQADWLjP0opuQ7AoFECHZDLh9MHC+//77LplcsCroQTAwAplTA/usWbNGudL07iCzWb/x+JrVwRu2bdsme/fula+//loJFw24mfRUrlxZ/Z9WuwEE3/Dhw+XHH39UVjG0v0+fPl63CRaeW265xfkcQeOnTp1SWVxACwBH0C8Cjn0FAp7hzoIo9Pa4mjVKL0D279/vFEoVKlRQ/yOY22ixwjbt9fT0LyE3OowBIiSbgVm5cQaObCqjRQbury1btpimi2vvxz6IPTGznGj7YPDEZyJ2RM+nn36arjbrj6n9jSwlozho166dSgeHy8ysPRqIXerWrZt89913yv3VtWtXtc1b4H6C8NNAdhdiiHBMgHgcuHneeustl/30rqyMgL5AvyMOyEyImB33m2++Udl5ehEMsaa1tUmTJkqoTZkyRVnDNJBZhixAxAKlt38JudGhBYiQbAbS1uGWgeurdu3aqubNn3/+qVLL9bzwwgtqoETtHQS9oj4MXFCIwcFAiQBpWGMwuMKSgrR0BLkiQBnHQ0Btr1691OfgGBBZcIfBMjJ//ny3OJXUgMsL73v++eeV2wvCAgLArO7ORx99pFLJkbaPQGWkYSPWBYG8SPvWg/YjIBi8/vrr6erHuLg46dChg0r1hpUEgg6fixo7AG2EKOrfv79qCyxOEBAQDmhL69atM+xye/vtt1UQNYKpBw8erL5HfDcIfkbf4289hQsXVm1DmjvS25EGjxggvBfA+oXUdLzevn17FbitpcEjzR31ljLSv4Tc0FidhkYISR+XLl2yDxo0yF60aFF73rx57V26dLHv3r3bXqFCBZWOrefChQv2oUOHqjTqsLAwe9myZdU++vRrpFyPHj3aXqlSJXtoaKi9ZMmS9rvvvtt+4MAB5z7nzp2z33XXXfbcuXPbCxUqZH/sscfs27dvN02Dz5Mnj2m7d+7cae/YsaNqM9qONPItW7a4HQPg2Ei5Rhp6rly57DVq1LCPGTPG7ZixsbGqPUgd16d/e5MG//fff9sfffRR9X606f7771f9ZQSp6OhjfAbaUqVKFfuDDz5oX79+vVfn7YkzZ87Yn3zySXu5cuWc/d6hQwf71KlTXT4bbf3xxx9V6j9S3SMiIuw9evRwS2MHM2fOtDdq1EiVCChcuLA6p+PHj7vtl1b/amnw+N7N+g4lEAjJ7tjwj9UijBBCMgJcVqVLl1Yp5V9++aVX79EKBqKgJFxHgQyKMiJOCYUiNUsXIcQ3MAaIEJJtmTt3roqZ0QdWE0KINzAGiBCS7UDmGpZ0QNxPo0aNVNwLIYSkB1qACCHZDgQno+oyMp8QxE0IIemFMUCEEEIIyXHQAkQIIYSQHAcFECGEEEJyHAyC9rBWz8mTJ1W5+sysiE0IIYQQ/4GoHlRNR3mMtNbmowAyAeKnXLlyVjeDEEIIIRng2LFjUrZs2VT3oQAyAZYfrQNRDt+XYE2hxYsXS+fOnT0uPkkyD/vZP7Cf/QP72X+wr7N3P0dGRioDhjaOpwYFkAma2wviJysEUO7cudVxeXFlHexn/8B+9g/sZ//Bvr4x+tmb8BUGQRNCCCEkx0EBRAghhJAcBwUQIYQQQnIcFECEEEIIyXFQABFCCCEkx0EBRAghhJAcBwUQIYQQQnIcFECEEEIIyXFQABFCCCEkx0EBRAghhJAcBwUQIYQQQnIcFECEEEIIyXFQABFCCMkexMVY3QJyA0EBRAghJPD5532Rt0qJHFzueZ+EWJHEBH+2imRjKIAIIYQEBic2iGyZYf7a0tcc/897xvz1xHiRj5uITGkjYrdnXRtzKuv+J7LsDbmRsFwATZ48WSpWrCi5cuWS5s2by9q1az3uGx8fL6+99ppUqVJF7d+gQQNZtGiRyz6vvPKK2Gw2l0fNmjX9cCaEpMKu+SKfthI5s9PqlhDiG3bNc/ymt8wU2fiNb0THF7eK/PKYyOF/Pe9jTzLffvGQyJWjIud2icTTVeYz9v0p8tvTIr8/J7LiXZEzOzJ/zA1fSfAPd0n5C39LjhVAM2fOlOHDh8u4ceNk48aNStB06dJFzp49a7r/yy+/LJ9//rl8/PHHsnPnThkyZIj07t1bNm3a5LJfnTp15NSpU87HypUr/XRGhHhg5v0iZ3eIzHrQv5979YLIL4+LHFkllnJ4pcinLVMf2EjWcu2yyOopItHm99d0M/MBx2/6l0dFfntKZK/rZDRNTm4W2fituXA6vyd1AXR8g+P3tP9P1+0aMRdT/k5KFFnwosi2nyWgQLvWTxM5u0sCmu/vEtn4tXnf6om76vgu9y4W+fcjkSvHRTZ8LXLxoPu+p7ZI0KG/JXfcecmxAuiDDz6QwYMHy6BBg6R27doyZcoUyZ07t0ybNs10/2+//VZGjRol3bt3l8qVK8vjjz+u/n7//fdd9gsJCZGSJUs6H0WLFvXTGRGSBjF+vuB/Hy6y5QeR6V0dLoLUmPOoyIz73Qek2CjHzHzFeynbDq0Q+XWoyPVI79rxVQ+Rszsd//s6KBaD+uVjYulAdnSNI/7EavBdQRyYiYp5T4sseknk54dct8dfc7Qfs/upN4tcv5Kxzz69XeTSEZG1XziOCfYsdBUpeqa2F/ltqMju+Y7n3lqQ0N8/DXD8nr67K2V7XHTK39cupfy981eRtZ+LzH5YAorNP4jMf1bk0xae94FA/Ky1yOWjGfuMpCSR+OueBYsnUnvNTOheOCAyobLI7EdEfrhHZMkYkQ/rOH5zHzVy/26vnFD/XQstLFYSYtUHx8XFyYYNG2TkyJHObUFBQdKxY0dZtcp8thobG6tcX3oiIiLcLDz79u2T0qVLq31btmwp48ePl/Lly3tsC46Lh0ZkZKTT5YaHL9GO5+vjZhW2/UskaM/vkth5vEhohG8Oem63BK/6WBLbPi9SqFLmjhV1SoJ2z5ek+n1FwvOZ9rMNN2B7ktirdc745+CGEJpbxGbz/j0ww+M9IhKavMmeGCcJfvjubRhUcuWX4CP/idZi+0eNJOGJ9SJBwe5viI2S0K0zHc2+cFikQFnnS0FrvpBgxGac2CDxLYc5zufrno4Xg3F+rdP8PWvnL2LP2G/fniRBG7+WpHItRIrXSmnb4jESvP5/Yl/3hSQMWZ2yf2Kc2E5uEnvpm0SCUz49QyTGoUc9HidoxTsS/M+7klT3bknsNUWyAm/vGyHTu4vt9FZJ6P2F2Gv3dnktFEIAHP7H5TjBsx6SoL0LnM8T1/5Pklo+nWabjL2RaBcJmnqz2K5dlMRLR9QxQn/s62j3iFNu/ac9Szz4jyRV7SrB8552zsgTEpPEHnlOgn/uL/bq3SSp+RMp11BSgkjCNefvWjsXW8xl54CWEH1O7MnbgyJPifaL9+a3l9F7tG3Tt2K7eECSbh3n1X0i+Mgq5/km/jFGkm4Zk/K+K8fFdn6PhEAgQscseUUS7/g8Xe1Rn/Ht7WK7sF8SBi12vabXT5PgP16UhLu+FntN10mJ7cRGCfmqsyS2Gqba5ParX/WJxLcfpQR/0NYZklSjhwRtnSXBCddFtptb2eJP7RApVsP5PATnBwEUViTLxtiAFkDnz5+XxMREKVGihMt2PN+9e7fpe+Aeg9WoXbt2Kg5o6dKlMmfOHHUcDcQRffXVV1KjRg3l/nr11Velbdu2sn37dsmXL2WA1AOBhP2MLF68WFmksoIlS5ZIdqDXpgHq/z2nY2RvqTt8csyu256U0IQoid63Uv6qNT5Tx+qw80XJG3taTq6bLxsrPub2+rJF8+W2rYPV3/PrT5XEYFcB7RG7XZoe+khKX9kg5/PWlKLRu+Vg0Y6yrZyjP9Ki9omZUu3s7/JPtZflYt7q0it5uy02Sk591luOFW4t5/PVlqwgIu68dN4x3HEazmFCxHblmCyd95PEhhZwbstz/bQUj9ou5/LVkg7J21Ys/UOic5VyiA57glQ/s1m0W9eC33+Xamfmi9bys/s3ilRqnebvWTt/dYwFKYMtyHfthBSJ3i2Hi94iYjM3Slc696fUP/6NBIlN/qr5htQ6NVvO5qsntU7NUoMbbvL649Y9/p1UObdY9hXvLjvLOAZhT4QmXJVW+9+R44VayIES3V1es9kTpdOO5yQi/qIcLNZJosJLu7Xz9k0Oy1jQ9p9lXshtUvH8MrmQt5ZERZQRXxIWHylxH7eUA4XbyMHinSU4MVbyXzsql/JUcban1+mt6v/zSz+RNYfD1d/BSbFS7/h3UkF3rA0/viklr2yUbWX7S0+d+AH7dm6VPZdct6nzS4qTJFtIymcZXt+9d5/UueZwj0Rv/lXWXSkvHZNfW/L7LxIfktdlf+39J/dvlY0JC6TX1h+cr+F+nWvTP1Lz9CqRo6tk/oWKKdfQ1bOSYAtzDl7a917q8npplrxt47/L5NTOaNXmyue2SB3Dvk6Sf+NJQWFu56v/TReJ2iVhCVFyqpD2Ce702vSs+v/fi4XkUp5qus+wS50TP0pURFk5WqSdc3PDY8ec30nwqo/kn4tF5EruSmr/LtufklwJKdbV08cPyzpD23PHnpUap3+R/cV7qGNrFIw5qK6XI0XaS9uj/6ltx2a+oL7rlLa+6Pjc2Q/Kb4107i1MZ/a9KfCZBP83SebHNHL7nsGh6Y9JaOJVqXR+mQQvHimHit4qqU1lY7/qLYeL3uq8vrpdOCxhyRYgX4+FMTExgS+AMsKkSZOUywxBzQhuhgiC+0zvMuvWrZvz7/r16ytBVKFCBfnpp5/k4YfNTaCwQiEWSW8BKleunHTu3Fny58/vc3WKL7xTp04SGprJmak/SA6vqlE0SKp2dx0cMkTcVQndFKX+zH/9hHJhZobQZIFWNmablNQdKz76oqxf+L006XiniGNMkC63thXJU8y7A0efkdDNG9SfED+g8vk/pdxjM7xr15uOdrWO+UMS733G2Y+g/MV/1CN+dNa4w2yw1iTHKdrE1fTc+fpvklS9r9hr9RLb7t8lZLbjRphUratzn/ZVcost/pTYts0U25XjklSnt8hpx2s96haUkM2znPuWKO2wrKrfc3Cy2Aoyua3ozt/4nYe+6XBR161QVJLq3ytSuIrb24O/+dR5PjeHbpWgKxul1JWNLvvoj6v1f7WzC6RKqUJqluq0ABqseUEr35fga4el4LXDUruoSFJjh4vIXqqRyJVjErrZMahXPue4UdetXUOSmj6a0t+bUvq41+aUGK9Uv9+YC45YHFiz4q5K8Pd3ir1Ca1crgOG+cXL6INXGgicOS80HJ0rwjL4SdOBPSWz2mAQdWiGJbV9w9nPJyC3So3Y+dcygfydK8BbXYNMWBz9U/1c0CUKtXjxCqrSoKVK4csrG2CgJ+bSp2IvXksSOb0jIr+6TjZq1aoucdPydP38Bad+6hUhyzH+n9i1FCpRzfUNyW8tGbZZStfO5/Ebq1qsnQcfXO393Pe2LXd4aYodVzvV7t22NEjnk2HZTrUpib9RNQqa2Ftv5vSn7duvqIl4RiGs7uVESntwkElHQ4z1a+z0lFnlIkrpOcDt3FX+U3P5WjWqLvWonZQEO2vyd2PMUl5DNDrdR3QfGO7/f4HmLRHThNG2a3ST28i2VCzJ0s6truWSZ8m7XTcj/bhHbxW1SLnqzJN7xhdgO/ClJnd6Q0LdLq9fLX0zxjOC3W+6et1Is7slttYnd7bjBP34tEu2IwepRv5jIZvfTrX5mnstzCKHUyBt3VuqenCE1+iNcxS6hmxwutmuhhXw+FmoenIAWQIjLCQ4OljNnzrhsx3PE7ZhRrFgxmTt3rly/fl0uXLig3FwjRoxQ8UCeKFiwoFSvXl3279/vcZ/w8HD1MIIvJatESlYeOysISoyVoMy2F3EG01zdUL7qA5vhWCE/3C7tzu2WxHIpN7tQScROrm/cOkvkyEqRbu+KhOhmgYnJMQwGnJ+BWiO4kZm5k05vc/6J2aWnfguNOStSoIzDT398rUjJ+iJhaVgcEWeibuA2kY1fiVRs62JaVr72X4d4fHsQXJr7l4g810Zk9sCU7ftS/Poh859yeU8wYi201wzWgqBQh0UtND5SQj9qLlKxjUjf710/NDbavQ8R0/DXWyItnkj5nH/fl+C1U0Se2yWSK9lKtepTkdhIkQv7Uj7TQxxVqCSIBIe5fSdBW75XD3nlikjkKZGPG4tU7SDS51vHDjq9AZM+Hoo+37v2rdbODdMluNWTpm1waU/0SZH1X4oUqynS8D7XF7/rpVzB8sRqRxzIyQ3qERwXKdJzkunxYMlx6cMDjtiaYMS34LuZ4xrbE/J9b5EiVUXKerZamBG09Uf1kFGnUn6PB1ap+DXb4X8kaNELjrYb22dPqb+DCWqo7hoKTbzueu3pYkJsCdccbdW3PThE5MqRlDZt/s5je1OuyZTPC7l0QCTpuohO/Kh9r18Q2fmbSN07RfIWFznkEIChh/8SgfhOpviVLRIx7Q2x3f6JSJnGKee4YZoEdxwncvmIyOzBIreOFqndS2TT965tD7KJvFvRLWMtdOFwkTbPihSp4thHf85hEY4+uuweZBx0fq8ExV4W2TZLpEFfkdyFRc447jO2uKsS8tN9zvZ57KefB4o8scoRQ2WcgDR+UKTrOyK4ntEOrU3f+DZmLzQxJtmlLGIPCpWE4Nw+HwvTcyzLBFBYWJg0btxYubHuuMPhWklKSlLPhw51+D09gdieMmXKKKU+e/ZsuffelB+ukejoaDlw4ID0759i/iNesPcP1+daUGN6glMRMwSRcH6/Q1z8+Yr7fqjZ0ai/SPPkGeV/H4ts/UmkzTOOwaxWcqxJWmCQ/OcDkbYOS54t+QYdtOkb13OIPicSfVqkZD2RP0Yrf7aieB2R5o+mnekAEHA7ubkILCN3THZ//fP2KX8jXsETkScdAmju4yIYdHG8e75KCZA8tVWky1sIjks+VpKjvyCCWg4VWfiCYztuwLBuNLzfYVm4lDwNTo2juniZtDihs7Tsc/1d2EPCRZJEbMfWiFy/7AhoRbsRkIrsj189CAUE4h5fJ7J9tuv2+KuOOjDIEOr8usgfKTGCTi4dNj/mB7VFSjUQue0Dz1lLSN0Gu35zpPbe9qFIWB7z/fHZZsGnV7wMuJ5UP+VvTQAte1Nkhc6CML27SLLbSLHhK48CyMUytH+pd224sF+koN75lQ7wfQJj7B9+t2kFzsIVF62b3OL3gOsa1+WtY0QQL5Iqdu8Df/FbKd1QZMHzKdtWf2p+z0Kg/+F/HL+7AXNTtp/bI/LLEJF2L4jkLy8tDyYn1mDCVvM212OgXfj9om8RjA1h/WuKkEd8kkCwmaXrb/rW8YA1zGgpRZ8gqeD7u93fh2y796o6/j61ReTOz0VswQgq9KqLHMdInsh8cYv7axu+EtmzyDHxMJvUpRdYsh6YLbLjF9d7AALstWD1PEXTF1OZBVjqAoPbaeDAgdKkSRNp1qyZTJw4Ua5evarcWmDAgAFK6CBGB6xZs0ZOnDghDRs2VP+j5g9E04svOsz44Pnnn5eePXsqt9fJkydVij0sTf369bPsPLMduJH9YBCVnjIJPAmEifUcsywMMJ80TrkozKwlC19MEUCLX3b8r2WqPL9fJK+XbqulrzoFkBP9jRY3pvcwG7Y7Zt6a+AG4Id7UP+Vmr88iMYLBEwM1bnIYpDEb0ywdGEz1NyXDbMuFlR840sMh3gBuFpoAgigCsFRU6+T4Oy4qZUariR+A4FY8UAdF2zctDv4lXoNz9SQ+IIDidIMl+Lxt6seDkMNNHCTPBl3A70E7XzMuHjDfDiGB8/roJvPXNfGjgdTeKrd4FkCeBuCMZHthYMN1pRc/WpvN2rn0dZF694hsmC7S7nmRBpjA6QaL7+70/rNT+y2nBtKzf+zn+E1VT3GRSlK8d5lDP+kmnTP7OyYeAO4hE8ua67FiRCIdmUJpgmyy7roMRQ30nRGIHwCL61WdJfGf5Pdv+VFs9xisTVqWmgZ+uxA/HiycShSZ3evSEtG4V6HaNaxLqYFkBViZ0iN+NGC51q49I/h+tO8os0BI4boKyeVeiuFAsnjPbX12tqVp8H369JH33ntPxo4dq0TN5s2bVWFDLTD66NGjKpBZA64v1AJCyjzq/0AcIQMMbi6N48ePK7GDIGhYhooUKSKrV69W7jPiJWYpkBAP+qJ+84aJJJgMXmDdFw6RAVGBWhDOY6RDROkHX1gSJrdwpNdq5nPjTUfDuF0/C1QiLtn0DuGh59hqxwwQM0F8ztWz3q1HhHgbgPe9Xd51FpqWBWjPghTxo0efMorZoFY7Jy0rHG7i07qIV+zQzX4zQ7DDdWzTz/bTAufkTcrzEUcAZ/pJR0E+pO9iJp3e42OgggXRW8to1GmHdc4bYKlCHZy/3nCIALjIYqNcAtrThZnI0uj5kefXYEVJjHUIgORsJI+iVX8tmKEfWFGSwVNqvF5oeyp4aIbxuvP2OzEhZNYDqb9vjSEba7xJsPvRDNTdgoX834le7GgX2eTZJZhmNWd/EJ7f3HqISepyh0HDluzCsxLLg6Dh7vLk8lq+3HXNl/bt26sCiKkxY4Z3QarEUCxvxxyHbxk3WbMqqnoLEIr6geK1Uyw3RpVvZgFJbbD4e4LD5WU2Y0GRNVR3xU0OIgpWEFSehRXHCG5GHcZ6tgClJkx2znU8QP6UrAoXcD762Z82S13+tmNGZrzBpCaAzFgzVaSIIaYNqaUVW/u2uq3eYuMLPAwmpmgzwLTwVjBkBpjktXgjDVgT0hpQtWUZPLnjjESdcnUtdHrdUSvFS4J/HiihadVx8kRqbWw8UCRfKUftFiOeRIoni9KJ9d63aY4jM9MjyXFNLiCWqf2LDmuS/lo27vPIEoerWBeLZ8pfb0qGyKqB+8x2yXJWOoLf3ShS1fW+lhp3Txf52eGl8UiuZAFktABpFjjIuACwAFkugIhFwIoB9wVuynDFwB2kbvo2kd4mNx+zGw5u6mkNrvqZY2oCyNPNCDNQfSCj5iIDKyemPjjhbPSiQS/i0hImkTrLlR7M+rUgWQBLGAJ1PVWuVdV3PxOv0bu2NLSBL71xWJkFAda6G5YZwf9NlKJVR4gt5Jz4FcRwpTXAecN/H3mevXqD0T2SmkDUXG1lm7pmWHlB0OEVYsihyjya5SvcNT3da3q87wgoTg4kzhQRhcyFVa6CKfeTJoMc7ji4nM3cY8gebPao50xEI75od3rIXcQ/oj41PLm4St/kLoDgqkTChBHE2aWFVpMtldpxiX1+ENnsI5dbdl0LjFgAbjQTKom8VdoR7OsyyNkdJncjpjFANp1LKirlb63gGtD7m9PybZsBP72n6smeTPGe0IuhjJafX/NZ+sr2w/2waIT4xCXpTwGEjLQKrb3atfX+t1N3GWYF/XW/MTNQZFPPgN881hhyQ+968bZsQlpEnUwZyDGoJ2fPZSkQFWa0exHpeyIDf3M8T2sm7kmsoZTAvd+IPOyDOi6e2pBcSFSBNoOi1d33q3KryNMbReonW7LS7dbMAvIbXGNa+72heko5F1NCfFSUtnQjkbunmX/HnoRxmBeCOdyDBUiHKlJqMRRAORFkkMA1hAdSgo3uHn3Qq4bZAKdF8C98SeSdio7sH2/NqN7yze2eX4PbLj3o4202G1K1Mxj74he0rIm0XGC+bBMGnnQM0kGIofIXuHHmKaJ73sjcBK+JlxZPilRuL9I8ObA8LZB5iFlu62dEHjJkQ3oDbv6VUgreOS1A2sQA9WZSGRhcuFVn8cyIxcGMW0aJjDjiKFkAilZzZCBCNJhloKUmgHAu5ZqZi5L0gJR0M/SWHO1+U10X54bsTYAMSE/vswpdZXoHdvOSBIUqum9Dn6ZGpTQSDbzlri9F6t4lUrhSJs5Lw+buAvPV6gFZBAXQjQosNnC9IMjTiD6WILUAST1wGSF7x+wHD389Xl/wgvUmXl9nw4R5uNhbpV6qwadogd2pWYAQxzLIvXpvhkH9F28HaT2eYqd8iZZ1N2Slo3ZJ08HmIuSRpSKd33QM+EAb8EENDwU4Ww9zzHwfWyHS6VVHvRZPDPBghYJb2RiMj2SAfYtTLHrezuBL1Ev99bzmNdMUiO3RgMAB7V9yCAl95hueI64Dogj1YBDbZ7RkVOviLnL0x9APdPkchficeLMEjTF708zKq33vTR52fO+DFok8tFDkwd9F6hiy4jq/kfJ376kifX8Qafuc+BUIysaGWJn+v4h0fdv12jLGoHnjovdWcD76tyMm0tP+IcmTpryuKzKk6uryJGr028OTzykj9xA/QgF0o4JIe7hevtTdfJBai0Ut9YsGpidrxpi6iRsngn81EKujD4AONDLSNu2mayR/accN2Bt/eGZBOftXCrj2tZF+M11dHr0miwycL/LSEYcVA7N4zPbSmnnqZ/dmN6+b0lgK5K7/uQ54Gb0Bpla7Rht4EQfUYoh58UjMQAtVcAhVzZSPVO42wx2ZT/1+ND+3ju5L4nik8s0iJeq6b4f1wTgR0LtcYWHSBp60QNtRjM/tM0JFRhx1BPx6QrdmmlTp4Nj/ZpO6Stq1rM3sjavFwyV4/0+Oui6e3FN6QQd3lB6c773fenZzwUKH9jW4z/wc8H01e0ykcnL9GtQUw/deoaVDPEDYarWyNMo1dZzvuMsiDfqIYL0rCIGXDjuuC1/Ex+mKeCr6mGRm9dTFKSI8AN9ni8ddxZiZADLLDtWTWgwZrgsN1NrBZw1eJhJqUu4hJCLF5W0EfQ4RXaqhyJB/HSIK9xBPtXv017p2jep/I3pQxywAoADKLuCmhGDb1Ar0aaz7MiWdEvEziM9BSjHSpJGqusVw8/d2FWa4nPQp4CA5pdFpTULl20AlI2nVni5gXOy4AWOGZTZ78gU9DAX9Trou/eAC3FV6sQYxBDM5XBTlW4g8vUmk3t0ieXSuBhRQ1M+UjdluZuIlrRsX2tBKV0nauPyBt6D9T3k4X2O2oFk7tRmoHgySqOKLzCdP1r60CrPBMoSMGcRNgF6fuJ6vUwBdNK9dhDgkFEX0NIuud69IwwdcBxJjnSK418aedwycqQVsRxR2bRP296bwnHE1eC2T03gt6M9B7y6teqt7zEjt21Mqb+u5aaBI17cc7epgkhV3+8eO76v7hPQXzTM7X/yuIAq85ZFlIs/tTbGm4XeFwocPzhfp+IqrJRECEhXlNVJrr/43bCaA4NrTRA6sd5jI6IvCpmaZ1FvgtGsDbXvKpExBaPLrcCnrs2pR/BETi2e2iTy8WKRkXZGh6xz3EE/ofw9axqOZGx2CytN9x89QAGUXvrvLUZkWtWrS4neDORnFzKZ3cy3K5oLd+zoV+mObBRJrZv7MgIsvK2JsUPwsvXgqNqbdWHCTw0CNi9rX6Erwp2kdgXtSP+h7SpnGTFAfI4LB2yzgEvFh+psX9ht53LSAXZI+4BSDnV6Q1OiaEg+j345tWCLCExiocJPHzNOIMcDVzJqSLwOiFBmHZiBlHcBNBIsfBhLETWjxR1rmkV5sIK4mte/OkwUIA2O1jq6izCiA7tCtOO9JSCF2SB8Hk56YGGOhQwhlMwGktyDqj28sAqhZlszEmuae1KyqcG1pYEAu6PPcN9e15qp2dGTl6bDr7z1lGzt+S3CzoTAlXG4a+A7xfv1vX19N3g3dfVb//RdwrKfn5JbRjmU5Bs5zuHBhAcNEBlWqvbHe6sst6D/H+DuyBbl+p3qLoUZwaNrWymK1TGpKJYs/M1cvrLMWV4DWoADKLiSv8mxcisArjKnMxliS9BQo1FuPvE0BTi+YoZnVBMoqEOOAmSZuPEbyeYix0N8UVMzI3yLP7xPpkrnV7V3wVKEYFp0n1zksREVrOLKKStR2dQMUNNxUndt1A4p2ozebpaHasYtJO6/H4MfrIQVd+0LfN7CWYMCue7djEHF+dljKwGqG5s7DzNOIcTA3u8kaM3C8wVNWYeunRUYcc1Q2N8Mo1jGwwBWIAfPhPz3UR/EgXGwmrgTdIBX/5AbH8inOzza5TiCaMFjqB8KMLm/w0OKUoGO92MKgrbc46i3DxrgkrY363w+OC0uK8fpqNlhk0EKRUSfNB2RfABcaXFjq8x5zrM2lx2xwhhjHd1qijuFYhmvE9UCeLe0QFhqwJOtB8D2+rwJlHS5c7Tejfz/ciYjVwnd920RHbJFZ7TX9b1N/P8E9Y/jujAsRWL80UJsJ1weEOyaC+L1q56Q/TyeBIX4ABVB2BxVBsS4VlkHwFmM2kdHkbTW4OD3Nxr0hdxGJf+6gLK2VSswMgkE1hu90xH+YDfDILoGZ3mjhMXO7IJOl5ROOgMsMY3MVMWbuNQxExaqLNH1Y5PF/RYbvShmcEJfU61ORMqmkmCKgFTdOzYph5uZD/7uIPM+uloRgwyCgH9zRfgzYd38pUraJ63nqBxMELCMWJK0UbmCM9zCboab2fk+kVmhQG4TMMN7kIdC0AROxKHrO7XVvszGWRI8SQHpXk3EWbzKYaOn+eqFoOhB5QAvcxfdRvnnKZ+g/K78uwNqYOYrv59md7uJLf315ylDDvhVaeRb/vqLfDMdvrnpnNwGb1MLh5k2qlYpA19BPHtwsLKm8T//dGDPD9Isy69ELTnzWY/+IPL/XUR9JH3elD6DWf+96EQxRny8Trnu9aESMmHZ9oE9fOJDyXeM3g6Bvo8gMECiAsjtYaA4LDCIDy1uM61Pp18QJBGDuTW+NH2Ml31z5JTrccJPWg2DQ5kMclp/U7lgY0G//yJEdpCc1s3Bm3Hf6myg+4+nN7vvoa9PgBqcPAsbMq5EhJdgIAoCf252yxpqZG0VZgCLM4xSwmKWO66GFXG+y+r7RxxzpwY0RrjfMxCE+IY5aeimA3FxgJmI0IzPbjP7mjFaY1KwtmujUt1lfbwluT/31qX4PunPxRhhou2fUBdbtHZEHFziy4DxhnCwYYwP1VipMaIzv8TYIPKuApVIT5HrB0fY5SWr7vKysNkoSbzdZ6NiIvrZURgtK4js1u86NYEKE2LP+cx2/b6wHZpYAoHfbe7oO0vo92NOxnIy+DIVZmxD0rbcYBYj7CwRAsQSSPjz8eLRChBnx8ZstzIcBL72WoUKVvFuJPC20G2ZG0cXnJDwwV0KwIryxZhAuQtzoXbaZzAe0G7XR555adhNqziCrQnNbpgfU/zigi9Mxu8F5EhXeggFaf7M2E0CV2rsOUnoLCBbnxG8GK0ijrFTxblIsPE5siFcw9o2ngFP0NQYeBJPqrQKwcMCaYFbbx6MLTNdOZHoZa8J4S0YWOdUsaHC5adWJjXFSyG7a8oPD0qMFkWOg0H8fj68SOfKvw9qody0bvyuvsupsmRNA6E8svZIabgLIpHYYAoLRJ5obE2Idwa+IQcyK2J6Mop+wwPoVFCIX8tb0rq9dxKpBABUxxoHZzQUG+ht1eB7/zzwg2kxAZ/T+iXOFdVfLqDOC88cisu29mFDDhYYkm9QCsvVo90SzbD+LoADKbniqZpuelYG9qSisZv/pFECpuQlwwXm7ArmnomgZ6CN7hTYOM7U3RROL10yHAErNAhTqCDZ+PQNr3dTv68j28BQAnVpqfkbRW3oww0TGIawy+tXQjTfmVk8rAZRUs6eci6gnCX1fktDQUHdB5amSstnvGN/To385XFFmsT/eWFjgdvJYqM0AXIW/PpH5lH24fDCDRzHITd+LdE4Omta441NHFpOndsHahRguPICxtEKxGpLY6lnZeeSc1DSbPSPOCFZgbdkZbR9fxAB5wugSNRNAZgHBxoy5QEBvAUpv7KGZuwluaEwOjJlOnqwqmjg1xhelF8TaIcsXAdTamoZGnt3uEKWlTNLewW0fOn6/3lxDcIMaXaGpgRhAJM6kVeTRj9AFlt3wZD6Etea/j0Uum1hz3PDCvJleEzWyIRr1T0nlNaIVYvOG1Iq7eQIzaE8WLuPN36xwnpaZZKy5os0OjS6ZtAbLjFaiRb83ech1EDRmGfk6QFwvWBC0DJM1zPIuFiCDAMKsb+QJSbwzOR3cKIQRB3XfLHcLFs5NC5w0A9aT1MSPmQtMP+NNzxIBcBUisw3VjxFQihT5zAyi+P3c+bm7gNfX2DEONrAIlWvuuh1iG9lvmGEnk3TLaDlY3ENBQcQZ3dQ/9RggX1VG1lL0IYDNCoMaCxJmB/S/p/TESnmKG4MbWv0OvFxGxVcuoYb9RHq85xAYd3zmCDQ3gt9matZVm4ffqi/AfUEfUxYA0AKU7fDw40ERQiwUql8sNDOkZzaMtYDKNHEEw2EWg6C+bT+57pOekugIzoNIWfeF9+/RFwYzVlHF0gmINUFqMlKaU4ujwEACV8yMfq43RFy0qAnyv1u9E4hmFzkGj80mxdL0mN2Au00Q6TDOsdI98Nbk7C36tuqFjl5MmAVBwzUT7yFwGIXnzEDmGr6D9MRLILBy7Rcpi9Aag6BRwwRF6NBevWvJG3CzR/VjPPyNJgbNSEsEmoHfyNJXU+LaskIAoe4RXMfG7w8FJlGQMLXBNTuQ3okf6gChb8u18GLndMTVZBbUmiJpQgGU3dBmd789JbLxm8wdCzEJ22eLRJ/J3I0AcR5qUApyZHBkVlAhc6jLWyLrp3l27aHIm34ZD71VxDhYY1DUx5qkRdUO5oGxemtGet0lWFkZA1NaAshsoFKzsrwOE3Lkqcybyo1cOmJeY0hvOfNmAURv0M4lPSBYFQ+nADLpI32RuJwKlpOAtVDr36wQQJ6+P1xjnq79QEc/AcB9xLjiT2pAfKPidGqWRy02EtmXVogh4hG6wLLrxeqN+EHhttSAZUSfDp6axSa1lcHTClq+52vzYF5PqOJbYY54Dj136p4bY3IgwBDTgcwuYyn+9KIXf/ry/fq4lfQIxKaPiDzws7vlwozUBirMsLXVrn0JrF74XCwWqh8MYAlDcC8evo47ygyBsNJ3oKIXJ1kZA3QjoY9Ty4hQhEU5tWsba/ShwGN3XZXo8tlULN5g0AIUiCAb5dgaR2yAcaBF0UJjGrsn9DVkUCU41hDUjJuip3Rj4+fe/7PIhEopRRORrQNzOGZMqQ2OWH+nzh0ip7el3V64RvSl1lF4DstXaK4wbfDHkh69PxeZbKixotK/M5gBZATWFqzgXbSq+evpsgCZuMNQfRbHMBapTG8Mgi9AwDXWSDJaefD7wBIaaH8gDaCB1JZAJissQDciKMaItcogHrMiPgUVrrF2mTHhAnV8PBVaJX6BV0Uggpo+G792/I21ppAiqecbLwp0OWuyRIgkXHO4TVCbY9ssQw0ZD+4IYyVdWHCQGbR2quM5Aj49VcbVCo3t/j2lyJt+ZWo9sNismZJiZcLNQg+qoeqBCMoKK4gR/crhZmsrZXapDgSNI/7EKIBSWx4iK/EU+Gh1vRYzOJh7BwWQ92CtMn/jKROL+A1eFYEE6mNAkGjiB0xt725tMA6anoDL5pE/HdlhN48Q+XuC6+u4KZpZgBCciSBW46KpWIJBE0BpDYw1ujkeniq/Yt0YDPaI7dAEkNn6NhBIFw+6HssqkNXR53uHe9Abd5aGWUq7MSUWlXNRyymzJQBuZDQxr4/RIp6hACIkVXhVBAoIRJ1U31HIzUh61uoyxkogmwQpmcCYIRMUap7ejfTc/Uvdj4d1p/T7pastNne/OVIiwTPbReKiHQGFZrVhUIk5UKh1m/f7orDZ0dUi9fuYvGj3XDmXmPPsDkcBxtJZsPDsjQgFECGpwiDoQEGz+uxd5LtjGkWH8SZoFgMEUQRQF8WIS4p0BlwjiAcyaxuqwmbVwodWArcj1usysxaVyECa8w1MUpIXWTEQyBQ/3uMSBJ0+AbT9xBX5aOk+uXjVdXmQnScj5UpMKmumZZJTV67JpqOXJFv/Tv3MsYsxcjkm7WVcrscnqv7VczbyusQmOGJKD5+/KpP+3Jel3y84FxUru09HBkRfclpgNdcuO4RFagsxegMW6zy1OfVgUTcBFOKI/UANHbiZ9FYiVPjEat07fzXPMPFUkTo12j4nsvQ17QCSI0Hg48lNjrTt1Z9KTmHaykNyOvK63FqzuDSvVFhsOgGMG2+vyf9K/xYV5PkuhmUkPNzIVx28IE0qFJKzUbFSpVjK7zIxyS4/rT8mrasUlfJF0pF5mAp2u11wrw4Osqm/Nx27LLVL5Zfjl2Ikf0SoFM/nvTU0LiFJxszdLnXLFlDniwFp5rpj8kCLClI4d5gEBXm+Lq7Gi6w5dFHaVC/hPFcMfEXymk9GohNE8hqufbznQnSsOk63uiXVOeF7KZk/l/M7uXItXvpNXS1RsQmy/sgl+eahZrL52GUZ+sNGOX7pmpQukEveu6eBhAQHSbNKhZ2D2r4zUdKkYmGJS0ySP3eekcYVCknZQhGycPtpKZE/l3oOTly+JqHBNrkQHSf5coVI2UKO7wl92+XDFRJ5PUGevrWqzN18Ul69vY7cXKOYs23X4hJl0tJ90rZaUWlRuYiM/mWblCucW568xZGscPLyNVmy84z0aVpOIq/Fy/uL98qDrSuq89t+8oo0KFdQ8ucKVb8hHOuN33epNoy5rbbqiwPnoqVQLtd7G4756rwdcvDcVdl3Nlpt++GR5lK/XEHJFRIkP6w9Kt+vPirv39tAdpy8ovrqiZuryrytJ6VdtWJSt4xj4hifmCQz1h6V2qULyLjftqt+/t+AplK9RF55dd5O1e5eDUvLQ20qScGIUJmx7pjsOhUp+XKFqvP5e89ZKVkgQjrVLiF/7jqjjrti7zkZOceRYFKndH4Z2LKivDh7q3StU1I+e+Am1W84z4e/Xif/HbigzvHbh5pJjZL5ZOKf++Tb1Y4SGLlCg+R6vCP/f8a6o/Jhn4aqTbjGrsUnqjYG2Wyy61SUDO9cXcoUjFDX2c8bjsvaQxdVmzrWKi5Vi+eVtYcuqb6+vUFpOXHJ8X3geh3ctpLM2nBcLusEVpcyQdJdrMNmx6+OuBAZGSkFChSQK1euSP78qSzvkAHi4+NlwYIF0r17dwk9vFzk+7tTAoUzMyAii+cdQwzNkJUiJeulPJ/3jGOdF42H/nDE9SQmiLxeJGWdqRf2Of5G7JBWWPGVK677NXpApJcXiwUaeaVASmo8ssMyg3YsrX2e+llboiHQWDVZ5I9Rpu33FbjRf/73QelRv5S6OWEADgmyqYEWs7CQoCC1HTdJvFYgt2tfYZamDcqnr1yXkXO2ymPtq6jBR9/Pnbp0lQPnr0tEGI7nCKjGreW3LSelUtE8cvsn/zqP+Vi7ynJ7w9LqRti6alEZPnOzzNnkWEfr8Ns9VDveWbRbGpUvKO2qF5Otx66oG/TczSfk+zVHXcKnMC5+/kBjqV4in1Qsmkc+XrpP3l/iWHH9436NpEieMDkXHSubjl6WobdWlaJ5w2XyX/vl7z3n5PP+jaVQHteq2msOXpDYhCQ1KGLQLpYvXG77aKUkJCXJ2J515I8dp+WHNUddEwZbVZQXu9ZQgxUG9VyhwdKkYiEJCw5SIuHvvefkxzVHJSIsWK7GJsjinY66Wy0rF1EDg566ZfIr8fb5CseEZMLd9aVZxcKy9/QVGTN7o5y5ZpMCEaFqv/NRcbL3bJR8/0hzaVWlqCzcdkrCQ4OkQdmCkic8RJ57b4pMjnX8vppe/1TOSUElSArmDpXtJyKVGNhy7LJ6/fVedaR/y4ry4ZK9SmDoKV84txy9aFjsNBm8D33fZ+pq9RzCcOepSNN9C+UOlajrCZJgmPn3aVJOouMS1HcSHWsoYqqjcrE8SoSYUapALvWb/CX5d5ReMKDnCQ+WvWccAqdgmF2qlCwk/ZpXkOdnbZHMMvQWCLoTSkAaqVkyn+w+nY61HNMBfu/nozO4xl0qRIQGK2GUWW4rnygfDu7m03t0esZvWoCsZPGYFOGDWjGZwaxSr7FeinENMM0i5LIwY4jrwng75qbEJen38zYV3xO+yC7CauJ7F4qUbylZxa+bT8j//jkkn9zXSCoU8WIlbhMuXY1TA/ydjcq4DriGuUdCYpIaMDFLx2CKgQizpy8HNpEOtXQlDUQk6nq8HLkQo2ab2L9llSLy7h971MwPs7tPlu2Xe5qUlW0nrqiZnhr0X7xZuk78R82or8YmqlkkeKZjNfU3jvdi15oyf8tJNcPHQP32wl1qxhoRGiSrDzoKT/6155wSFnXKFJDLMbFiiwmSYav+dLYN772jYWm5/39rlKXBCAZ3bYA3AsE17rcd8uNaV5HhCXTho99ucJ4HzlXjqR+Rwp/CV/8dVtYnrU0T/9wrz3aqLscuXlOiAH02c73npWQGTltruh3HxSO9GMUPgDDBQ+PFn/UL6qZYaP7dn/Le+75Yo2b2+B3ouckWJ5J8mSUkRztgANYGYU38gDG/7pAJf+xRAsWIJ/GjvU+PJ/EDLnlwraTW53o8iR9w6sr1DIsfzSql53KcTTYcvawevuCTv/Z7fC2rxA/ICvEDfCF+QBnfGGkzDAWQlejXrMqsC8ysNorRTRVvuJGZxQXoRQ5cXoNNgqEz016sfXRqiyMNPLNgkcmtP3m3QrKILN11RokYWDzMwAACIYABArN6DCrDZjjciu3fXS6ju9eS/i0ryBPfb5Rlu8+qQXPyfTcpK4YGrB6vz98lIcE2Gd6puhqs+ny+Sg5fiFFxFRikYWZfvOOMND99QrQV0sYv2CXT/zssDcsWlLWHXUXDw1+vV+Zj3HQgePQDpBEIJs2cvWjHaed2uCVajnesMv/r5pMu79GLBrhnQNsJKQvX6gdKjQtX45T53YHr7wwz5p83HDMVP2lRedQCySj68/CEvk1frzqiHjcCRvHj9rq43h/MBJOZ+EkPdzcuK2cirytXS1rtaVC2gHI7QeAdPO9Z2KSHPLCwxbkOzHDXYUIA0e4LutQpIX/sMKmcrwMWVqOVKzXgKoLrCxbHF37eqiY/el7oUkNNblLj5yEt5e4puvUQUwHuw0Pnr6r7nPY9PPvTZnm+cw05Gxkrby7YpfabPqipDP1+o+rTzrVLSLUSeSUxSeTLlQclPjHl/PTuM437mpdX8WJwCZYuGKHcdSO61pQtxy/LvC0nnX1YJo+1DigKICvRr1mV0Uyv9Igi42foBdCtL4sse0PktoneHdu44Ki3dHlTfAYKMCYXGEOwZu6wYOV+gJtBklIuSAiOR79ZLweSZ5Bws2hEXo+X8Qt2qxsQ3DWae+P1+TslPMR1YMeNQbs5ALhwYOWY/1QbdbEXyRumYiam/XtIvT7VYOXADFU/S80THCm9ky2/mkXEKH40vvjHcUxvMN6MrECzFvkazcUC9xjiDmCdMwbqasDdY+ZySC9wh91UvpByZaUF3FWIvYBQhXA1c0fAFQRrCNwiA1pWUGIE4yXcbX/tPiufLt8vPRuUVm4tuBzXHXYNCn65Ry01WOL3qVm/9AxqXVGm/+tqkXq5Z33ZeDpOxZXAo7ludEe5eDVWWfnCQ4LlyIWryv0E4dKmalE1wOHaeHuhY0FWDJIbjlxUEwgMlCv2nZNB09c5jz/3ydbSsFxBF9fr3jNR6tgl8odLaHCQ2h9u1z+Ht5fi+R1xU890qi5P/7hJ2lcvpqyaG49elrzhIbJmVAdJSLTLF/8cVHEmiFPSQKzJ/wY2VZ8B0Qa3X56wENWP6F/YyR6cvk7qly0gr/Wqq7Zj3/1no1UcDgSaJijgnh17W2357O8D8lyn6vLKvJ1y8Fy0DK8TJ+3bt5ch329Skxftu/2oXyOp8bIjUaV7vZLyQpea0n3SP1KucIR0r1dK5m89pQb6myoUUseBO3XwN+slJlmY4TzRF+i/T/o1Uq7KVlWKOGOcFg5rqyZRW45fkdfm7VBuV/TrbfVLqXvMnjNRTqEPIYNtcFtjn4NvdZcvVx6Sf/afV7FTFQrnVt/34QtX5d4mZWXhttMqvgifCeITk1Rfwer8z4uO6vnov9kbj6tzuaVGcdnxmntW8ohuNZXL/MM/96o4Mvyu75j8r5oUgd6NyshbvXWhFzoQr3Vb/dLy+9ZTci02TkJPGuJW/QxjgKyMAfq4oUjkcVd3TkbQYnn0MTHgqY2uC2d+dZtrDaEn1zpW39aIi0l7yQrtM2r0EOn3Q8baCwtCdKyEhwarm50Gfor64FhvQRBt54krVOwIYgFwI4dPv3zwFVl8wj1YGzMWBELio85EZo2J2BtuD/pPPgpzrD5f8XrG+9LITeULyqS+jZSIwwB3f/PysnzPORczP/oH8SUbjlySD5bsVeJRu0nrwSCLmxkCK7VZ/bcPN5O3FuyWkvnDpVnFQvK/5Xvk6c615a4mjlnfvZ+7z0QL5wmTiX0aqpgegEwTWMeKJw+OyGSBwEDA6S01i6t4DtyMH/jfGhVIicBLDILPdqyuRK4etAtB1npx+vad9aRvs/Ly0s9b1Q39h8Et1IAIC1+pAhHqfBEXhJs4AnhBlWJ55N17GiiLHQQAhMS4ninrrmE/uEQRyI22IH4KwaEQwWj/X8/f7BKQDMEwYvY2FTjbr1l5Gdeztjon9D3w5reOc1u+56zUKpFH/jd3mbx4f1fJFZ7iRoVgwuuD21WWb1cdkUGtK0nJArmUtfH6oVXS5M/kEgyjz6iSEntORykBVK1E2it+43qC6xTfmRbAbMwg+mvPWenVsIzbd2KGpzizs1HXpVjecCUKEVDbonJhKZjbNTYLg/XuU1HKCgELCwbtzIB+hZWyTukCziBlAKF0KfqarF2xVN2jQ0JClLW3dun86ncDpq44oITO14OaKZc2+gGiQhMWZlQc8bv6H4Hvr9+RuQxQ/Ib2no6SVlWLSkxcggpO9qb/s5K45NjCv/edUzFouN7TIqviNNMzflMAWSmAJtUViU52U1RqL3Lo74wdVAugfbeayNWzKduf3uxaRfqLW0VObPAskLz6rOSbRbXOIvfrqkqnAwTTthi/VM14MROFYBk7d7ss23NW7m1SThqVK6j84gh6ROzLFysOKpNtWEiQzN10wmlexo2kb9NyUm10BoVjGpi5CfRgIMGMBrEz6QWm5571SkjwktGyPqmG/J7kupo0gmpjYhOVRQiDAmZwmDk99u0GFQCL2db1uEQ5eSVlZowBGAGy2s0HljBkGWlBybi5I45i49FLakDG+QEtHbXLxBXq5ooZHmZ/6G/c2DBbR/8v2HZKZf00rVg41ZsYhE2H95fL+eg49R3BxI92ZSW4jSEOB+3t27S8c5DFwIl+MA6oGjh3ZDvtPhUpzZMDu2Ep+GXTcZVZlDssbSM5fr/4XE83/YwK+0wPFsfWinzZyfH3mAvudcCI3wZmuN+R7ffWnfWUxYRIQAggXhFWoncjYf0vT6A2jyeXk76ODxbd+6RJKjFAHoKgM5Ju36CveoobPwZKAFcE4lQww4VwgLsHs26kmmrxIlP7N3aa7TGjfm3eTqfLSL3uITjWDMSraDEr6QVt1FtEYPZHm7XA1C8GNJEONYurQNkjF2OUmfql2ducZvPFz7ZTlguw/vBF+XHtMWXef/vO+sr0j0GxeeXC8sWKQ8oCgUyPx2+uovoE2T+wTkAUnG74jRQ5fFGaRscq6wHECbJuMJMHmF0u3X1WxVfg83a97mqSxgzwnimrVFYGTN76NGrMSDXxAzBLxEwWDz3ae35/uq2yimnnpQfnrKUapwXO649n2ilzffNKRZxCKyuBwND6TA/OxZP40c4d7dXED0CMGFwb3qKsmKmMaZkVPz6B66dZCiZyxkQGYj0UQAETA+QhVgFLL8x60PMxBukKJxatJtLkYZH1X5rf9LDExYLnMyWAlraYJj8vWiatohrL4fk7ldn9tV51pGPtEsoXjrosSMt94Ms1bqs9AGPMgl78ZBaztOLOtYtL5zqlVPYPBAEyfa4nJCofOQKGIdpgRdJM6oiFgHtMy9Ya3jnFRdi7UVn1XgxnepGA2id4aNQrm2JSR9/UKpVPutUrpUQX3AV64K6AhUQDsR96YH2BtcYTsFBAuPgCTcj6Aoi5VpzpBg6BIMIICTAogKxEn0ruyQKU1hIJWFVYj17UGC1AEEdYvX3e0xlaeRzulId/RFBkaVmoS38dgUJcycW4AGIiMsKwDtXcapB4qjvx29DW8syMzc4MkltqFJM3e9eTl2ZvVZaOioUjJPrUARl7b32JyJUyEI/tmVLhGoF+RlJLdYdASK9IgOh6pG3ldL2HEJ9QOJ3ubUJyGBRAVqJPJfeUBQb3V3pmb3pRY6wDhCUZyjXXPXf/+pGCCRcOLBWIXUBV1lG/bFPZNGbp0N6CwNHBbStLvy8cBdOMoGgdrB8IckSGAII631m4W6UtI44ExdIav75EFUzb/koXJSx+Hdpa1Sj5Y/tpeaRdZWW5+fbh5jr/8v5MB0sSkm3B0iFDN6Sd2EBIDoUCyEr0cT3xHgSQmZUGK8Zj8VAz9FYfs+Uq9NsMAmj/2SjpNsmRJYaAPaSLplalFOXsUQV2x8lIl7obCMxF2uzDbSqpoFrEyNxco7iyzKwe2UFufX+5yjhCBg0EC2qH1C/rSKFFoC8e4H8Dm8iKveela3LJ/iXD24td7M5sC5SIRxYHHoQQE4p6F7dFSE6EAshK7EleWIBMviKs3+VJAOnjftIQQCej4sV+/ZqKi8GaQoiJ0YD4AUbxgwBdLCmAasJYAwmBzHd+9q/UL1NQVUvW1p5B2X+AjIeudUu5xLwsf+Fm+XXTSRXYi1gb1DQxAwIHSzjo30sIIYT4AgqgQCEhNQuQwQVWoKxI1Cnz/V0sPEFuSzLklWDRbEo3v/+PxDmfpU3xfOGy4sVbXLYhlV0roqWhiR/Px8mlXFyEEEKIVVAABQqpxQAZuWOKyM+DHKurp+ECQ8FBlHmBm+m2j1fC7CSfhDaXGHsuU/FTrXhe54rHGqO611SVbR9s5Z5mTAghhGRHKIACHWRt6Rm00OHXH6Kr6OxBAMXbbdLz45UuxfJgTRoaP8y0zguK+r13d321ZMSlmDhpU62oRF6Ll7KFGERJCCHkxoICKNBB9VZ9FliFVqnvrxNAnT9cKScjzSsZI4YHVZchfFC7B2vUaGjr9ABkVhFCCCE3GhRAgY6ZC8wD3646LImrjohWNvFEJBanC3Vbpfe12+u4pIfXci+HQwghhNzQWF4kZfLkyVKxYkXJlSuXNG/eXNauXetxX9R2ee2116RKlSpq/wYNGsiiRYsydcyAx8tihVj8cMyvO+R8dEpqvWOtXwdYKRiroGNhS9bGIYQQktOxdCScOXOmDB8+XMaNGycbN25UgqZLly5y9qxuQU8dL7/8snz++efy8ccfy86dO2XIkCHSu3dv2bRpU4aPGfAgDT5P8TR3cwQ4u4qexOSv94076rqsEUUIIYTkdCwVQB988IEMHjxYBg0aJLVr15YpU6ZI7ty5Zdq0aab7f/vttzJq1Ci1emzlypXl8ccfV3+///77GT5mwJaub9RfpPt7jvifvt+LlKwv8sBst7d89e8hqTUmxQpm132li4a1lx2vdpH7m3teS4oQQgjJiVgWAxQXFycbNmyQkSNHOrcFBQVJx44dZdWqVabviY2NVW4tPREREbJy5coMH1M7Lh4akZGRTpcbHr5EOx7+Nzq3kopWl6CLBxyvtx4uUqAcdhQpWkvk4WXaAdR/CYlJ8tmKQ/LRsgOux9BZgCoXjVCSKCFBt+hqDkHfzyTrYD/7B/az/2BfZ+9+Ts/xLBNA58+fl8TERClRooTLdjzfvRsLbroDVxYsPO3atVNxQEuXLpU5c+ao42T0mGD8+PHy6quvum1fvHixsh5lBUuWLJFehm2nzl4QbenTZX/9LdfDUlYY17ieIDLzYJAcu2qTc9fd3Vr5w2zQPIoFCxZITgf9TLIe9rN/YD/7D/Z19uznmJiYGzMLbNKkScq9VbNmTbXkAkQQXF2ZdW/BYoS4Ib0FqFy5ctK5c2fJnz+/+Fqd4gvv1KmTSErokqJU2fIilx0B27d27CSS11XIgcnLD8rGC/tdtj3SpqLcUqOo7D4dLXde3yXyr2M73IM5FX0/h4YylT+rYD/7B/az/2BfZ+9+1jw4AS2AihYtKsHBwXLmTMr6UwDPS5YsafqeYsWKydy5c+X69ety4cIFKV26tIwYMULFA2X0mCA8PFw9jOBLyaoLwO24tmAJ0q37FRqWCzu5vW/WhhNu24Z1rK7WzWpdrYTIqgjPn5EDycrvkKTAfvYP7Gf/wb7Onv2cnmNZFgQdFhYmjRs3Vm4sjaSkJPW8ZcuWqb4XcUBlypRRsS2zZ8+WXr16ZfqYlmNc9FS3qGlSkl1+3XxCao9dJCcuX3Nu71GvlPwwuLkSP6kugEoIIYSQwHGBwe00cOBAadKkiTRr1kwmTpwoV69eVW4tMGDAACV0EKMD1qxZIydOnJCGDRuq/1955RUlcF588UWvjxmwKMGjq9psSxFAczefkOE/bXHZPSw4SD7q10iCjentFECEEEJIYAugPn36yLlz52Ts2LFy+vRpJWxQ2FALYj569KjK4tKA6wu1gA4ePCh58+ZVMS5IjS9YsKDXx8xuFiC73S7jF7oHcK8b3dFd/AAKIEIIISTwg6CHDh2qHmYsX77c5Xn79u1VAcTMHDNggXCx290E0cLtp+VcVEqKPqhVKr8UyO3Bz6lfN4wQQgghptBcECgowePuAluy0zWgG4QGpyJyaAEihBBC0oSjpVXYk9J0gX3+9wH5ZZMj66tfs3IyunstiQgNlldur+P5uBRAhBBCSOC7wHIsSY7ijS4xPzoX2OpDF11if17oUlMK5wmTQa0rpr6YaV7P6f6EEEIIcUABZBVJ8R7T3kHfqaudfz/WvrISPyDNldyrdxFp/YxI6YY+bCwhhBByY0EBFCgWIBXzo4sB0jGyWy3vj4sg6E7uy3oQQgghJAUGjFhFUoJ7DJA+CyyZf168xX9tIoQQQnIIFEABI4BcXWDgz+HtpFzhrFmMlRBCCMnJUAAFkgXIUOm5QpE8/m0TIYQQkkOgAAqYGCB8FSkusBol80loWgHPhBBCCMkQHGEDJgssRK7GpliF+reo4P82EUIIITkECqAAigFauf+c8+k9Tcr6v02EEEJIDoECKEBcYAkSLAmJKdWhbVzTixBCCMkyKIACxAIUFWcXSh5CCCHEP7AQogXkvX5Sgv/6xmXbleuw/lACEUIIIf6AAsgCWh54T4LizrtsO3s1QWz8OgghhBC/QBeYBeQ2iB8QHWeXYBqACCGEEL9AAWQBSSbdnijBkjvMvRo0IYQQQnwPBZAFXAsr4rYtUYIkVyi/DkIIIcQfcMS1gPjgCLdtCRBAIbQAEUIIIf6AAsgCEoJzm7rFaAEihBBC/ANHXAuwm3Q7CiGWKZDLkvYQQgghOQ0KIAuwSUrFZ42I8DCJyO8eG0QIIYQQ38PCMxZgs7sLoPDQUJGOr4hcOizSZJAl7SKEEEJyChRAAWIBggtM8pUQeWihJW0ihBBCchJ0gVloAYrv8JpzW+Xi+S1sESGEEJKzoACyBLv692pSigGuSokCFraHEEIIyVlQAFloAboan7L2hS2I3khCCCHEX1AAWSmAEnQbg1gEkRBCCPEXFEAWBkFHx+s2UgARQgghfoMCyEILUGSsbiNdYIQQQojfoACy0AJ0LiZRt5EWIEIIIcRfUABZgM3uyAI7G60LAqIFiBBCCPEbFEAWWoDOuAggWoAIIYQQf0EBZGEM0KkoXRQ0BRAhhBDiNyiALBRAMYm67rfxqyCEEEL8BUddS3AIoESs/6XBIGhCCCHEb1AAWWgBSrDTAkQIIYRYAUddC4OgE0SX+UUBRAghhPgNjrpWWoD03W9LWReMEEIIITe4AJo8ebJUrFhRcuXKJc2bN5e1a9emuv/EiROlRo0aEhERIeXKlZNnn31Wrl+/7nz9lVdeEZvN5vKoWbOmBKIFyDUGyPKvghBCCMkxWFp9b+bMmTJ8+HCZMmWKEj8QN126dJE9e/ZI8eLF3fb/4YcfZMSIETJt2jRp1aqV7N27Vx588EElcj744APnfnXq1JE///zT+TwkJCQgCyHGUwARQgghlmDpqAvRMnjwYBk0aJDUrl1bCaHcuXMrgWPGf//9J61bt5b77rtPWY06d+4s/fr1c7MaQfCULFnS+ShatKgEvAWIdYAIIYQQv2GZaSQuLk42bNggI0eOdG4LCgqSjh07yqpVq0zfA6vPd999pwRPs2bN5ODBg7JgwQLp37+/y3779u2T0qVLK7day5YtZfz48VK+fHmPbYmNjVUPjcjISPV/fHy8evgSHC8kOQZIbwFKSLKL3ceflZPRvjdff3/EFfazf2A/+w/2dfbu5/QczzIBdP78eUlMTJQSJUq4bMfz3bt3m74Hlh+8r02bNmK32yUhIUGGDBkio0aNcu4DV9pXX32l4oROnTolr776qrRt21a2b98u+fLlMz0uBBL2M7J48WJlkfIpdrv0EocLLNGeIoC2bt0mx04s8O1nEVmyZInVTcgRsJ/9A/vZf7Cvs2c/x8TEeL1vYAXHpMHy5cvlrbfekk8//VQJnf3798uwYcPk9ddflzFjxqh9unXr5ty/fv36ar8KFSrITz/9JA8//LDpcWGFQiyS3gKEAGu42PLnz+/Tc4iPvS6yWdyywOo3bCT16nX36WflZDALwIXVqVMnCQ0Ntbo5NyzsZ//AfvYf7Ovs3c+aByegBRDicoKDg+XMmTMu2/EccTtmQOTA3fXII4+o5/Xq1ZOrV6/Ko48+KqNHj1YuNCMFCxaU6tWrK7HkifDwcPUwgi/F5xdAYpzzzwSdCywkJBQf6NvPIlnzHRI32M/+gf3sP9jX2bOf03Msy4Kgw8LCpHHjxrJ06VLntqSkJPUccTueTFtGkQMRBeASMyM6OloOHDggpUqVkoAgKdH5J9PgCSGEEGuw1AUGt9PAgQOlSZMmKqgZafCw6CArDAwYMEDKlCmjYnRAz549VeZYo0aNnC4wWIWwXRNCzz//vHoOt9fJkydl3Lhx6jVkiwUE9kRTCxAFECGEEJJDBFCfPn3k3LlzMnbsWDl9+rQ0bNhQFi1a5AyMPnr0qIvF5+WXX1Y1f/D/iRMnpFixYkrsvPnmm859jh8/rsTOhQsX1OsImF69erX6OyBIcmSAAQogQgghxBosD4IeOnSoengKejbW94FFBw9PzJgxQwIanQUo0WUpDAogQgghxF9w1LUwBihXmC5YiwKIEEII8RscdS2yACXZbVIgIixlOwUQIYQQ4jc46loUAwT3V+5wnQeSS2EQQgghfoMCyM/YDi5T/1+TcIkIZRA0IYQQYgUcdf2M7fRW9f+KpHqSK1QfBG2zrlGEEEJIDoMCyKIYoH1JZSUXLUCEEEKIJXDU9TO25CwwxADRBUYIIYRYA0ddf+MUQMESERacInxK1re2XYQQQkgOwvJCiDnVBZYoNskVEiwy4qhI/HWR3IWtbhkhhBCSY6AAskwABTksQOH5HA9CCCGE+A26wPxNUoLTBeYSBE0IIYQQv0EBZGEhRJcgaEIIIYT4DQogq5bCUC4wdj8hhBBiBRyBLcoCS5BgCUcQNCGEEEL8DgWQRTFASWKT8BB2PyGEEGIFHIH9jT05BsgeJGEUQIQQQoglcAS2KAYILjAKIEIIIcQaOAJb5gILYgwQIYQQYhEUQP5GtxYYLUCEEEKINXAEtioGCAIomN1PCCGEWAFHYMsqQdMCRAghhFgFR2ALXWBMgyeEEEKsgSOwn7E5XWDMAiOEEEKsgiOwhS4wWoAIIYQQa+AIbOFaYLQAEUIIIdbAEdjP2LW1wFAJmllghBBCiCVwBLawECItQIQQQog1ZGgE/uuvv3zfkhxmAWIaPCGEEGIdGRqBu3btKlWqVJE33nhDjh075vtW3choLjBkgdEFRgghhFhChkbgEydOyNChQ+Xnn3+WypUrS5cuXeSnn36SuLg437fwBhVAQUHBYrPZrG4NIYQQkiPJkAAqWrSoPPvss7J582ZZs2aNVK9eXZ544gkpXbq0PP3007Jlyxbft/QGywKzBXEhVEIIIcQqMu2Duemmm2TkyJHKIhQdHS3Tpk2Txo0bS9u2bWXHjh2+aeUNaAFKogAihBBCsp8Aio+PVy6w7t27S4UKFeSPP/6QTz75RM6cOSP79+9X2+655x7ftvYGEkBiowAihBBCrCIkI2966qmn5McffxS73S79+/eXCRMmSN26dZ2v58mTR9577z3lEiOu2JJdYBRAhBBCSDYTQDt37pSPP/5Y7rzzTgkPD/cYJ8R0eROSBZCdLjBCCCEkewmgpUuXpn3gkBBp3759Rg5/Q2NLdoHZaAEihBBCslcM0Pjx41WwsxFse+edd3zRrhsXWoAIIYSQ7CmAPv/8c6lZs6bb9jp16siUKVN80a4bk6QksYnd8XdQhoxvhBBCCLFKAJ0+fVpKlSrltr1YsWJy6tQpX7TrxsSe5PzTZmMVaEIIIcQqMjQKlytXTv7991+37diW3syvyZMnS8WKFSVXrlzSvHlzWbt2bar7T5w4UWrUqCERERGqHSjIeP369Uwd0xIBFMQq0IQQQki2EkCDBw+WZ555RqZPny5HjhxRD8T/QIzgNW+ZOXOmDB8+XMaNGycbN26UBg0aqGU1zp49a7r/Dz/8ICNGjFD779q1S7788kt1jFGjRmX4mP7FntLxtAARQgghlpGhQJQXXnhBLly4oJa/0Nb/grXlpZdeUlWhveWDDz5QgmnQoEHqOeKHfv/9dyWmIHSM/Pfff9K6dWu577771HNYefr166eW48joMa0iKIgCiBBCCMlWAgiLeCLba8yYMcoSA3dUtWrVPNYEMgPCacOGDS6CCaKgY8eOsmrVKtP3tGrVSr777jvl0mrWrJkcPHhQFixYoIoxZvSYIDY2Vj00IiMjndWu8fAZCXESqrXL5jg+yRq0vmUfZy3sZ//AfvYf7Ovs3c/pOV6mUpHy5s0rTZs2zdB7z58/L4mJiVKiRAmX7Xi+e/du0/fA8oP3tWnTRlWhTkhIkCFDhjhdYBk5ppbW/+qrr7ptX7x4seTOnVt8RVBSnPRM/jsqKlqJN5K1LFmyxOom5AjYz/6B/ew/2NfZs59jYmKyXgCtX79efvrpJzl69KjTDaYxZ84cyQqWL18ub731lnz66acquBlrjg0bNkxef/11ZY3KKLAYIW5IbwFCgHXnzp0lf/78PpSmMSJbHH8WLFhAune/xXfHJm6zAFxYnTp1ktBQze5GfA372T+wn/0H+zp797PmwckyATRjxgwZMGCACi6GlQRCYe/evWoh1N69e3t1DCyVERwcrN6jB89Llixp+h6IHLi7HnnkEfW8Xr16cvXqVXn00Udl9OjRGTomgOvOzH2HL8WnF4A9pbvRTl5cWY/Pv0NiCvvZP7Cf/Qf7Onv2c3qOlaFIXFhhPvzwQ5k3b56EhYXJpEmTlIvp3nvvlfLly3t1DLyvcePGLstqJCUlqectW7b0aNoyBg9DSAC4xDJyTL9i12WBMQ2eEEIIsYwMCaADBw5Ijx491N8QHbDCIDAaafBTp071+jhwO33xxRfy9ddfq2Dqxx9/XB1Ly+CClUkf0NyzZ0/57LPPlAXq0KFDynwGqxC2a0IorWMGTho8BRAhhBBiFRlygRUqVEiioqLU32XKlJHt27crd9Tly5fTFYDUp08fOXfunIwdO1ZVl27YsKEsWrTIGcSM+CK9xefll19WQgv/nzhxQlWehvh58803vT5moEALECGEEJLNBFC7du2U9QWi55577lGByMuWLVPbOnTokK5jDR06VD08BT27NDYkRBU4xCOjxwwUF1gwCyESQggh2UsAffLJJ87lJxB8jKAjFCm86667lHWGeCJFAHEpDEIIISQbCSDU3pk/f77KAANwUQVSheWARm8BYiVoQgghxDLSPQrDDYXig8YFSIk3cC0wQgghJBDI0CiMZSg2b97s+9bc6LikwVMAEUIIIdkqBgiLoCLd/NixY6ruTp48eVxer1+/vq/ad8PCNHhCCCEkmwmgvn37qv+ffvpp5zakp6MYIf7HelwkdYIZBE0IIYRkLwGEIoQks0HQFECEEEJIthJAFSpU8H1LclwaPGOACCGEkGwlgL755ptUX8cSFsQEpsETQggh2VcAofKzcVl7LIGBdcFy585NAeQRLoZKCCGEBAIZMkNcunTJ5REdHS179uyRNm3ayI8//uj7Vt5gFqAku01CKIAIIYQQy/CZH6ZatWry9ttvu1mHiB6uBk8IIYQEAj4NREGV6JMnT/rykDesDGIWGCGEEJLNYoB+++03l+eo/3Pq1Cm1SGrr1q191bYb1gVmF5tQ/xBCCCHZTADdcccdLs9R/LBYsWJy6623yvvvv++rtt2ApAigEGaBEUIIIdlLACUlJfm+JTnKAoQsMKsbQwghhORcOAxbZAEKZhA0IYQQkr0E0F133SXvvPOO2/YJEybIPffc44t23eCFEG2sA0QIIYRkNwG0YsUK6d69u9v2bt26qddI2tACRAghhGQzAYTCh6j6bCQ0NFQiIyN90a4blJQYIKbBE0IIIdlMANWrV09mzpzptn3GjBlSu3ZtX7Trhk+DpwAihBBCslkW2JgxY+TOO++UAwcOqNR3sHTpUrUMxqxZs3zdxhsICiBCCCEk2wqgnj17yty5c+Wtt96Sn3/+WSIiIqR+/fry559/Svv27X3fyhsxDZ76hxBCCMleAgj06NFDPUh6oAWIEEIIybYxQOvWrZM1a9a4bce29evX+6JdN3gaPIOgCSGEkGwngJ588kk5duyY2/YTJ06o10jqsBAiIYQQkg0F0M6dO+Wmm25y296oUSP1Gkkdx1IYFECEEEJIthJA4eHhcubMGbftWBE+JCTDYUU5qhI0LUCEEEJINhNAnTt3lpEjR8qVK1ec2y5fviyjRo2STp06+bJ9NxgshEgIIYQEAhky17z33nvSrl07qVChgnJ7gc2bN0uJEiXk22+/9XUbbxxYCJEQQgjJvgKoTJkysnXrVvn+++9ly5Ytqg7QoEGDpF+/fmo5DOKJFAFE/UMIIYRYR4YDdvLkySNt2rSR8uXLS1xcnNq2cOFC9f/tt9/uuxbeSDANnhBCCMm+AujgwYPSu3dv2bZtm9hsNrHb7ep/jcTERF+28YZDxQAxCJoQQgjJXkHQw4YNk0qVKsnZs2cld+7csn37dvn777+lSZMmsnz5ct+38oZBFwMUTAFECCGEZCsL0KpVq2TZsmVStGhRCQoKkuDgYOUOGz9+vDz99NOyadMm37f0RguCpgWIEEIIyV4WILi48uXLp/6GCDp58qT6G1lhe/bs8W0Lbyh0i6EyBogQQgjJXhagunXrquwvuMGaN28uEyZMkLCwMJk6dapUrlzZ9628UWAhREIIIST7CqCXX35Zrl69qv5+7bXX5LbbbpO2bdtKkSJFZObMmb5u4w0Es8AIIYSQbCuAunTp4vy7atWqsnv3brl48aIUKlTIJRuMGGAhREIIIST7xgCZUbhw4QyLn8mTJ0vFihUlV65cyqW2du1aj/vefPPN6nOMjx49ejj3efDBB91e79q1qwQKKgaI+ocQQgixDMtXLoXLbPjw4TJlyhQlfiZOnKgsTAimLl68uNv+c+bMcRZeBBcuXJAGDRrIPffc47IfBM/06dNdFnC1HlqACCGEkBvKApRRPvjgAxk8eLBaSqN27dpKCKG20LRp0zxamkqWLOl8LFmyRO1vFEAQPPr94J4LJBdYEF2FhBBCSM60AMGSs2HDBrWyvAbqCnXs2FHVGvKGL7/8Uvr27auW5tCDgoywIEH43HrrrfLGG2+oIG0zYmNj1UMjMjJS/R8fH68ePiMhXrBSmpJBSYm+PTZxQetb9nHWwn72D+xn/8G+zt79nJ7jWSqAzp8/r2oKYRV5PXiOwOq0QKwQqlBDBBndX3feeadK0z9w4ICMGjVKunXrpkQVijYaQQHHV1991W374sWLlXXJVxSMOSjtky1Aa1avkiO+OzTxACyEJOthP/sH9rP/YF9nz36OiYnJPjFAmQHCp169etKsWTOX7bAIaeD1+vXrS5UqVZRVqEOHDm7HgQUKcUh6C1C5cuWkc+fOkj9/fp+113Zyo0hyncg2rVtL9VIFfHZs4j4LwIXVqVMnCQ2F3Y1kBexn/8B+9h/s6+zdz5oHJ+AFEKpIwyJz5swZl+14jrid1EAdohkzZqg6RGmB4oz4rP3795sKIMQLmQVJ40vx6QUQ7OhuWIDCw3x8bGKKz79DYgr72T+wn/0H+zp79nN6jmVpEDSqRzdu3FiWLl3q3JaUlKSet2zZMtX3zpo1S8XtPPDAA2l+zvHjx1W2WKlSpSRQYBYYIYQQkoOzwOB6+uKLL+Trr7+WXbt2yeOPP66sO8gKAwMGDHAJkta7v+644w63wObo6Gh54YUXZPXq1XL48GElpnr16qUKNuoLOFpDchaYnWnwhBBCiJVYHgPUp08fOXfunIwdO1ZOnz4tDRs2lEWLFjkDo48ePaoyw/SgRtDKlStVkLIRuNS2bt2qBNXly5eldOnSKpbn9ddft74WkDMNnoUQCSGEkBwtgMDQoUPVwwwELhupUaOG2J0Li7oSEREhf/zxhwQmLIRICCGEBAKWu8ByEohv0mAhREIIIcQ6KID8SJLdIYBoASKEEEKshQLIj2heO/xH+UMIIYRYBwWQH9HilmABogeMEEIIsQ4KIIsEEG1AhBBCiHVQAPmVlMw1WoAIIYQQ66AA8idJOheY1W0hhBBCcjAUQH7ELilp8LQAEUIIIdZBAWRVEDRtQIQQQohlUABZgEqDp/4hhBBCLIMCyI/YkytBMwaIEEIIsRYKIAtywFQaPE1AhBBCiGVQAFkSA8QqQIQQQoiVUAD5EZtuBXsagAghhBDroADyJ04BRPVDCCGEWAkFkAV1gBgETQghhFgLBZBVq8HTB0YIIYRYBgWQX+FSGIQQQkggQAHkR+zOtcAYBE0IIYRYCQWQH7HrLUBUQIQQQohlUAD5E10aPCGEEEKsgwLIj9jtKVlghBBCCLEOCiB/QgsQIYQQEhBQAFm1FhghhBBCLIMCyJ/o1gIjhBBCiHVQAFmUBUYIIYQQ66AA8ifJdYAIIYQQYi0UQBZYgLgYKiGEEGItFECWpMETQgghxEoogCwJgqYFiBBCCLESCiBLoAAihBBCrIQCyII6iHSBEUIIIdZCAeRP6AIjhBBCAgIKIL/iCIImhBBCiLVQAPkROy1AhBBCSEBAAeRPuBgqIYQQEhBQAPkRWoAIIYSQwIACyAIogAghhBBroQCywAJECCGEEGsJCAE0efJkqVixouTKlUuaN28ua9eu9bjvzTffLDabze3Ro0cPF6ExduxYKVWqlEREREjHjh1l3759Yj3MAiOEEEICAcsF0MyZM2X48OEybtw42bhxozRo0EC6dOkiZ8+eNd1/zpw5curUKedj+/btEhwcLPfcc49znwkTJshHH30kU6ZMkTVr1kiePHnUMa9fvy6WwhggQgghJCCwXAB98MEHMnjwYBk0aJDUrl1biZbcuXPLtGnTTPcvXLiwlCxZ0vlYsmSJ2l8TQLD+TJw4UV5++WXp1auX1K9fX7755hs5efKkzJ07VwIjCJoQQgghVhJi5YfHxcXJhg0bZOTIkc5tQUFBymW1atUqr47x5ZdfSt++fZWVBxw6dEhOnz6tjqFRoEAB5VrDMbGvkdjYWPXQiIyMVP/Hx8erh69ITExI/svm0+MSd7T+ZT9nLexn/8B+9h/s6+zdz+k5nqUC6Pz585KYmCglSpRw2Y7nu3fvTvP9iBWCCwwiSAPiRzuG8Zjaa0bGjx8vr776qtv2xYsXK+uSr8h7aq+UhQXIZlOWK5L1sJ/9A/vZP7Cf/Qf7Onv2c0xMTPYQQJkFwqdevXrSrFmzTB0HFijEIektQOXKlZPOnTtL/vz5xVec/uesSLIG69Spk4SGhvrs2MR9FoALi/2ctbCf/QP72X+wr7N3P2senIAXQEWLFlUBzGfOnHHZjueI70mNq1evyowZM+S1115z2a69D8dAFpj+mA0bNjQ9Vnh4uHoYwZfiyy8myGZzBkH7+tjEHPazf2A/+wf2s/9gX2fPfk7PsSwNgg4LC5PGjRvL0qVLnduSkpLU85YtW6b63lmzZqm4nQceeMBle6VKlZQI0h8TihDZYGkdM+th+DMhhBASCFjuAoPraeDAgdKkSRPlykIGF6w7yAoDAwYMkDJlyqg4HaP764477pAiRYq4bEdNoGeeeUbeeOMNqVatmhJEY8aMkdKlS6v9rYRLYRBCCCGBgeUCqE+fPnLu3DlVuBBBynBTLVq0yBnEfPToUZUZpmfPnj2ycuVKFaRsxosvvqhE1KOPPiqXL1+WNm3aqGOi0KKV2FgJmhBCCAkILBdAYOjQoephxvLly9221ahRI9VlJWAFQmyQMT7IamgBIoQQQgIDywsh5iwogAghhJBAgALIAugII4QQQqyFAsgCFxjtP4QQQoi1UAD5FbrACCGEkECAAsiP2O1Jjv8pgAghhBBLoQDyJ0yDJ4QQQgICCiB/kqx/aAEihBBCrIUCyK/QAkQIIYQEAhRAVhRCTF4UlRBCCCHWQAHkV2gBIoQQQgIBCiBLgqBpASKEEEKshALIkrXACCGEEGIlFEB+hRYgQgghJBCgAPIjXA2eEEIICQwogCyALjBCCCHEWiiA/AkrQRNCCCEBAQWQP2EWGCGEEBIQUAD5FcYAEUIIIYEABZAFQdCEEEIIsRYKIL9CCxAhhBASCFAA+RPGABFCCCEBAQWQBdARRgghhFgLBZAfsduTHH/QAEQIIYRYCgWQBTAGiBBCCLEWCiB/whggQgghJCCgAPIjV/NXkXmJLWSXVLa6KYQQQkiOJsTqBuQkzpXpKE/FF5TyYXYZZnVjCCGEkBwMLUAWeMBs9IARQgghlkIB5EeY/k4IIYQEBhRAFiyFQQMQIYQQYi0UQH6EFiBCCCEkMKAA8iOMASKEEEICAwogv0IbECGEEBIIUABZYQGyuiGEEEJIDocCyI+wDjQhhBASGFAAWbESBiGEEEIshQLIj9iTbUAMgiaEEEKshUthWGIBoimIkJxMYmKixMfHe70/9g0JCZHr16+r95Ksg30d2P0cGhoqwcHBPmkDBZAF0ABESM4thnr69Gm5fPlyut9XsmRJOXbsmNhoQs5S2NeB388FCxZU783s92O5AJo8ebK8++676qbQoEED+fjjj6VZs2Ye98eNY/To0TJnzhy5ePGiVKhQQSZOnCjdu3dXr7/yyivy6quvurynRo0asnv3brEa2n0Iydlo4qd48eKSO3dur2/gSUlJEh0dLXnz5pWgIEYuZCXs68DtZ4immJgYOXv2rHpeqlSp7CuAZs6cKcOHD5cpU6ZI8+bNlZDp0qWL7NmzR90gjMTFxUmnTp3Uaz///LOUKVNGjhw5otSgnjp16siff/7pfA4zWyAthUEIyXnAzK+JnyJFiqR7sMD9L1euXByUsxj2dWD3c0REhPofIgjXUmbcYZYqgw8++EAGDx4sgwYNUs8hhH7//XeZNm2ajBgxwm1/bIfV57///lN+QFCxYkW3/SB4YB4LVGhVJSTnocX8wPJDCMk42jWEaypbCiAovw0bNsjIkSOd26ACO3bsKKtWrTJ9z2+//SYtW7aUJ598Un799VcpVqyY3HffffLSSy+5dMK+ffukdOnSSlli//Hjx0v58uU9tiU2NlY9NCIjI52dm55AxbSIT0gJ9PLlcYk7Wv+yn7MW9rP3oI9gBcYDs9+MWI8z8l6SPtjXgd/P2nVkJoDScy+yTACdP39emYRLlCjhsh3PPcXrHDx4UJYtWyb333+/LFiwQPbv3y9PPPGEOuFx48apfeBK++qrr1Tcz6lTp1Q8UNu2bWX79u2SL18+0+NCIBnjhsDixYt9OlvbfA6mn2AVBL1kyRKfHZd4hv3sH9jPaaNZphH3gAlgRoiKivJ5u0jG+hpxp/Be3HPPPZJdgBcF49qMGTMkUMjIbxrXz7Vr12TFihWSkJDg8hpihLzFZrcoMOXkyZMqhgfuLFhpNF588UX5+++/Zc2aNW7vqV69ukqZO3TokFP1wY2GIGqIHTPgc0egNPZ7+OGHvbYAlStXTom0/Pnzi6+Yu/mkvDB7u9QskCRzhnVwuvGI74EoxqCMmDH2c9bBfvYe3LuQ8QK3PazT6QG3aQwUmMQxMylr8aav4Y2A52HHjh1qEO7QoUOqx1y6dKkcPnzYOQbhuJjsY3I+YcIEUw9F7dq11ViHhzGk49Zbb1VJQx9++KHzOcbN77//Xvr27evcb9KkSeoB44EmHKpUqSI//PCD+mwrycxvGtcS+hPjtPFawvhdtGhRuXLlSprjt2UWIDQQIubMmTMu2/HcU/wOIr6NNQBq1aqlMivwxYaFhbm9BwHSEE6wFnkiPDxcPYzgs3x5Uw8KCs6yYxNz2M/+gf2cNrB440YPV396g2s1F4H2fuIQ31nxm/Omrz/55BNl/YFVr02bNi4T8GHDhqlBePr06c5thQsXlqNHj6oBGUk+GPwhbODB6NOnj9uEf+XKlcrCcffdd8u3336rxJYRY/sgBMaOHassUlq/aMJC2w/7IGwE7W/fvr1YSWZ+09gf7zO776TnN2HZlQSx0rhxY6WM9R2C53qLkJ7WrVsrIaP3F+7du1cJIzPxA2BuPnDgQKbT5Xy6FhgncISQbMSiRYvUQI8JJTLYbrvtNnVf1XP8+HHp16+fGuzz5MkjTZo0cRnY582bJ02bNlWDMCbAvXv3dr6GwWzu3Lkux8NnIZwBYLaPfZA5jIEbx4C148KFC+oz4U1AuEK9evXkxx9/dDkOxgtYWapWraomurC2vPnmm07LydChQ132P3funMou0o9NxtcRitGzZ0/1HGMPJu3aA1lK+Bz9Nm18wjngOcajVq1aKYvQ2rVrnXGnGl9++aUSKv3791duK29AP8Dj8cUXX6S6H9oNC9a1a9ckp2PpVAIp8Piyvv76a9m1a5c8/vjjcvXqVWdW2IABA1yCpPE6ssCgsCF8kDH21ltvqaBojeeff16ZAnHBwL2GiwwWI/w4rIZp8IQQt7omcQlePa7FJXq9rzeP9NyPcF/G/Xr9+vVKGGAGjnurNhnFRBPC5MSJE2pw3bJliwpn0F7HvRr7I25m06ZN6hip1XvzBLKDcf/HeIGSKXCFYCKN4yPO89FHH1WiAaJCA2PI22+/LWPGjJGdO3cq948We/rII4+o5/oQCAgrCBSIIzNgnYHYgvchMyCN+5dfflHjk96rAbfQrFmz5IEHHlCuZbhy/vnnnzSPB+sSauS99tpr6vvyBIQp4mbWmISZ5DQsTYOH6Q9qGmY7uLEaNmyoZhrajxMmQ71pDP6+P/74Q5599lmpX7++Uv24GPTmQW0WgpkBssQwa1m9erX622q4GjwhRM+1+ESpPfYPSz5752tdJHeYd0PAXXfd5fIcVgncUyEo6tatq0QE7uXr1q1TFiAAi4sGLC6ITdEnmyCGJb0888wzcuedd7psw6RX46mnnlJjxE8//aQEFsQEYmDg8hk4cKDaBzEwGBcAjgULELKK7733XrUNE3JYXzzFpaD2HMaojLgiIWZQ+E8r6AeefvppZTHTQIBytWrVVD07gH6DRcibmB241HC+iHmF4DMD4q1AgQLqPHI6llcIxI/PaILUWL58uds2uMcgaDwRSNHtbtAARAjJhqC0CCaqsBogOUSz7GCSCgG0efNmadSokVP8GMHrqPmWWWC9MMZVwQsAwQPrE2JBYc3RsndhKcJzT0HKcKVpbiYIoI0bNypLEuJuPAHXUXqD2DUQ8IvPQPzSwoULlbVJc8dpoC2w/mjgb1jXsEqCp0xmDbjeYAGCEITHxBNw08WkI1vqRsVyAZST4GrwhBA9EaHByhKTFhAcUZFRki9/Pp8FQeOzvQVxI8imRcgCaqyhPRA+Wjq/Vp3X42el8TqsLUaXnFk9F72lBCADGBYPrCKA+B+8DiuRt+3S3GDwPsB7gMDlW265JdW6cYhfunTpkmQEfHeaZQwuNMRRQahoggsWNUzw4cLTezYg9DC590ZEQjC999578sYbb5gWCgYIJSkWAF4Rq2E6gR/Rrm/qH0KINvDDDeXNIyIs2Ot9vXl4m3qMcAJkLr388svKkoKB2ygAEJIAKw8GVjPwuqegYoDBWJ9JBYuTNxaKf//9V3r16qUGfbjUKleurOJDNeBKgghK7bMhnGBZgriDK0+LQfUELF0I2cioCDLGNCGwG1YhAFdXu3btVAwV+lN7IP4Kr3krslDb7rPPPlOxsEYguhA71ahRI8npUAD5EXrACCHZjUKFCqnMr6lTp6osXGRAYUDWg7hLZDfdcccdSpSg7szs2bOdVf1RqBbZWfgfbqlt27bJO++843w/Ao4Rp4MAaQRaDxkyxKt0Zggc1KFCwguO+9hjj7mUVoGrCpYUBGR/8803avCHhcUoJmAFQqA0rFD67DQzIBxgBcJ5ZhbEteLz4F6ExQuWIPQlrGv6B9oH9yPqDnlDjx49VFHgzz//3O01BFRDKFapUkVyOhRAfoQWIEJIdgMWBbhfsHQRBmMkocD1pAdp3qgwjPRxZHrBqgJBoWU33XzzzSqzCRlicDdB8Ogztd5//30lBhDoiwBkBDZ7U4UfVqmbbrpJZYThMzQRpgfBwM8995wSGbBeIflGW01cA6IDNX3wf1rxPTgnWIkQv+ML0J/IYkPgMqxtZgIM7cbDWysQgMCEpccIhKgv4rFuBCyrBB3IoCYDouS9qSSZHr5fc0RG/7Jd6hVKkjnDu7JwXBaC2RSWS8HNmP2cdbCfvUerYl+pUqV0B9Ei5gb3JdyPWAjR98BVBIsIstgg0NLqa7jAkKUF1xVio7ILsCBBfMJNWKBAAUvbkpnfdGrXUnrGb15JVliAaAIihJCAEPAQM7AktWjRQlmTvAGWJlhjkAWXnUCcFVyBVoufQIFZYH6EpjZCCAkcEMeDrC8sl/Tzzz+n671GV1t2oGPHjlY3IaCgACKEEJIjQdwQo0ByLnSB+ZPkC40eMEIIIcRaKID8CJfCIIQQQgIDCiA/4rS0UgERQgghlkIB5Ec0XzP1DyGEEGItFEB+hKF2hBBCSGBAAeRHWAmaEEIICQwogPwILUCEEHJjgEVLsXgq8S1TpkyRnj17ij+gALIiBogmIEIIybZgTTMsutq3b1+Ji4tTi6Ni7TMzXn/9dSlRooSqOq1Rs2ZNCQ8PV1WozWoTPfPMMx4/22azydy5c12ea488efKoBWIffPBBtXabGcePH1drt9WtW9e57ZVXXnE5jtkD4LjGApDHjh2Thx56SEqXLq2Oi6VBhg0bptY1M54XjoN15fRMmjRJKlas6HyOY2GJESzamtVQABFCCMl26AWFv/noo4/UgqhYwwqD/gMPPCDTp083nfR+9dVXMmDAAOdaeStXrpRr167J3XffLV9//bVP2oPPxjIXWOtr8uTJEh0drVaDx7IXRtCee++9V62ZhRXmARafxfu1R9myZeW1115z2WbGwYMHpUmTJrJv3z61yOr+/fuVBWfp0qXSsmVLuXjxosv+WLcLy46k9t2hP7EgLvo4q6EA8iOMASKEZEcWLVokbdq0kYIFC0qRIkXktttukwMHDrhZFrCaeuHChZUlAgOjNsCCefPmSdOmTdUgCIuJftVzo1UD4LMwWGuLlWKfmTNnSvv27dUxsBo7rAz4zDJlyqjV47EKPQZi46KbEyZMkKpVqyqrS/ny5eXNN99Ur2Fh0KFDh7rsf+7cObWqPQZxM/D6smXLXNw0Dz/8sFpgFOJGz99//61EAl7XwBpiGOD79+8v06ZNE1+AvsL6ZLCkdO7cWS3rcf/996tzu3Tpkosgg1jCZ993333O1eXz5s2r3q89sOJ9vnz5XLaZ8eSTTyrBsnjxYvW9oG+7desmf/75p5w4cUJGjx7tsj++q8uXL8sXX3yR6vmgb2Flg1DMSiiA/Ig9OQqIAogQ4pwVxV317hEf4/2+3jzSsQTE1atXZfjw4bJ+/XolDGD5gICBuACwOGAAxKCHgWvLli3y4osvOl///fff1f7du3eXTZs2qWM0a9Ys3d01YsQI5V7ZtWuXdOnSRa0K3rhxY3X87du3y6OPPqoG97Vr1zrfM3LkSOWeGjNmjOzcuVPF7cAlBR555BH1PDY21rk/hFWpUqWUODIDIgdiq1atWs5tEF4Qd0ZBA7HRqlUr5fICUVFRMmvWLGUx6tSpk1qxPKtcPc8++6z6vCVLlji3/fXXXxITE6PWBHvggQeUOwrfbUaAdeePP/6QJ554QiIiIlxeg2CCAINg1S81gtXZIYpgXUrtcyGeExISXAR0VsC1wPwICyESQlyAqHmrtFcz1YK+/uxRJ0XC8ni161133eXyHAN9sWLFlKBALAlEBCwj69atUxYgAIuLBiwuiJd59dVXndsaNGiQ7iYjNubOO+902Qb3jcZTTz2lBuWffvpJCSwIAMSYfPLJJzJw4EC1T5UqVZQ1C+BYsJL8+uuvyi0E4JaCdUSLezFy5MgRJaAgAvXAyoO2wHUDiwo+G5YYvSsHggMxOnXq1FHP0SewwrRt21Z8jSa6YD3TwGfhM2HhqVu3rlSuXFkJMsT2pBe4vSBu9EJQD7bD+qRZ1DQgmPCdfPjhh/L000+bvhcCEyvWo6+zElqA/Aj1DyEkO4LBDu4LDJiYxWtBq0ePHlX/b968WRo1auQUP0bweocOHTLdDlgG9CQmJqogY1hg8NkQHhBAWrtgKYJ1x9Nnw5Wmd0Uh+BaWJJyrJ+CWwfuM4D1oD8QXgPUDIqlPnz7OffA5sLxo4G8IEIilrEu6cYw4cD3NmTPH7fO/THaDZfZzvAVuSFiA3n//fbdAaT2wKsFalZXQAuRHuOgwIcSF0NwOS0wawJUUGRUl+fPlc7M8ZOqzvQQxGcjuQewGsn3QHlgQkAEFjC4QI2m9jkHaOJCaBcoitkjPu+++q6wJEydOVCIIr8NK5G27NDdYw4YNVQwTXFa33HKLimXxBOKX9HE1GhCGCGzGMZDJhP9hVYIoA7CWrV69WrnnXnrpJef7IJpgGRo8eLD4Eog/UKlSJfU/rHRwGSI4WsNut6vvEvFL1atXT9fxYeHD94bP0cdz6T+/UKFCylJoBMLrvffeUw9PnwsXm9l7fQktQH6EMUCEEBcwO4cbypsHBIu3+3rz8LIeB2bpe/bsUdk7sKRorg099evXV1YeY9aP/nVPQcUAA50+0wgWJ29m///++6/06tVLDahwqcFChcFcA+4miKDUPhvCCZYliDuIBGR3pQYsXUhfNxNBcIMhRmj+/Pny33//uQU/o3YQ4qPQV9oDsVWZtcKYAVEIUYZ4H+3zn3vuOZfP3rJli3K/ZSQYG8HwiGP69NNP3YKV0T+IpYL1y8yVCBEPtyg+V++i00CAPcQa+joroQDyI7QAEUKyG5jFY7CbOnWqSnNGBhQGbaP7B4GvqBEDUYLMp9mzZ8uqVavU6+PGjVPZWfgfloFt27bJO++843w/Ao4Rp4MAaQRaDxkyxJk2nhoQOAjyhdjAcR977DFVn0cDripYWxCQjZRwDKywwhgFB6xACJSGRcTMmqEHgzKsQDhPIxA4sIwg7R0xOAiA1qxZ3377reonWM70D3w2gn2Rwq6BuBm9UMFDf15G4N6C6EDMDPoDliiIuc8++0xliOH9cO/hs4yf369fPxX3hKDj9ILvDC5GBKSvWLFC1QRCxiCEETLztGw7M3r06KEC2PG7MoLAcIhZxGtlJRRAfiQkyCbhIUESwl4nhGQTMFuHiwaF9TBgIrsIric9Wio0gl2R6QWrCgQFgm21IniIdUGGGNxNEDz6TC3Eg5QrV05ZIxCAjGBiBMKmBaxSN910kxqA8RmaCNOD7C9YPsaOHausV7BKnD171mUfiICQkBD1v1l8jx6cE6xEsHAYgbUD7i9Yh/C/Bs4bljQzcYU24aEXZRAvEFr6R2qp42gPMtcguh5//HHldkP/oi8Bjl27dm1nYLSe3r17q/5YsGCBpBcIUAhWiBW4+yBYkIkHNyLEr6eYMH0BRlh6jEAs+9olaIbNnt4IphwACkQhAh0pijAh+hLMBPBDw03CmxkOyRjsZ//AfvYe3OgPHTqkYjLSGmRNY4AiI9X9yGcxQMQJ3DAYvJHFBoGWVl/D2oJMLlhVEBtF0o+n3zQsYRDIcGViHE7vtZSe8ZtXEiGEkBwr4CFmYElq0aKFsiZ5AyxNsKpo2WbEdyAWDO5KT+LHlzALjBBCSI4EcTxw1yATCTV70oPR1UZ8gxa07Q8ogAghhORIEDfEKJCcC11ghBBCCMlxUAARQgghJMdBAUQIIX6ELhdCAuMaogAihBA/oJUJyOr1jQi50YlJvoYyW3qDQdCEEOIHUEAPVXm1Inwo9OdpxXGzmilY3wr1T1gHKGthXwduP8PyA/GDawjXklZoM6NQABFCiJ9A/RhgrETszY0f6y1hXStvRRPJGOzrwO9niB/tWsoMFECEEOIncKPHkgVYMsJstXNPYF+stYS1plhxO2thXwd2P2PfzFp+NCiACCHEz+AGnp6bOPbFYpUo+89BOWthX/uHQOhnOjgJIYQQkuOgACKEEEJIjoMCiBBCCCE5DsYApVJkKTIyMksCv5DGh2PTv5x1sJ/9A/vZP7Cf/Qf7Onv3szZue1MskQLIhKioKPV/uXLlrG4KIYQQQjIwjhcoUCDVfWx21mU3LdB08uRJyZcvn8/rQECdQlgdO3ZM8ufP79NjkxTYz/6B/ewf2M/+g32dvfsZkgbip3Tp0mkWWKQFyAR0WtmyZbP0M/CF8+LKetjP/oH97B/Yz/6DfZ19+zkty48Gg6AJIYQQkuOgACKEEEJIjoMCyM+Eh4fLuHHj1P8k62A/+wf2s39gP/sP9nXO6WcGQRNCCCEkx0ELECGEEEJyHBRAhBBCCMlxUAARQgghJMdBAUQIIYSQHAcFkB+ZPHmyVKxYUXLlyiXNmzeXtWvXWt2kbMX48eOladOmqkJ38eLF5Y477pA9e/a47HP9+nV58sknpUiRIpI3b16566675MyZMy77HD16VHr06CG5c+dWx3nhhRckISHBz2eTfXj77bdVRfRnnnnGuY397BtOnDghDzzwgOrHiIgIqVevnqxfv975OnJUxo4dK6VKlVKvd+zYUfbt2+dyjIsXL8r999+viskVLFhQHn74YYmOjrbgbAKTxMREGTNmjFSqVEn1YZUqVeT11193WSuK/ZwxVqxYIT179lRVl3GPmDt3rsvrvurXrVu3Stu2bdXYierREyZMEJ+ALDCS9cyYMcMeFhZmnzZtmn3Hjh32wYMH2wsWLGg/c+aM1U3LNnTp0sU+ffp0+/bt2+2bN2+2d+/e3V6+fHl7dHS0c58hQ4bYy5UrZ1+6dKl9/fr19hYtWthbtWrlfD0hIcFet25de8eOHe2bNm2yL1iwwF60aFH7yJEjLTqrwGbt2rX2ihUr2uvXr28fNmyYczv7OfNcvHjRXqFCBfuDDz5oX7Nmjf3gwYP2P/74w75//37nPm+//ba9QIEC9rlz59q3bNliv/322+2VKlWyX7t2zblP165d7Q0aNLCvXr3a/s8//9irVq1q79evn0VnFXi8+eab9iJFitjnz59vP3TokH3WrFn2vHnz2idNmuTch/2cMXBdjx492j5nzhyoSfsvv/zi8rov+vXKlSv2EiVK2O+//3517//xxx/tERER9s8//9yeWSiA/ESzZs3sTz75pPN5YmKivXTp0vbx48db2q7szNmzZ9VF9/fff6vnly9ftoeGhqobnMauXbvUPqtWrXJesEFBQfbTp0879/nss8/s+fPnt8fGxlpwFoFLVFSUvVq1avYlS5bY27dv7xRA7Gff8NJLL9nbtGnj8fWkpCR7yZIl7e+++65zG/o+PDxcDQJg586dqt/XrVvn3GfhwoV2m81mP3HiRBafQfagR48e9oceeshl25133qkGVMB+9g1GAeSrfv3000/thQoVcrlv4NqpUaNGpttMF5gfiIuLkw0bNijzn369MTxftWqVpW3Lzly5ckX9X7hwYfU/+jg+Pt6ln2vWrCnly5d39jP+h5uhRIkSzn26dOmiFubbsWOH388hkIGLCy4sfX8C9rNv+O2336RJkyZyzz33KBdho0aN5IsvvnC+fujQITl9+rRLP2ONI7jP9f0MtwGOo4H9cX9Zs2aNn88oMGnVqpUsXbpU9u7dq55v2bJFVq5cKd26dVPP2c9Zg6/6Ffu0a9dOwsLCXO4lCH+4dOlSptrIxVD9wPnz55UfWj8YADzfvXu3Ze3KziQlJamYlNatW0vdunXVNlxsuEhwQRn7Ga9p+5h9D9prxMGMGTNk48aNsm7dOrfX2M++4eDBg/LZZ5/J8OHDZdSoUaqvn376adW3AwcOdPaTWT/q+xniSU9ISIiaFLCfHYwYMUIJb4j04OBgdS9+8803VdwJYD9nDb7qV/yP+C3jMbTXChUqlOE2UgCRbGud2L59u5rJEd9y7NgxGTZsmCxZskQFHZKsE/GY+b711lvqOSxA+E1PmTJFCSDiG3766Sf5/vvv5YcffpA6derI5s2b1eQJgbvs55wNXWB+oGjRomrmYcySwfOSJUta1q7sytChQ2X+/Pny119/SdmyZZ3b0ZdwN16+fNljP+N/s+9Be404XFxnz56Vm266Sc3G8Pj777/lo48+Un9j9sV+zjzIjKldu7bLtlq1aqnsOX0/pXbfwP/4rvQg0w6ZNexnB8g+hBWob9++yi3bv39/efbZZ1VWKWA/Zw2+6tesvJdQAPkBmLQbN26s/ND62R+et2zZ0tK2ZScQZwfx88svv8iyZcvczKLo49DQUJd+hp8YA4rWz/h/27ZtLhcdLB1IwTQORjmVDh06qD7CTFl7wFIBl4H2N/s588B9ayzjgDiVChUqqL/x+8YNXt/PcOUgNkLfzxCiEK0auDZwf0GsBRGJiYlRMSV6MCFFHwH2c9bgq37FPki3R9yh/l5So0aNTLm/FJkOoyZep8Ej+v2rr75Ske+PPvqoSoPXZ8mQ1Hn88cdVSuXy5cvtp06dcj5iYmJc0rORGr9s2TKVnt2yZUv1MKZnd+7cWaXSL1q0yF6sWDGmZ6eBPgsMsJ99U2IgJCREpWnv27fP/v3339tz585t/+6771zSiHGf+PXXX+1bt2619+rVyzSNuFGjRiqVfuXKlSpzL6enZ+sZOHCgvUyZMs40eKRsoyTDiy++6NyH/ZzxTFGUucADcuKDDz5Qfx85csRn/YrMMaTB9+/fX6XBYyzFdcI0+GzGxx9/rAYN1ANCWjzqHhDvwQVm9kBtIA1cWE888YRKm8RF0rt3byWS9Bw+fNjerVs3VUsCN8LnnnvOHh8fb8EZZV8BxH72DfPmzVNCEZOjmjVr2qdOneryOlKJx4wZowYA7NOhQwf7nj17XPa5cOGCGjBQ2wZlBgYNGqQGJuIgMjJS/XZx782VK5e9cuXKqnaNPq2a/Zwx/vrrL9N7MkSnL/sVNYRQMgLHgJiFsPIFNvyTORsSIYQQQkj2gjFAhBBCCMlxUAARQgghJMdBAUQIIYSQHAcFECGEEEJyHBRAhBBCCMlxUAARQgghJMdBAUQIIYSQHAcFECGEEEJyHBRAhJCA4MEHHxSbzeb26Nq1q3q9YsWKzm158uRRi7XOmjXL5RhYRBErfWM9LazBhxW/H3roIecCo3pOnz4tTz31lFSuXFnCw8OlXLly0rNnT5e1i/CZEydOdHvvK6+8Ig0bNnRZb2rkyJFSpUoVyZUrlxQrVkzat28vv/76q497iRDiK0J8diRCCMkkEDvTp0932QZxovHaa6/J4MGD1aKK77//vvTp00fKlCkjrVq1UuKnRYsWSvhMmTJF6tSpI4cPH5aXX35ZmjZtKqtWrVJiB2A7FiMtWLCgvPvuu2qVcCy2+Mcff8iTTz4pu3fvTle7hwwZohZ5/Pjjj9VirxcuXJD//vtP/U8ICUwogAghAQPEDlaQ9kS+fPnU63hMnjxZvvvuO5k3b54SQKNHj5aTJ0/K/v37nccoX768EjXVqlVTwmbhwoVq+xNPPKEsSWvXrlXWJA2IJliM0stvv/0mkyZNku7duzstR40bN85ADxBC/AVdYISQbElISIiEhoZKXFycJCUlyYwZM+T+++93E1ARERFK8EAIwUqEx6JFi5Qg0osfDViF0gs+c8GCBRIVFZWpcyKE+A8KIEJIwDB//nzJmzevy+Ott95y2w+iZ/z48XLlyhW59dZb5dy5c3L58mWpVauW6XGxHes+wzqEB/6uWbOmV2166aWX0mzT1KlTlcurSJEiyt327LPPyr///pvBXiCE+AO6wAghAcMtt9win332mcu2woULu4gRxPRcv35dCZG3335bevToIWfOnFGvQ9ikhTf76HnhhRdUgLaejz76SFasWOF83q5dOzl48KCsXr1aCSEEUsMl9uqrr8qYMWPS9XmEEP9AAUQICRjgkqpatWqaYgTip0SJEiqOByDrCq6rXbt2mb4P27Gvdmz87W2gc9GiRd3apBdlGnDHtW3bVj0g1N544w0VtI2/EZhNCAks6AIjhGQbNDGCmBtN/ICgoCC599575YcfflDp7XquXbsmn376qXTp0kUJFzzwN4Kor1696vYZcKX5AmSDJSQkKGsVISTwoAAihAQMsbGxSsDoH+fPn/fqvYjLgTDq1KmTyvY6duyYclNB7CDFHYJHA38nJiZKs2bNZPbs2bJv3z5lJYJrq2XLlulu98033yyff/65bNiwQaXYIyB61KhRyqWXP3/+dB+PEJL10AVGCAkYkJ1VqlQpl201atTwyl2FAGTE4MDt9NhjjynxBGtPt27dVLo8UuI1UA9o48aN8uabb8pzzz0np06dUm40pK4bY5C8ASLr66+/VqIHRRFRgPG2226TsWPHpvtYhBD/YLOnNyKQEEIIISSbQxcYIYQQQnIcFECEEEIIyXFQABFCCCEkx0EBRAghhJAcBwUQIYQQQnIcFECEEEIIyXFQABFCCCEkx0EBRAghhJAcBwUQIYQQQnIcFECEEEIIyXFQABFCCCEkx0EBRAghhJAcx/8BR41rHdkgZlYAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy of the model : 11.35%\n"
          ]
        }
      ],
      "source": [
        "plt.plot(MnistHistory.history['accuracy'], label='accuracy (TRAIN)')\n",
        "plt.plot(MnistHistory.history['val_accuracy'], label='accuracy (VALIDATION)')\n",
        "plt.xlabel('EPOCHS')\n",
        "plt.ylabel('accuracy')\n",
        "plt.title('accuracy per epoch')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "print(f\"accuracy of the model : {acc*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## proceed with a class prediciton for our random sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "use a random variable to our data sample "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [],
      "source": [
        "id = np.random.choice(xTestMnist.shape[0], 1000, replace=False) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 27ms/step\n"
          ]
        }
      ],
      "source": [
        "MnistPredictions = MnistModel.predict(xTestMnist[id[0]:id[0]+1]) # make predictions on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "predicted class: 1\n",
            "rial class:  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "predicted_class = np.argmax(MnistPredictions)\n",
        "plt.imshow(xTestMnist[id[0]])\n",
        "\n",
        "print(\"predicted class:\" , predicted_class)\n",
        "print(\"rial class: \",yTestMnist[id[0]]) # display the true class of the image"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "icrGGZQg7Kb1"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cnnDvlEnv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
