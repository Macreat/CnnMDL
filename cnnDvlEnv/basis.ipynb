{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CONVOLUTIONAL NEURAL NETWORK NB (CNN-First on DL)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## libraries and env configuration \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "#packages from tensor flow\n",
        "import tensorflow as Tf\n",
        "\n",
        "# tensor flow for optimizing the model \n",
        "\n",
        "\n",
        "# basis packages \n",
        "import os as os \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy \n",
        "import ssl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available and used :   1\n"
          ]
        }
      ],
      "source": [
        "physicalDevice = Tf.config.experimental.list_physical_devices('GPU')\n",
        "Tf.config.experimental.set_memory_growth(physicalDevice[0], True)\n",
        "print(\"Num GPUs Available and used :  \", len(physicalDevice))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '2' # Suppress TensorFlow logging (1)\t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## data preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load MNIST dataset\n",
        "(xTrainMnist,yTrainMnist),(xTestMnist,yTestMnist)=Tf.keras.datasets.mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### normalization "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "255"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xTrainMnist.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "255"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xTestMnist.max()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "xTestMnist = xTestMnist.astype('float32') / 255 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### changing normalization for data train: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' also could use : \\nmean = np.mean(xTrainMnist)\\nprint(mean)\\n'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# we normalize tghe data lessing the mean and the standar deviation to xtrain data in order to have a better performance on the training process \n",
        "\n",
        "MnistMean = xTrainMnist.mean()\n",
        "MnistStd = xTrainMnist.std()\n",
        "\n",
        "# normalize the data dividing by the sd assecuring the none zero value of the sd\n",
        "\n",
        "xTrainMnist = (xTrainMnist-MnistMean)/(MnistStd+1e-7)\n",
        "\n",
        "# also normalize the test data using mean and std from training data cause the idea is that the network doesnt know these parameters of the test set\n",
        "\n",
        "xTestMnist = (xTestMnist - MnistMean)/(MnistStd+1e-7)\n",
        "\n",
        "\"\"\" also could use : \n",
        "mean = np.mean(xTrainMnist)\n",
        "print(mean)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### split train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "xTrainMnist, xValidMnist = xTrainMnist[5000:],xTrainMnist[:5000]\n",
        "yTrainMnist, yValidMnist = yTrainMnist[5000:],yTrainMnist[:5000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "look dimension size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((55000, 28, 28), (55000,), (5000, 28, 28), (5000,), (10000, 28, 28), (10000,))"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xTrainMnist.shape, yTrainMnist.shape, xValidMnist.shape, yValidMnist.shape , xTestMnist.shape, yTestMnist.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### data argumentation for best performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "datagen = Tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    width_shift_range = 0.1,\n",
        "    height_shift_range = 0.1,\n",
        "    horizontal_flip = True,\n",
        "    vertical_flip = True\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### see number of classes/labels in order to binarize "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(np.unique(yTrainMnist)) # 10 classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####  binarizing the labels in order to use only categorical cross entropy without sparse \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "yTrainMnist = Tf.keras.utils.to_categorical(yTrainMnist)\n",
        "yTestMnist = Tf.keras.utils.to_categorical(yTestMnist)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [1., 0.]], dtype=float32)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "yTrainMnist[0] # one hot encoding of the labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(yTrainMnist[0:10]) # display the first 10 labels of the training set\n",
        "print(yTestMnist[0:10]) # display the first 10 labels of the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xTrainMnist.shape, yTrainMnist.shape, xTestMnist.shape, yTestMnist.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## creating another structure for the sequential model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "proceed to use a kernel recognition filter on every layer, besides of another techniques to avoid overfitting: \n",
        "\n",
        "- dropout\n",
        "- batch normalization \n",
        "- flatten \n",
        "- global average pooling\n",
        "- regularizacion l1 o l2\n",
        "- estructura de hyperparámetros\n",
        "- funciones de activacion \n",
        "- (PRUNNING & SPARSITY ¿?) \n",
        "    * Función: Eliminan conexiones o neuronas innecesarias en la red, reduciendo la complejidad del modelo y mejorando la eficiencia computacional.​\n",
        "\n",
        "    * Implementación: Se aplican después del entrenamiento inicial para identificar y eliminar pesos insignificantes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "KernelBase = 32 \n",
        "WeightRegularizer = 1e-4 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "MnistModel = Tf.keras.models.Sequential() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "making layer by layer \n",
        "First one is a convolutional sequence of layers : \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "MnistModel.add(Tf.keras.layers.Conv2D(KernelBase, (3,3), padding = 'same', input_shape=(28,28,1), kernel_regularizer = Tf.keras.regularizers.l2(WeightRegularizer)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "once we add the first convolutional layer with 32 filters of size 3x3 and a regularization term to avoid overfitting\n",
        "proceed adding the activation function ReLU to the output of the convolutional layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MnistModel.add(Tf.keras.layers.Activation('relu'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " finally goes with the batch normalization to normalize the output of the previous layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "MnistModel.add(Tf.keras.layers.BatchNormalization())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "also implement a maxpooling 2d layer and dropout on this another layer : \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "MnistModel.add(Tf.keras.layers.Conv2D(KernelBase, (3, 3), padding='same', activation='relu', kernel_regularizer=Tf.keras.regularizers.l2(WeightRegularizer), input_shape=(28, 28, 1)))\n",
        "MnistModel.add(Tf.keras.layers.LeakyReLU(alpha=0.1)) # we use a different activation function to see if it improves the performance of the model\n",
        "MnistModel.add(Tf.keras.layers.BatchNormalization())\n",
        "MnistModel.add(Tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')) \n",
        "MnistModel.add(Tf.keras.layers.Dropout(0.25)) # we add a dropout layer to reduce overfitting "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nLeft this to the end of the model \\n# Add a Flatten layer to convert the 2D images to 1D vectors  ( transforming my 28 x 28 tensor to a 1D array ) \\nBasicMnistModel.add(Tf.keras.layers.Flatten(input_shape=(28, 28)))  \\nBasicMnistModel.summary() # display the summary of the model \"\\n'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Left this to the end of the model \n",
        "# Add a Flatten layer to convert the 2D images to 1D vectors  ( transforming my 28 x 28 tensor to a 1D array ) \n",
        "BasicMnistModel.add(Tf.keras.layers.Flatten(input_shape=(28, 28)))  \n",
        "BasicMnistModel.summary() # display the summary of the model \"\n",
        "\"\"\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# proceed compiling the model \n",
        "MnistModel.compile(optimizer='adam', loss = 'sparse_categorical_crossentropy', metrics= ['accuracy']) # use Adam optimizer and sparse categorical crossentropy loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### proceding training the model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fit the model to the training data\n",
        "MnistHistory = BasicMnistModel.fit(xTrainMnist, yTrainMnist, epochs = 5 , batch_size = 32, validation_split = 0.3, validation_data=(xTestMnist, yTestMnist), verbose = 1) # fit the model to the training data with 5 epochs and a batch size of 32, using 30% of the training data for validation and displaying the progress bar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### lets see accuracy of the Mnist model : \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss, acc = BasicMnistModel.evaluate(xTestMnist, yTestMnist) # evaluate the model on the test set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(MnistHistory.history['loss'], label='Loss (TRAIN)')\n",
        "plt.plot(MnistHistory.history['val_loss'], label='Loss (VALIDATION)')\n",
        "plt.xlabel('EPOCHS')\n",
        "plt.ylabel('LOSS')\n",
        "plt.title('Loss per epoch')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "print(f\" error w/h noisy at the entry of the model : {loss*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(MnistHistory.history['accuracy'], label='accuracy (TRAIN)')\n",
        "plt.plot(MnistHistory.history['val_accuracy'], label='accuracy (VALIDATION)')\n",
        "plt.xlabel('EPOCHS')\n",
        "plt.ylabel('accuracy')\n",
        "plt.title('accuracy per epoch')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "print(f\"accuracy w/h noisy at the entry of the model : {acc*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## proceed with a class prediciton for our random sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "use a random variable to our data sample "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "id = np.random.choice(xTestMnist.shape[0], 1000, replace=False) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MnistPredictions = BasicMnistModel.predict(xTestMnist[id[0]:id[0]+1]) # make predictions on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predicted_class = np.argmax(MnistPredictions, axis=1)\n",
        "plt.imshow(xTestMnist[id[0]])\n",
        "\n",
        "print(\"predicted class:\" , predicted_class)\n",
        "print(\"rial class: \",yTestMnist[id[0]]) # display the true class of the image"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "icrGGZQg7Kb1"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cnnDvlEnv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
