{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CONVOLUTIONAL NEURAL NETWORK NB (CNN-First on DL)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## libraries and env configuration \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "#packages from tensor flow\n",
        "import tensorflow as Tf\n",
        "\n",
        "# tensor flow for optimizing the model \n",
        "\n",
        "\n",
        "# basis packages \n",
        "import os as os \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy \n",
        "import ssl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available and used :   1\n"
          ]
        }
      ],
      "source": [
        "physicalDevice = Tf.config.experimental.list_physical_devices('GPU')\n",
        "Tf.config.experimental.set_memory_growth(physicalDevice[0], True)\n",
        "print(\"Num GPUs Available and used :  \", len(physicalDevice))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '2' # Suppress TensorFlow logging (1)\t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## data preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load MNIST dataset\n",
        "(xTrainMnist,yTrainMnist),(xTestMnist,yTestMnist)=Tf.keras.datasets.mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### normalization "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "255"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xTrainMnist.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "255"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xTestMnist.max()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "#xTestMnist = xTestMnist.astype('float32') / 255 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### changing normalization for data train: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' also could use : \\nmean = np.mean(xTrainMnist)\\nprint(mean)\\n'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# we normalize tghe data lessing the mean and the standar deviation to xtrain data in order to have a better performance on the training process \n",
        "\n",
        "MnistMean = xTrainMnist.mean()\n",
        "MnistStd = xTrainMnist.std()\n",
        "\n",
        "# normalize the data dividing by the sd assecuring the none zero value of the sd\n",
        "\n",
        "xTrainMnist = (xTrainMnist-MnistMean)/(MnistStd+1e-7)\n",
        "\n",
        "# also normalize the test data using mean and std from training data cause the idea is that the network doesnt know these parameters of the test set\n",
        "\n",
        "xTestMnist = (xTestMnist - MnistMean)/(MnistStd+1e-7)\n",
        "\n",
        "\"\"\" also could use : \n",
        "mean = np.mean(xTrainMnist)\n",
        "print(mean)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### split train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "xTrainMnist, xValidMnist = xTrainMnist[5000:],xTrainMnist[:5000]\n",
        "yTrainMnist, yValidMnist = yTrainMnist[5000:],yTrainMnist[:5000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "look dimension size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((55000, 28, 28), (55000,), (5000, 28, 28), (5000,), (10000, 28, 28), (10000,))"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xTrainMnist.shape, yTrainMnist.shape, xValidMnist.shape, yValidMnist.shape , xTestMnist.shape, yTestMnist.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### data argumentation for best performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "datagen = Tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    width_shift_range = 0.1,\n",
        "    height_shift_range = 0.1,\n",
        "    horizontal_flip = True,\n",
        "    vertical_flip = True\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### see number of classes/labels in order to binarize "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(np.unique(yTrainMnist)) # 10 classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####  binarizing the labels in order to use only categorical cross entropy without sparse \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "yTrainMnist = Tf.keras.utils.to_categorical(yTrainMnist,num_classes=10)\n",
        "yTestMnist = Tf.keras.utils.to_categorical(yTestMnist, num_classes= 10)\n",
        "yValidMnist = Tf.keras.utils.to_categorical(yValidMnist, num_classes=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], dtype=float32)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "yTrainMnist[0] # one hot encoding of the labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
            "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ]
        }
      ],
      "source": [
        "print(yTrainMnist[0:10]) # display the first 10 labels of the training set\n",
        "print(yTestMnist[0:10]) # display the first 10 labels of the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((55000, 28, 28), (55000, 10), (10000, 28, 28), (10000, 10))"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xTrainMnist.shape, yTrainMnist.shape, xTestMnist.shape, yTestMnist.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## creating another structure for the sequential model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "proceed to use a kernel recognition filter on every layer, besides of another techniques to avoid overfitting: \n",
        "\n",
        "- dropout\n",
        "- batch normalization \n",
        "- flatten \n",
        "- global average pooling\n",
        "- regularizacion l1 o l2\n",
        "- estructura de hyperparámetros\n",
        "- funciones de activacion \n",
        "- (PRUNNING & SPARSITY ¿?) \n",
        "    * Función: Eliminan conexiones o neuronas innecesarias en la red, reduciendo la complejidad del modelo y mejorando la eficiencia computacional.​\n",
        "\n",
        "    * Implementación: Se aplican después del entrenamiento inicial para identificar y eliminar pesos insignificantes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "KernelBase = 32 \n",
        "WeightRegularizer = 1e-4 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "MnistModel = Tf.keras.models.Sequential() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "making layer by layer \n",
        "First one is a convolutional sequence of layers : \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "MnistModel.add(Tf.keras.layers.Conv2D(KernelBase, (3,3), padding = 'same', input_shape=(28,28,1), kernel_regularizer = Tf.keras.regularizers.l2(WeightRegularizer)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "once we add the first convolutional layer with 32 filters of size 3x3 and a regularization term to avoid overfitting\n",
        "proceed adding the activation function ReLU to the output of the convolutional layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "MnistModel.add(Tf.keras.layers.Activation('relu'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " finally goes with the batch normalization to normalize the output of the previous layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "MnistModel.add(Tf.keras.layers.BatchNormalization())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "also implement a maxpooling 2d layer and dropout on this another layer : \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "MnistModel.add(Tf.keras.layers.Conv2D(KernelBase, (3, 3), padding='same', activation='relu', kernel_regularizer=Tf.keras.regularizers.l2(WeightRegularizer), input_shape=(28, 28, 1)))\n",
        "MnistModel.add(Tf.keras.layers.LeakyReLU(alpha=0.1)) # we use a different activation function to see if it improves the performance of the model\n",
        "MnistModel.add(Tf.keras.layers.BatchNormalization())\n",
        "MnistModel.add(Tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')) \n",
        "MnistModel.add(Tf.keras.layers.Dropout(0.25)) # we add a dropout layer to reduce overfitting "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "now, we nee to do the last classification layer with a dense and a flatten function "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add a Flatten layer to convert the 2D images to 1D vectors  (transform the img to an array)\n",
        "MnistModel.add(Tf.keras.layers.Flatten()) # without the input size "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "finally,  10 classes for the output layer with a softmax activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "MnistModel.add(Tf.keras.layers.Dense(10,activation= 'softmax'))  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "lets see a summary of the structure: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 28, 28, 32)        320       \n",
            "                                                                 \n",
            " activation (Activation)     (None, 28, 28, 32)        0         \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 28, 28, 32)       128       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 28, 28, 32)        9248      \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 28, 28, 32)        0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 28, 28, 32)       128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 14, 14, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 14, 14, 32)        0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 6272)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                62730     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 72,554\n",
            "Trainable params: 72,426\n",
            "Non-trainable params: 128\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "MnistModel.summary() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "proceed compiling the model \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "MnistModel.compile(optimizer=Tf.keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy']) # we use adam optimizer with a learning rate of 0.001 and categorical cross entropy as loss function\n",
        "# use addam optimizer with a learning rate of 0.001 and categorical cross entropy as loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "also we can do a callback checkpoint when were we are fitting the model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "CBCheckPoint = Tf.keras.callbacks.ModelCheckpoint(\"best_model,keras\", verbose = 1, save_best_only = True, monitor = \"val_accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### proceding training the model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Asegúrate de que los datos tengan 4 dimensiones\n",
        "xTrainMnist = np.expand_dims(xTrainMnist, axis=-1)  # Agrega la dimensión del canal\n",
        "xValidMnist = np.expand_dims(xValidMnist, axis=-1)  # Agrega la dimensión del canal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "\n",
            "Epoch 1: val_accuracy improved from -inf to 0.66080, saving model to best_model,keras\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "429/429 - 42s - loss: 1.2438 - accuracy: 0.6089 - val_loss: 0.9951 - val_accuracy: 0.6608 - 42s/epoch - 99ms/step\n",
            "Epoch 2/1000\n",
            "\n",
            "Epoch 2: val_accuracy improved from 0.66080 to 0.80460, saving model to best_model,keras\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "429/429 - 19s - loss: 0.8163 - accuracy: 0.7326 - val_loss: 0.5972 - val_accuracy: 0.8046 - 19s/epoch - 45ms/step\n",
            "Epoch 3/1000\n",
            "\n",
            "Epoch 3: val_accuracy improved from 0.80460 to 0.83600, saving model to best_model,keras\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "429/429 - 19s - loss: 0.6719 - accuracy: 0.7763 - val_loss: 0.4899 - val_accuracy: 0.8360 - 19s/epoch - 45ms/step\n",
            "Epoch 4/1000\n",
            "\n",
            "Epoch 4: val_accuracy did not improve from 0.83600\n",
            "429/429 - 17s - loss: 0.5873 - accuracy: 0.8041 - val_loss: 0.5614 - val_accuracy: 0.8336 - 17s/epoch - 40ms/step\n",
            "Epoch 5/1000\n",
            "\n",
            "Epoch 5: val_accuracy improved from 0.83600 to 0.83860, saving model to best_model,keras\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "429/429 - 18s - loss: 0.5317 - accuracy: 0.8229 - val_loss: 0.5140 - val_accuracy: 0.8386 - 18s/epoch - 42ms/step\n",
            "Epoch 6/1000\n",
            "\n",
            "Epoch 6: val_accuracy improved from 0.83860 to 0.88880, saving model to best_model,keras\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "429/429 - 19s - loss: 0.4998 - accuracy: 0.8334 - val_loss: 0.3479 - val_accuracy: 0.8888 - 19s/epoch - 44ms/step\n",
            "Epoch 7/1000\n",
            "\n",
            "Epoch 7: val_accuracy did not improve from 0.88880\n",
            "429/429 - 19s - loss: 0.4756 - accuracy: 0.8427 - val_loss: 0.3822 - val_accuracy: 0.8802 - 19s/epoch - 45ms/step\n",
            "Epoch 8/1000\n",
            "\n",
            "Epoch 8: val_accuracy improved from 0.88880 to 0.89540, saving model to best_model,keras\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "429/429 - 23s - loss: 0.4426 - accuracy: 0.8541 - val_loss: 0.3593 - val_accuracy: 0.8954 - 23s/epoch - 54ms/step\n",
            "Epoch 9/1000\n",
            "\n",
            "Epoch 9: val_accuracy did not improve from 0.89540\n",
            "429/429 - 24s - loss: 0.4398 - accuracy: 0.8555 - val_loss: 0.3391 - val_accuracy: 0.8888 - 24s/epoch - 55ms/step\n",
            "Epoch 10/1000\n",
            "\n",
            "Epoch 10: val_accuracy improved from 0.89540 to 0.90560, saving model to best_model,keras\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "429/429 - 27s - loss: 0.4178 - accuracy: 0.8613 - val_loss: 0.2931 - val_accuracy: 0.9056 - 27s/epoch - 63ms/step\n",
            "Epoch 11/1000\n",
            "\n",
            "Epoch 11: val_accuracy did not improve from 0.90560\n",
            "429/429 - 23s - loss: 0.4142 - accuracy: 0.8623 - val_loss: 0.3101 - val_accuracy: 0.9000 - 23s/epoch - 55ms/step\n",
            "Epoch 12/1000\n",
            "\n",
            "Epoch 12: val_accuracy did not improve from 0.90560\n",
            "429/429 - 19s - loss: 0.3999 - accuracy: 0.8681 - val_loss: 0.3090 - val_accuracy: 0.9028 - 19s/epoch - 44ms/step\n",
            "Epoch 13/1000\n",
            "\n",
            "Epoch 13: val_accuracy improved from 0.90560 to 0.91800, saving model to best_model,keras\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "429/429 - 27s - loss: 0.3905 - accuracy: 0.8726 - val_loss: 0.2773 - val_accuracy: 0.9180 - 27s/epoch - 62ms/step\n",
            "Epoch 14/1000\n",
            "\n",
            "Epoch 14: val_accuracy improved from 0.91800 to 0.92120, saving model to best_model,keras\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "429/429 - 21s - loss: 0.3915 - accuracy: 0.8718 - val_loss: 0.2568 - val_accuracy: 0.9212 - 21s/epoch - 50ms/step\n",
            "Epoch 15/1000\n",
            "\n",
            "Epoch 15: val_accuracy did not improve from 0.92120\n",
            "429/429 - 17s - loss: 0.3782 - accuracy: 0.8764 - val_loss: 0.2782 - val_accuracy: 0.9170 - 17s/epoch - 40ms/step\n",
            "Epoch 16/1000\n",
            "\n",
            "Epoch 16: val_accuracy did not improve from 0.92120\n",
            "429/429 - 16s - loss: 0.3764 - accuracy: 0.8774 - val_loss: 0.2974 - val_accuracy: 0.9118 - 16s/epoch - 38ms/step\n",
            "Epoch 17/1000\n",
            "\n",
            "Epoch 17: val_accuracy did not improve from 0.92120\n",
            "429/429 - 16s - loss: 0.3750 - accuracy: 0.8789 - val_loss: 0.2636 - val_accuracy: 0.9160 - 16s/epoch - 38ms/step\n",
            "Epoch 18/1000\n",
            "\n",
            "Epoch 18: val_accuracy did not improve from 0.92120\n",
            "429/429 - 16s - loss: 0.3642 - accuracy: 0.8803 - val_loss: 0.2891 - val_accuracy: 0.9066 - 16s/epoch - 38ms/step\n",
            "Epoch 19/1000\n",
            "\n",
            "Epoch 19: val_accuracy did not improve from 0.92120\n",
            "429/429 - 16s - loss: 0.3597 - accuracy: 0.8798 - val_loss: 0.2792 - val_accuracy: 0.9050 - 16s/epoch - 38ms/step\n",
            "Epoch 20/1000\n",
            "\n",
            "Epoch 20: val_accuracy did not improve from 0.92120\n",
            "429/429 - 17s - loss: 0.3611 - accuracy: 0.8814 - val_loss: 0.2981 - val_accuracy: 0.8950 - 17s/epoch - 39ms/step\n",
            "Epoch 21/1000\n",
            "\n",
            "Epoch 21: val_accuracy improved from 0.92120 to 0.93040, saving model to best_model,keras\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "429/429 - 18s - loss: 0.3584 - accuracy: 0.8835 - val_loss: 0.2381 - val_accuracy: 0.9304 - 18s/epoch - 43ms/step\n",
            "Epoch 22/1000\n",
            "\n",
            "Epoch 22: val_accuracy did not improve from 0.93040\n",
            "429/429 - 17s - loss: 0.3523 - accuracy: 0.8855 - val_loss: 0.2912 - val_accuracy: 0.9022 - 17s/epoch - 40ms/step\n",
            "Epoch 23/1000\n",
            "\n",
            "Epoch 23: val_accuracy did not improve from 0.93040\n",
            "429/429 - 17s - loss: 0.3484 - accuracy: 0.8863 - val_loss: 0.3778 - val_accuracy: 0.8768 - 17s/epoch - 40ms/step\n",
            "Epoch 24/1000\n",
            "\n",
            "Epoch 24: val_accuracy did not improve from 0.93040\n",
            "429/429 - 17s - loss: 0.3489 - accuracy: 0.8864 - val_loss: 0.4772 - val_accuracy: 0.8532 - 17s/epoch - 40ms/step\n",
            "Epoch 25/1000\n",
            "\n",
            "Epoch 25: val_accuracy did not improve from 0.93040\n",
            "429/429 - 19s - loss: 0.3408 - accuracy: 0.8881 - val_loss: 0.2578 - val_accuracy: 0.9244 - 19s/epoch - 43ms/step\n",
            "Epoch 26/1000\n",
            "\n",
            "Epoch 26: val_accuracy did not improve from 0.93040\n",
            "429/429 - 22s - loss: 0.3419 - accuracy: 0.8882 - val_loss: 0.3742 - val_accuracy: 0.8806 - 22s/epoch - 51ms/step\n",
            "Epoch 27/1000\n",
            "\n",
            "Epoch 27: val_accuracy did not improve from 0.93040\n",
            "429/429 - 20s - loss: 0.3402 - accuracy: 0.8894 - val_loss: 0.2752 - val_accuracy: 0.9144 - 20s/epoch - 46ms/step\n",
            "Epoch 28/1000\n",
            "\n",
            "Epoch 28: val_accuracy did not improve from 0.93040\n",
            "429/429 - 18s - loss: 0.3343 - accuracy: 0.8923 - val_loss: 0.2824 - val_accuracy: 0.9164 - 18s/epoch - 41ms/step\n",
            "Epoch 29/1000\n",
            "\n",
            "Epoch 29: val_accuracy did not improve from 0.93040\n",
            "429/429 - 17s - loss: 0.3331 - accuracy: 0.8922 - val_loss: 0.2352 - val_accuracy: 0.9256 - 17s/epoch - 39ms/step\n",
            "Epoch 30/1000\n",
            "\n",
            "Epoch 30: val_accuracy did not improve from 0.93040\n",
            "429/429 - 19s - loss: 0.3340 - accuracy: 0.8914 - val_loss: 0.2709 - val_accuracy: 0.9198 - 19s/epoch - 44ms/step\n",
            "Epoch 31/1000\n",
            "\n",
            "Epoch 31: val_accuracy did not improve from 0.93040\n",
            "429/429 - 17s - loss: 0.3260 - accuracy: 0.8951 - val_loss: 0.2487 - val_accuracy: 0.9276 - 17s/epoch - 39ms/step\n",
            "Epoch 32/1000\n",
            "\n",
            "Epoch 32: val_accuracy improved from 0.93040 to 0.93160, saving model to best_model,keras\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "429/429 - 18s - loss: 0.3297 - accuracy: 0.8934 - val_loss: 0.2346 - val_accuracy: 0.9316 - 18s/epoch - 42ms/step\n",
            "Epoch 33/1000\n",
            "\n",
            "Epoch 33: val_accuracy did not improve from 0.93160\n",
            "429/429 - 18s - loss: 0.3294 - accuracy: 0.8951 - val_loss: 0.2705 - val_accuracy: 0.9146 - 18s/epoch - 41ms/step\n",
            "Epoch 34/1000\n",
            "\n",
            "Epoch 34: val_accuracy improved from 0.93160 to 0.94360, saving model to best_model,keras\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "429/429 - 50s - loss: 0.3277 - accuracy: 0.8949 - val_loss: 0.2199 - val_accuracy: 0.9436 - 50s/epoch - 117ms/step\n",
            "Epoch 35/1000\n",
            "\n",
            "Epoch 35: val_accuracy did not improve from 0.94360\n",
            "429/429 - 65s - loss: 0.3271 - accuracy: 0.8945 - val_loss: 0.2623 - val_accuracy: 0.9204 - 65s/epoch - 151ms/step\n",
            "Epoch 36/1000\n",
            "\n",
            "Epoch 36: val_accuracy did not improve from 0.94360\n",
            "429/429 - 26s - loss: 0.3224 - accuracy: 0.8957 - val_loss: 0.2461 - val_accuracy: 0.9270 - 26s/epoch - 60ms/step\n",
            "Epoch 37/1000\n",
            "\n",
            "Epoch 37: val_accuracy did not improve from 0.94360\n",
            "429/429 - 24s - loss: 0.3180 - accuracy: 0.8980 - val_loss: 0.2209 - val_accuracy: 0.9328 - 24s/epoch - 55ms/step\n",
            "Epoch 38/1000\n",
            "\n",
            "Epoch 38: val_accuracy did not improve from 0.94360\n",
            "429/429 - 25s - loss: 0.3212 - accuracy: 0.8961 - val_loss: 0.2678 - val_accuracy: 0.9216 - 25s/epoch - 59ms/step\n",
            "Epoch 39/1000\n",
            "\n",
            "Epoch 39: val_accuracy did not improve from 0.94360\n",
            "429/429 - 29s - loss: 0.3173 - accuracy: 0.8981 - val_loss: 0.2343 - val_accuracy: 0.9314 - 29s/epoch - 68ms/step\n",
            "Epoch 40/1000\n",
            "\n",
            "Epoch 40: val_accuracy did not improve from 0.94360\n",
            "429/429 - 24s - loss: 0.3210 - accuracy: 0.8985 - val_loss: 0.2642 - val_accuracy: 0.9162 - 24s/epoch - 56ms/step\n",
            "Epoch 41/1000\n",
            "\n",
            "Epoch 41: val_accuracy did not improve from 0.94360\n",
            "429/429 - 23s - loss: 0.3171 - accuracy: 0.8989 - val_loss: 0.2641 - val_accuracy: 0.9178 - 23s/epoch - 53ms/step\n",
            "Epoch 42/1000\n",
            "\n",
            "Epoch 42: val_accuracy did not improve from 0.94360\n",
            "429/429 - 25s - loss: 0.3135 - accuracy: 0.9002 - val_loss: 0.2023 - val_accuracy: 0.9388 - 25s/epoch - 58ms/step\n",
            "Epoch 43/1000\n",
            "\n",
            "Epoch 43: val_accuracy did not improve from 0.94360\n",
            "429/429 - 26s - loss: 0.3165 - accuracy: 0.8981 - val_loss: 0.2856 - val_accuracy: 0.9120 - 26s/epoch - 61ms/step\n",
            "Epoch 44/1000\n",
            "\n",
            "Epoch 44: val_accuracy did not improve from 0.94360\n",
            "429/429 - 28s - loss: 0.3095 - accuracy: 0.9007 - val_loss: 0.2700 - val_accuracy: 0.9166 - 28s/epoch - 64ms/step\n",
            "Epoch 45/1000\n",
            "\n",
            "Epoch 45: val_accuracy did not improve from 0.94360\n",
            "429/429 - 30s - loss: 0.3128 - accuracy: 0.9003 - val_loss: 0.2233 - val_accuracy: 0.9282 - 30s/epoch - 70ms/step\n",
            "Epoch 46/1000\n",
            "\n",
            "Epoch 46: val_accuracy did not improve from 0.94360\n",
            "429/429 - 39s - loss: 0.3088 - accuracy: 0.9012 - val_loss: 0.2976 - val_accuracy: 0.9068 - 39s/epoch - 90ms/step\n",
            "Epoch 47/1000\n",
            "\n",
            "Epoch 47: val_accuracy did not improve from 0.94360\n",
            "429/429 - 25s - loss: 0.3085 - accuracy: 0.9015 - val_loss: 0.2467 - val_accuracy: 0.9266 - 25s/epoch - 57ms/step\n",
            "Epoch 48/1000\n",
            "\n",
            "Epoch 48: val_accuracy did not improve from 0.94360\n",
            "429/429 - 92s - loss: 0.3109 - accuracy: 0.9001 - val_loss: 0.1946 - val_accuracy: 0.9430 - 92s/epoch - 214ms/step\n",
            "Epoch 49/1000\n",
            "\n",
            "Epoch 49: val_accuracy did not improve from 0.94360\n",
            "429/429 - 49s - loss: 0.3082 - accuracy: 0.9024 - val_loss: 0.2454 - val_accuracy: 0.9182 - 49s/epoch - 114ms/step\n",
            "Epoch 50/1000\n",
            "\n",
            "Epoch 50: val_accuracy did not improve from 0.94360\n",
            "429/429 - 32s - loss: 0.3092 - accuracy: 0.9025 - val_loss: 0.2665 - val_accuracy: 0.9176 - 32s/epoch - 74ms/step\n",
            "Epoch 51/1000\n",
            "\n",
            "Epoch 51: val_accuracy did not improve from 0.94360\n",
            "429/429 - 25s - loss: 0.3071 - accuracy: 0.9021 - val_loss: 0.1928 - val_accuracy: 0.9436 - 25s/epoch - 59ms/step\n",
            "Epoch 52/1000\n",
            "\n",
            "Epoch 52: val_accuracy did not improve from 0.94360\n",
            "429/429 - 25s - loss: 0.3096 - accuracy: 0.9008 - val_loss: 0.2304 - val_accuracy: 0.9264 - 25s/epoch - 59ms/step\n",
            "Epoch 53/1000\n",
            "\n",
            "Epoch 53: val_accuracy did not improve from 0.94360\n",
            "429/429 - 42s - loss: 0.3082 - accuracy: 0.9022 - val_loss: 0.2302 - val_accuracy: 0.9316 - 42s/epoch - 98ms/step\n",
            "Epoch 54/1000\n",
            "\n",
            "Epoch 54: val_accuracy did not improve from 0.94360\n",
            "429/429 - 38s - loss: 0.3015 - accuracy: 0.9041 - val_loss: 0.2433 - val_accuracy: 0.9244 - 38s/epoch - 87ms/step\n",
            "Epoch 55/1000\n",
            "\n",
            "Epoch 55: val_accuracy did not improve from 0.94360\n",
            "429/429 - 37s - loss: 0.3070 - accuracy: 0.9033 - val_loss: 0.2079 - val_accuracy: 0.9422 - 37s/epoch - 86ms/step\n",
            "Epoch 56/1000\n",
            "\n",
            "Epoch 56: val_accuracy did not improve from 0.94360\n",
            "429/429 - 46s - loss: 0.3024 - accuracy: 0.9053 - val_loss: 0.2665 - val_accuracy: 0.9152 - 46s/epoch - 108ms/step\n",
            "Epoch 57/1000\n",
            "\n",
            "Epoch 57: val_accuracy did not improve from 0.94360\n",
            "429/429 - 52s - loss: 0.3010 - accuracy: 0.9047 - val_loss: 0.2063 - val_accuracy: 0.9386 - 52s/epoch - 122ms/step\n",
            "Epoch 58/1000\n",
            "\n",
            "Epoch 58: val_accuracy did not improve from 0.94360\n",
            "429/429 - 35s - loss: 0.3019 - accuracy: 0.9042 - val_loss: 0.2115 - val_accuracy: 0.9372 - 35s/epoch - 82ms/step\n",
            "Epoch 59/1000\n",
            "\n",
            "Epoch 59: val_accuracy did not improve from 0.94360\n",
            "429/429 - 23s - loss: 0.2990 - accuracy: 0.9049 - val_loss: 0.2101 - val_accuracy: 0.9380 - 23s/epoch - 54ms/step\n",
            "Epoch 60/1000\n",
            "\n",
            "Epoch 60: val_accuracy did not improve from 0.94360\n",
            "429/429 - 29s - loss: 0.2964 - accuracy: 0.9051 - val_loss: 0.1950 - val_accuracy: 0.9424 - 29s/epoch - 68ms/step\n",
            "Epoch 61/1000\n",
            "\n",
            "Epoch 61: val_accuracy did not improve from 0.94360\n",
            "429/429 - 42s - loss: 0.2983 - accuracy: 0.9049 - val_loss: 0.2232 - val_accuracy: 0.9324 - 42s/epoch - 98ms/step\n",
            "Epoch 62/1000\n",
            "\n",
            "Epoch 62: val_accuracy did not improve from 0.94360\n",
            "429/429 - 51s - loss: 0.2983 - accuracy: 0.9054 - val_loss: 0.2220 - val_accuracy: 0.9354 - 51s/epoch - 118ms/step\n",
            "Epoch 63/1000\n",
            "\n",
            "Epoch 63: val_accuracy did not improve from 0.94360\n",
            "429/429 - 40s - loss: 0.3008 - accuracy: 0.9041 - val_loss: 0.2315 - val_accuracy: 0.9258 - 40s/epoch - 94ms/step\n",
            "Epoch 64/1000\n",
            "\n",
            "Epoch 64: val_accuracy did not improve from 0.94360\n",
            "429/429 - 41s - loss: 0.2980 - accuracy: 0.9051 - val_loss: 0.2288 - val_accuracy: 0.9322 - 41s/epoch - 96ms/step\n",
            "Epoch 65/1000\n",
            "\n",
            "Epoch 65: val_accuracy did not improve from 0.94360\n",
            "429/429 - 41s - loss: 0.2940 - accuracy: 0.9062 - val_loss: 0.2594 - val_accuracy: 0.9158 - 41s/epoch - 96ms/step\n",
            "Epoch 66/1000\n",
            "\n",
            "Epoch 66: val_accuracy did not improve from 0.94360\n",
            "429/429 - 37s - loss: 0.2995 - accuracy: 0.9058 - val_loss: 0.2381 - val_accuracy: 0.9270 - 37s/epoch - 85ms/step\n",
            "Epoch 67/1000\n",
            "\n",
            "Epoch 67: val_accuracy did not improve from 0.94360\n",
            "429/429 - 55s - loss: 0.2969 - accuracy: 0.9072 - val_loss: 0.2387 - val_accuracy: 0.9248 - 55s/epoch - 129ms/step\n",
            "Epoch 68/1000\n",
            "\n",
            "Epoch 68: val_accuracy did not improve from 0.94360\n",
            "429/429 - 30s - loss: 0.3008 - accuracy: 0.9044 - val_loss: 0.2120 - val_accuracy: 0.9332 - 30s/epoch - 69ms/step\n",
            "Epoch 69/1000\n",
            "\n",
            "Epoch 69: val_accuracy did not improve from 0.94360\n",
            "429/429 - 27s - loss: 0.2966 - accuracy: 0.9070 - val_loss: 0.2520 - val_accuracy: 0.9278 - 27s/epoch - 63ms/step\n",
            "Epoch 70/1000\n",
            "\n",
            "Epoch 70: val_accuracy did not improve from 0.94360\n",
            "429/429 - 68s - loss: 0.2961 - accuracy: 0.9059 - val_loss: 0.2258 - val_accuracy: 0.9330 - 68s/epoch - 159ms/step\n",
            "Epoch 71/1000\n",
            "\n",
            "Epoch 71: val_accuracy did not improve from 0.94360\n",
            "429/429 - 68s - loss: 0.2981 - accuracy: 0.9067 - val_loss: 0.2372 - val_accuracy: 0.9300 - 68s/epoch - 160ms/step\n",
            "Epoch 72/1000\n",
            "\n",
            "Epoch 72: val_accuracy did not improve from 0.94360\n",
            "429/429 - 70s - loss: 0.2915 - accuracy: 0.9060 - val_loss: 0.3003 - val_accuracy: 0.8998 - 70s/epoch - 164ms/step\n",
            "Epoch 73/1000\n",
            "\n",
            "Epoch 73: val_accuracy did not improve from 0.94360\n",
            "429/429 - 35s - loss: 0.2970 - accuracy: 0.9061 - val_loss: 0.2286 - val_accuracy: 0.9288 - 35s/epoch - 81ms/step\n",
            "Epoch 74/1000\n",
            "\n",
            "Epoch 74: val_accuracy did not improve from 0.94360\n",
            "429/429 - 38s - loss: 0.2971 - accuracy: 0.9061 - val_loss: 0.2310 - val_accuracy: 0.9332 - 38s/epoch - 88ms/step\n",
            "Epoch 75/1000\n",
            "\n",
            "Epoch 75: val_accuracy improved from 0.94360 to 0.94600, saving model to best_model,keras\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "429/429 - 28s - loss: 0.2935 - accuracy: 0.9083 - val_loss: 0.1919 - val_accuracy: 0.9460 - 28s/epoch - 66ms/step\n",
            "Epoch 76/1000\n",
            "\n",
            "Epoch 76: val_accuracy did not improve from 0.94600\n",
            "429/429 - 22s - loss: 0.2905 - accuracy: 0.9087 - val_loss: 0.2271 - val_accuracy: 0.9364 - 22s/epoch - 50ms/step\n",
            "Epoch 77/1000\n",
            "\n",
            "Epoch 77: val_accuracy did not improve from 0.94600\n",
            "429/429 - 17s - loss: 0.2935 - accuracy: 0.9058 - val_loss: 0.2275 - val_accuracy: 0.9306 - 17s/epoch - 40ms/step\n",
            "Epoch 78/1000\n",
            "\n",
            "Epoch 78: val_accuracy did not improve from 0.94600\n",
            "429/429 - 48s - loss: 0.2883 - accuracy: 0.9086 - val_loss: 0.2300 - val_accuracy: 0.9312 - 48s/epoch - 112ms/step\n",
            "Epoch 79/1000\n",
            "\n",
            "Epoch 79: val_accuracy did not improve from 0.94600\n",
            "429/429 - 40s - loss: 0.2914 - accuracy: 0.9090 - val_loss: 0.2128 - val_accuracy: 0.9394 - 40s/epoch - 93ms/step\n",
            "Epoch 80/1000\n",
            "\n",
            "Epoch 80: val_accuracy did not improve from 0.94600\n",
            "429/429 - 31s - loss: 0.2937 - accuracy: 0.9065 - val_loss: 0.2086 - val_accuracy: 0.9402 - 31s/epoch - 72ms/step\n",
            "Epoch 81/1000\n",
            "\n",
            "Epoch 81: val_accuracy did not improve from 0.94600\n",
            "429/429 - 34s - loss: 0.2945 - accuracy: 0.9066 - val_loss: 0.2081 - val_accuracy: 0.9374 - 34s/epoch - 78ms/step\n",
            "Epoch 82/1000\n",
            "\n",
            "Epoch 82: val_accuracy did not improve from 0.94600\n",
            "429/429 - 34s - loss: 0.2885 - accuracy: 0.9084 - val_loss: 0.1866 - val_accuracy: 0.9448 - 34s/epoch - 78ms/step\n",
            "Epoch 83/1000\n",
            "\n",
            "Epoch 83: val_accuracy did not improve from 0.94600\n",
            "429/429 - 34s - loss: 0.2906 - accuracy: 0.9086 - val_loss: 0.1992 - val_accuracy: 0.9430 - 34s/epoch - 80ms/step\n",
            "Epoch 84/1000\n",
            "\n",
            "Epoch 84: val_accuracy did not improve from 0.94600\n",
            "429/429 - 34s - loss: 0.2902 - accuracy: 0.9075 - val_loss: 0.2403 - val_accuracy: 0.9262 - 34s/epoch - 79ms/step\n",
            "Epoch 85/1000\n",
            "\n",
            "Epoch 85: val_accuracy did not improve from 0.94600\n",
            "429/429 - 35s - loss: 0.2879 - accuracy: 0.9097 - val_loss: 0.2217 - val_accuracy: 0.9296 - 35s/epoch - 81ms/step\n",
            "Epoch 86/1000\n",
            "\n",
            "Epoch 86: val_accuracy did not improve from 0.94600\n",
            "429/429 - 49s - loss: 0.2927 - accuracy: 0.9072 - val_loss: 0.2264 - val_accuracy: 0.9312 - 49s/epoch - 115ms/step\n",
            "Epoch 87/1000\n",
            "\n",
            "Epoch 87: val_accuracy did not improve from 0.94600\n",
            "429/429 - 42s - loss: 0.2895 - accuracy: 0.9084 - val_loss: 0.2278 - val_accuracy: 0.9314 - 42s/epoch - 97ms/step\n",
            "Epoch 88/1000\n",
            "\n",
            "Epoch 88: val_accuracy did not improve from 0.94600\n",
            "429/429 - 45s - loss: 0.2921 - accuracy: 0.9079 - val_loss: 0.2239 - val_accuracy: 0.9296 - 45s/epoch - 104ms/step\n",
            "Epoch 89/1000\n",
            "\n",
            "Epoch 89: val_accuracy did not improve from 0.94600\n",
            "429/429 - 23s - loss: 0.2900 - accuracy: 0.9082 - val_loss: 0.2345 - val_accuracy: 0.9302 - 23s/epoch - 53ms/step\n",
            "Epoch 90/1000\n",
            "\n",
            "Epoch 90: val_accuracy did not improve from 0.94600\n",
            "429/429 - 17s - loss: 0.2903 - accuracy: 0.9079 - val_loss: 0.2251 - val_accuracy: 0.9324 - 17s/epoch - 40ms/step\n",
            "Epoch 91/1000\n",
            "\n",
            "Epoch 91: val_accuracy did not improve from 0.94600\n",
            "429/429 - 17s - loss: 0.2859 - accuracy: 0.9112 - val_loss: 0.2150 - val_accuracy: 0.9360 - 17s/epoch - 39ms/step\n",
            "Epoch 92/1000\n",
            "\n",
            "Epoch 92: val_accuracy did not improve from 0.94600\n",
            "429/429 - 17s - loss: 0.2897 - accuracy: 0.9081 - val_loss: 0.2239 - val_accuracy: 0.9328 - 17s/epoch - 39ms/step\n",
            "Epoch 93/1000\n",
            "\n",
            "Epoch 93: val_accuracy did not improve from 0.94600\n",
            "429/429 - 17s - loss: 0.2904 - accuracy: 0.9096 - val_loss: 0.2132 - val_accuracy: 0.9374 - 17s/epoch - 38ms/step\n",
            "Epoch 94/1000\n",
            "\n",
            "Epoch 94: val_accuracy did not improve from 0.94600\n",
            "429/429 - 16s - loss: 0.2936 - accuracy: 0.9072 - val_loss: 0.1993 - val_accuracy: 0.9414 - 16s/epoch - 38ms/step\n",
            "Epoch 95/1000\n",
            "\n",
            "Epoch 95: val_accuracy did not improve from 0.94600\n",
            "429/429 - 16s - loss: 0.2855 - accuracy: 0.9106 - val_loss: 0.2142 - val_accuracy: 0.9374 - 16s/epoch - 38ms/step\n",
            "Epoch 96/1000\n",
            "\n",
            "Epoch 96: val_accuracy did not improve from 0.94600\n",
            "429/429 - 17s - loss: 0.2891 - accuracy: 0.9096 - val_loss: 0.2012 - val_accuracy: 0.9458 - 17s/epoch - 40ms/step\n",
            "Epoch 97/1000\n",
            "\n",
            "Epoch 97: val_accuracy improved from 0.94600 to 0.94660, saving model to best_model,keras\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "429/429 - 19s - loss: 0.2866 - accuracy: 0.9101 - val_loss: 0.1932 - val_accuracy: 0.9466 - 19s/epoch - 44ms/step\n",
            "Epoch 98/1000\n",
            "\n",
            "Epoch 98: val_accuracy did not improve from 0.94660\n",
            "429/429 - 18s - loss: 0.2863 - accuracy: 0.9083 - val_loss: 0.1994 - val_accuracy: 0.9398 - 18s/epoch - 41ms/step\n",
            "Epoch 99/1000\n",
            "\n",
            "Epoch 99: val_accuracy did not improve from 0.94660\n",
            "429/429 - 17s - loss: 0.2893 - accuracy: 0.9080 - val_loss: 0.2149 - val_accuracy: 0.9398 - 17s/epoch - 40ms/step\n",
            "Epoch 100/1000\n",
            "\n",
            "Epoch 100: val_accuracy did not improve from 0.94660\n",
            "429/429 - 17s - loss: 0.2855 - accuracy: 0.9111 - val_loss: 0.2057 - val_accuracy: 0.9396 - 17s/epoch - 40ms/step\n",
            "Epoch 101/1000\n",
            "\n",
            "Epoch 101: val_accuracy did not improve from 0.94660\n",
            "429/429 - 18s - loss: 0.2873 - accuracy: 0.9105 - val_loss: 0.2430 - val_accuracy: 0.9282 - 18s/epoch - 41ms/step\n",
            "Epoch 102/1000\n",
            "\n",
            "Epoch 102: val_accuracy did not improve from 0.94660\n",
            "429/429 - 17s - loss: 0.2895 - accuracy: 0.9090 - val_loss: 0.2047 - val_accuracy: 0.9438 - 17s/epoch - 40ms/step\n",
            "Epoch 103/1000\n",
            "\n",
            "Epoch 103: val_accuracy did not improve from 0.94660\n",
            "429/429 - 18s - loss: 0.2882 - accuracy: 0.9084 - val_loss: 0.2467 - val_accuracy: 0.9234 - 18s/epoch - 43ms/step\n",
            "Epoch 104/1000\n",
            "\n",
            "Epoch 104: val_accuracy did not improve from 0.94660\n",
            "429/429 - 18s - loss: 0.2811 - accuracy: 0.9118 - val_loss: 0.2951 - val_accuracy: 0.9086 - 18s/epoch - 41ms/step\n",
            "Epoch 105/1000\n",
            "\n",
            "Epoch 105: val_accuracy did not improve from 0.94660\n",
            "429/429 - 16s - loss: 0.2888 - accuracy: 0.9090 - val_loss: 0.2297 - val_accuracy: 0.9306 - 16s/epoch - 38ms/step\n",
            "Epoch 106/1000\n",
            "\n",
            "Epoch 106: val_accuracy did not improve from 0.94660\n",
            "429/429 - 17s - loss: 0.2895 - accuracy: 0.9085 - val_loss: 0.2162 - val_accuracy: 0.9396 - 17s/epoch - 39ms/step\n",
            "Epoch 107/1000\n",
            "\n",
            "Epoch 107: val_accuracy did not improve from 0.94660\n",
            "429/429 - 16s - loss: 0.2876 - accuracy: 0.9104 - val_loss: 0.1933 - val_accuracy: 0.9428 - 16s/epoch - 38ms/step\n",
            "Epoch 108/1000\n",
            "\n",
            "Epoch 108: val_accuracy did not improve from 0.94660\n",
            "429/429 - 16s - loss: 0.2846 - accuracy: 0.9115 - val_loss: 0.2379 - val_accuracy: 0.9248 - 16s/epoch - 38ms/step\n",
            "Epoch 109/1000\n",
            "\n",
            "Epoch 109: val_accuracy did not improve from 0.94660\n",
            "429/429 - 16s - loss: 0.2873 - accuracy: 0.9110 - val_loss: 0.2516 - val_accuracy: 0.9224 - 16s/epoch - 37ms/step\n",
            "Epoch 110/1000\n",
            "\n",
            "Epoch 110: val_accuracy did not improve from 0.94660\n",
            "429/429 - 16s - loss: 0.2847 - accuracy: 0.9109 - val_loss: 0.2063 - val_accuracy: 0.9366 - 16s/epoch - 37ms/step\n",
            "Epoch 111/1000\n",
            "\n",
            "Epoch 111: val_accuracy did not improve from 0.94660\n",
            "429/429 - 17s - loss: 0.2846 - accuracy: 0.9105 - val_loss: 0.2305 - val_accuracy: 0.9280 - 17s/epoch - 40ms/step\n",
            "Epoch 112/1000\n",
            "\n",
            "Epoch 112: val_accuracy did not improve from 0.94660\n",
            "429/429 - 18s - loss: 0.2826 - accuracy: 0.9105 - val_loss: 0.2540 - val_accuracy: 0.9230 - 18s/epoch - 42ms/step\n",
            "Epoch 113/1000\n",
            "\n",
            "Epoch 113: val_accuracy did not improve from 0.94660\n",
            "429/429 - 17s - loss: 0.2859 - accuracy: 0.9101 - val_loss: 0.2095 - val_accuracy: 0.9388 - 17s/epoch - 39ms/step\n",
            "Epoch 114/1000\n",
            "\n",
            "Epoch 114: val_accuracy did not improve from 0.94660\n",
            "429/429 - 17s - loss: 0.2862 - accuracy: 0.9098 - val_loss: 0.2392 - val_accuracy: 0.9266 - 17s/epoch - 39ms/step\n",
            "Epoch 115/1000\n",
            "\n",
            "Epoch 115: val_accuracy did not improve from 0.94660\n",
            "429/429 - 19s - loss: 0.2870 - accuracy: 0.9091 - val_loss: 0.2247 - val_accuracy: 0.9298 - 19s/epoch - 44ms/step\n",
            "Epoch 116/1000\n",
            "\n",
            "Epoch 116: val_accuracy did not improve from 0.94660\n",
            "429/429 - 22s - loss: 0.2839 - accuracy: 0.9120 - val_loss: 0.2071 - val_accuracy: 0.9380 - 22s/epoch - 51ms/step\n",
            "Epoch 117/1000\n",
            "\n",
            "Epoch 117: val_accuracy did not improve from 0.94660\n",
            "429/429 - 18s - loss: 0.2861 - accuracy: 0.9115 - val_loss: 0.2233 - val_accuracy: 0.9388 - 18s/epoch - 41ms/step\n",
            "Epoch 118/1000\n",
            "\n",
            "Epoch 118: val_accuracy did not improve from 0.94660\n",
            "429/429 - 17s - loss: 0.2816 - accuracy: 0.9108 - val_loss: 0.2082 - val_accuracy: 0.9388 - 17s/epoch - 40ms/step\n",
            "Epoch 119/1000\n",
            "\n",
            "Epoch 119: val_accuracy did not improve from 0.94660\n",
            "429/429 - 17s - loss: 0.2850 - accuracy: 0.9093 - val_loss: 0.2103 - val_accuracy: 0.9412 - 17s/epoch - 40ms/step\n",
            "Epoch 120/1000\n",
            "\n",
            "Epoch 120: val_accuracy did not improve from 0.94660\n",
            "429/429 - 16s - loss: 0.2815 - accuracy: 0.9123 - val_loss: 0.2149 - val_accuracy: 0.9336 - 16s/epoch - 38ms/step\n",
            "Epoch 121/1000\n",
            "\n",
            "Epoch 121: val_accuracy did not improve from 0.94660\n",
            "429/429 - 17s - loss: 0.2824 - accuracy: 0.9110 - val_loss: 0.2259 - val_accuracy: 0.9262 - 17s/epoch - 39ms/step\n",
            "Epoch 122/1000\n",
            "\n",
            "Epoch 122: val_accuracy did not improve from 0.94660\n",
            "429/429 - 16s - loss: 0.2828 - accuracy: 0.9117 - val_loss: 0.2109 - val_accuracy: 0.9390 - 16s/epoch - 38ms/step\n",
            "Epoch 123/1000\n",
            "\n",
            "Epoch 123: val_accuracy did not improve from 0.94660\n",
            "429/429 - 16s - loss: 0.2813 - accuracy: 0.9120 - val_loss: 0.2207 - val_accuracy: 0.9338 - 16s/epoch - 38ms/step\n",
            "Epoch 124/1000\n",
            "\n",
            "Epoch 124: val_accuracy did not improve from 0.94660\n",
            "429/429 - 16s - loss: 0.2820 - accuracy: 0.9109 - val_loss: 0.2275 - val_accuracy: 0.9346 - 16s/epoch - 37ms/step\n",
            "Epoch 125/1000\n",
            "\n",
            "Epoch 125: val_accuracy did not improve from 0.94660\n",
            "429/429 - 17s - loss: 0.2836 - accuracy: 0.9105 - val_loss: 0.2165 - val_accuracy: 0.9362 - 17s/epoch - 39ms/step\n",
            "Epoch 126/1000\n",
            "\n",
            "Epoch 126: val_accuracy did not improve from 0.94660\n",
            "429/429 - 16s - loss: 0.2778 - accuracy: 0.9123 - val_loss: 0.2011 - val_accuracy: 0.9406 - 16s/epoch - 38ms/step\n",
            "Epoch 127/1000\n",
            "\n",
            "Epoch 127: val_accuracy did not improve from 0.94660\n",
            "429/429 - 17s - loss: 0.2813 - accuracy: 0.9123 - val_loss: 0.2058 - val_accuracy: 0.9386 - 17s/epoch - 39ms/step\n",
            "Epoch 128/1000\n",
            "\n",
            "Epoch 128: val_accuracy did not improve from 0.94660\n",
            "429/429 - 16s - loss: 0.2849 - accuracy: 0.9108 - val_loss: 0.2013 - val_accuracy: 0.9404 - 16s/epoch - 38ms/step\n",
            "Epoch 129/1000\n",
            "\n",
            "Epoch 129: val_accuracy did not improve from 0.94660\n",
            "429/429 - 17s - loss: 0.2799 - accuracy: 0.9127 - val_loss: 0.2287 - val_accuracy: 0.9308 - 17s/epoch - 39ms/step\n",
            "Epoch 130/1000\n",
            "\n",
            "Epoch 130: val_accuracy did not improve from 0.94660\n",
            "429/429 - 17s - loss: 0.2799 - accuracy: 0.9127 - val_loss: 0.2154 - val_accuracy: 0.9360 - 17s/epoch - 39ms/step\n",
            "Epoch 131/1000\n",
            "\n",
            "Epoch 131: val_accuracy did not improve from 0.94660\n",
            "429/429 - 17s - loss: 0.2772 - accuracy: 0.9136 - val_loss: 0.2088 - val_accuracy: 0.9378 - 17s/epoch - 39ms/step\n",
            "Epoch 132/1000\n",
            "\n",
            "Epoch 132: val_accuracy did not improve from 0.94660\n",
            "429/429 - 16s - loss: 0.2823 - accuracy: 0.9115 - val_loss: 0.2175 - val_accuracy: 0.9356 - 16s/epoch - 38ms/step\n",
            "Epoch 133/1000\n",
            "\n",
            "Epoch 133: val_accuracy did not improve from 0.94660\n",
            "429/429 - 16s - loss: 0.2812 - accuracy: 0.9129 - val_loss: 0.2004 - val_accuracy: 0.9412 - 16s/epoch - 38ms/step\n",
            "Epoch 134/1000\n",
            "\n",
            "Epoch 134: val_accuracy did not improve from 0.94660\n",
            "429/429 - 16s - loss: 0.2790 - accuracy: 0.9128 - val_loss: 0.1982 - val_accuracy: 0.9428 - 16s/epoch - 38ms/step\n",
            "Epoch 135/1000\n",
            "\n",
            "Epoch 135: val_accuracy did not improve from 0.94660\n",
            "429/429 - 16s - loss: 0.2837 - accuracy: 0.9120 - val_loss: 0.2199 - val_accuracy: 0.9366 - 16s/epoch - 37ms/step\n",
            "Epoch 136/1000\n",
            "\n",
            "Epoch 136: val_accuracy did not improve from 0.94660\n",
            "429/429 - 16s - loss: 0.2816 - accuracy: 0.9131 - val_loss: 0.2339 - val_accuracy: 0.9216 - 16s/epoch - 38ms/step\n",
            "Epoch 137/1000\n",
            "\n",
            "Epoch 137: val_accuracy did not improve from 0.94660\n",
            "429/429 - 16s - loss: 0.2808 - accuracy: 0.9117 - val_loss: 0.2300 - val_accuracy: 0.9330 - 16s/epoch - 37ms/step\n",
            "Epoch 138/1000\n",
            "\n",
            "Epoch 138: val_accuracy did not improve from 0.94660\n",
            "429/429 - 16s - loss: 0.2780 - accuracy: 0.9129 - val_loss: 0.2039 - val_accuracy: 0.9422 - 16s/epoch - 37ms/step\n",
            "Epoch 139/1000\n",
            "\n",
            "Epoch 139: val_accuracy did not improve from 0.94660\n",
            "429/429 - 16s - loss: 0.2797 - accuracy: 0.9117 - val_loss: 0.1909 - val_accuracy: 0.9458 - 16s/epoch - 38ms/step\n",
            "Epoch 140/1000\n",
            "\n",
            "Epoch 140: val_accuracy did not improve from 0.94660\n",
            "429/429 - 16s - loss: 0.2804 - accuracy: 0.9127 - val_loss: 0.1965 - val_accuracy: 0.9452 - 16s/epoch - 38ms/step\n",
            "Epoch 141/1000\n",
            "\n",
            "Epoch 141: val_accuracy did not improve from 0.94660\n",
            "429/429 - 16s - loss: 0.2801 - accuracy: 0.9131 - val_loss: 0.2348 - val_accuracy: 0.9282 - 16s/epoch - 38ms/step\n",
            "Epoch 142/1000\n",
            "\n",
            "Epoch 142: val_accuracy did not improve from 0.94660\n",
            "429/429 - 16s - loss: 0.2767 - accuracy: 0.9133 - val_loss: 0.2053 - val_accuracy: 0.9360 - 16s/epoch - 38ms/step\n",
            "Epoch 143/1000\n",
            "\n",
            "Epoch 143: val_accuracy did not improve from 0.94660\n",
            "429/429 - 17s - loss: 0.2795 - accuracy: 0.9131 - val_loss: 0.2263 - val_accuracy: 0.9318 - 17s/epoch - 39ms/step\n",
            "Epoch 144/1000\n",
            "\n",
            "Epoch 144: val_accuracy did not improve from 0.94660\n",
            "429/429 - 17s - loss: 0.2816 - accuracy: 0.9133 - val_loss: 0.2187 - val_accuracy: 0.9344 - 17s/epoch - 39ms/step\n",
            "Epoch 145/1000\n",
            "\n",
            "Epoch 145: val_accuracy did not improve from 0.94660\n",
            "429/429 - 17s - loss: 0.2807 - accuracy: 0.9126 - val_loss: 0.2282 - val_accuracy: 0.9306 - 17s/epoch - 40ms/step\n",
            "Epoch 146/1000\n",
            "\n",
            "Epoch 146: val_accuracy did not improve from 0.94660\n",
            "429/429 - 17s - loss: 0.2834 - accuracy: 0.9112 - val_loss: 0.2096 - val_accuracy: 0.9384 - 17s/epoch - 39ms/step\n",
            "Epoch 147/1000\n",
            "\n",
            "Epoch 147: val_accuracy did not improve from 0.94660\n",
            "429/429 - 16s - loss: 0.2773 - accuracy: 0.9138 - val_loss: 0.2078 - val_accuracy: 0.9370 - 16s/epoch - 37ms/step\n",
            "Epoch 148/1000\n",
            "\n",
            "Epoch 148: val_accuracy did not improve from 0.94660\n",
            "429/429 - 16s - loss: 0.2834 - accuracy: 0.9096 - val_loss: 0.2068 - val_accuracy: 0.9398 - 16s/epoch - 37ms/step\n",
            "Epoch 149/1000\n",
            "\n",
            "Epoch 149: val_accuracy did not improve from 0.94660\n",
            "429/429 - 16s - loss: 0.2790 - accuracy: 0.9132 - val_loss: 0.2392 - val_accuracy: 0.9232 - 16s/epoch - 37ms/step\n",
            "Epoch 150/1000\n",
            "\n",
            "Epoch 150: val_accuracy did not improve from 0.94660\n",
            "429/429 - 16s - loss: 0.2779 - accuracy: 0.9132 - val_loss: 0.2061 - val_accuracy: 0.9344 - 16s/epoch - 37ms/step\n",
            "Epoch 151/1000\n",
            "\n",
            "Epoch 151: val_accuracy did not improve from 0.94660\n",
            "429/429 - 16s - loss: 0.2782 - accuracy: 0.9120 - val_loss: 0.1957 - val_accuracy: 0.9408 - 16s/epoch - 38ms/step\n",
            "Epoch 152/1000\n",
            "\n",
            "Epoch 152: val_accuracy did not improve from 0.94660\n",
            "429/429 - 16s - loss: 0.2795 - accuracy: 0.9125 - val_loss: 0.2277 - val_accuracy: 0.9278 - 16s/epoch - 37ms/step\n",
            "Epoch 153/1000\n",
            "\n",
            "Epoch 153: val_accuracy improved from 0.94660 to 0.94700, saving model to best_model,keras\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "429/429 - 20s - loss: 0.2804 - accuracy: 0.9127 - val_loss: 0.1921 - val_accuracy: 0.9470 - 20s/epoch - 47ms/step\n",
            "Epoch 154/1000\n",
            "\n",
            "Epoch 154: val_accuracy did not improve from 0.94700\n",
            "429/429 - 17s - loss: 0.2765 - accuracy: 0.9126 - val_loss: 0.2120 - val_accuracy: 0.9382 - 17s/epoch - 39ms/step\n",
            "Epoch 155/1000\n",
            "\n",
            "Epoch 155: val_accuracy did not improve from 0.94700\n",
            "429/429 - 16s - loss: 0.2763 - accuracy: 0.9147 - val_loss: 0.2160 - val_accuracy: 0.9370 - 16s/epoch - 38ms/step\n",
            "Epoch 156/1000\n",
            "\n",
            "Epoch 156: val_accuracy did not improve from 0.94700\n",
            "429/429 - 17s - loss: 0.2780 - accuracy: 0.9135 - val_loss: 0.1913 - val_accuracy: 0.9460 - 17s/epoch - 38ms/step\n",
            "Epoch 157/1000\n",
            "\n",
            "Epoch 157: val_accuracy did not improve from 0.94700\n",
            "429/429 - 16s - loss: 0.2780 - accuracy: 0.9143 - val_loss: 0.2273 - val_accuracy: 0.9292 - 16s/epoch - 38ms/step\n",
            "Epoch 158/1000\n",
            "\n",
            "Epoch 158: val_accuracy did not improve from 0.94700\n",
            "429/429 - 17s - loss: 0.2776 - accuracy: 0.9128 - val_loss: 0.2161 - val_accuracy: 0.9392 - 17s/epoch - 40ms/step\n",
            "Epoch 159/1000\n",
            "\n",
            "Epoch 159: val_accuracy did not improve from 0.94700\n",
            "429/429 - 17s - loss: 0.2795 - accuracy: 0.9130 - val_loss: 0.2041 - val_accuracy: 0.9398 - 17s/epoch - 39ms/step\n",
            "Epoch 160/1000\n",
            "\n",
            "Epoch 160: val_accuracy did not improve from 0.94700\n",
            "429/429 - 17s - loss: 0.2790 - accuracy: 0.9137 - val_loss: 0.2270 - val_accuracy: 0.9360 - 17s/epoch - 39ms/step\n",
            "Epoch 161/1000\n",
            "\n",
            "Epoch 161: val_accuracy did not improve from 0.94700\n",
            "429/429 - 16s - loss: 0.2784 - accuracy: 0.9129 - val_loss: 0.2131 - val_accuracy: 0.9350 - 16s/epoch - 37ms/step\n",
            "Epoch 162/1000\n",
            "\n",
            "Epoch 162: val_accuracy did not improve from 0.94700\n",
            "429/429 - 16s - loss: 0.2788 - accuracy: 0.9133 - val_loss: 0.2394 - val_accuracy: 0.9218 - 16s/epoch - 37ms/step\n",
            "Epoch 163/1000\n",
            "\n",
            "Epoch 163: val_accuracy did not improve from 0.94700\n",
            "429/429 - 16s - loss: 0.2819 - accuracy: 0.9123 - val_loss: 0.2094 - val_accuracy: 0.9374 - 16s/epoch - 37ms/step\n",
            "Epoch 164/1000\n",
            "\n",
            "Epoch 164: val_accuracy did not improve from 0.94700\n",
            "429/429 - 16s - loss: 0.2778 - accuracy: 0.9145 - val_loss: 0.2051 - val_accuracy: 0.9382 - 16s/epoch - 37ms/step\n",
            "Epoch 165/1000\n",
            "\n",
            "Epoch 165: val_accuracy did not improve from 0.94700\n",
            "429/429 - 16s - loss: 0.2785 - accuracy: 0.9123 - val_loss: 0.1989 - val_accuracy: 0.9436 - 16s/epoch - 37ms/step\n",
            "Epoch 166/1000\n",
            "\n",
            "Epoch 166: val_accuracy did not improve from 0.94700\n",
            "429/429 - 16s - loss: 0.2786 - accuracy: 0.9134 - val_loss: 0.2406 - val_accuracy: 0.9318 - 16s/epoch - 37ms/step\n",
            "Epoch 167/1000\n",
            "\n",
            "Epoch 167: val_accuracy did not improve from 0.94700\n",
            "429/429 - 18s - loss: 0.2780 - accuracy: 0.9135 - val_loss: 0.1972 - val_accuracy: 0.9406 - 18s/epoch - 42ms/step\n",
            "Epoch 168/1000\n",
            "\n",
            "Epoch 168: val_accuracy did not improve from 0.94700\n",
            "429/429 - 16s - loss: 0.2737 - accuracy: 0.9146 - val_loss: 0.2236 - val_accuracy: 0.9334 - 16s/epoch - 37ms/step\n",
            "Epoch 169/1000\n",
            "\n",
            "Epoch 169: val_accuracy did not improve from 0.94700\n",
            "429/429 - 17s - loss: 0.2741 - accuracy: 0.9147 - val_loss: 0.1957 - val_accuracy: 0.9454 - 17s/epoch - 38ms/step\n",
            "Epoch 170/1000\n",
            "\n",
            "Epoch 170: val_accuracy did not improve from 0.94700\n",
            "429/429 - 16s - loss: 0.2758 - accuracy: 0.9152 - val_loss: 0.2155 - val_accuracy: 0.9402 - 16s/epoch - 38ms/step\n",
            "Epoch 171/1000\n",
            "\n",
            "Epoch 171: val_accuracy did not improve from 0.94700\n",
            "429/429 - 18s - loss: 0.2804 - accuracy: 0.9135 - val_loss: 0.2295 - val_accuracy: 0.9312 - 18s/epoch - 42ms/step\n",
            "Epoch 172/1000\n",
            "\n",
            "Epoch 172: val_accuracy did not improve from 0.94700\n",
            "429/429 - 17s - loss: 0.2732 - accuracy: 0.9144 - val_loss: 0.2187 - val_accuracy: 0.9304 - 17s/epoch - 40ms/step\n",
            "Epoch 173/1000\n",
            "\n",
            "Epoch 173: val_accuracy did not improve from 0.94700\n",
            "429/429 - 17s - loss: 0.2746 - accuracy: 0.9137 - val_loss: 0.2135 - val_accuracy: 0.9308 - 17s/epoch - 40ms/step\n",
            "Epoch 174/1000\n",
            "\n",
            "Epoch 174: val_accuracy did not improve from 0.94700\n",
            "429/429 - 17s - loss: 0.2794 - accuracy: 0.9130 - val_loss: 0.1971 - val_accuracy: 0.9416 - 17s/epoch - 39ms/step\n",
            "Epoch 175/1000\n",
            "\n",
            "Epoch 175: val_accuracy did not improve from 0.94700\n",
            "429/429 - 16s - loss: 0.2762 - accuracy: 0.9147 - val_loss: 0.2220 - val_accuracy: 0.9380 - 16s/epoch - 38ms/step\n",
            "Epoch 176/1000\n",
            "\n",
            "Epoch 176: val_accuracy did not improve from 0.94700\n",
            "429/429 - 16s - loss: 0.2758 - accuracy: 0.9152 - val_loss: 0.2096 - val_accuracy: 0.9366 - 16s/epoch - 37ms/step\n",
            "Epoch 177/1000\n",
            "\n",
            "Epoch 177: val_accuracy improved from 0.94700 to 0.94720, saving model to best_model,keras\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "429/429 - 18s - loss: 0.2741 - accuracy: 0.9141 - val_loss: 0.1833 - val_accuracy: 0.9472 - 18s/epoch - 41ms/step\n",
            "Epoch 178/1000\n",
            "\n",
            "Epoch 178: val_accuracy did not improve from 0.94720\n",
            "429/429 - 16s - loss: 0.2774 - accuracy: 0.9135 - val_loss: 0.2004 - val_accuracy: 0.9374 - 16s/epoch - 38ms/step\n",
            "Epoch 179/1000\n",
            "\n",
            "Epoch 179: val_accuracy did not improve from 0.94720\n",
            "429/429 - 16s - loss: 0.2699 - accuracy: 0.9153 - val_loss: 0.2418 - val_accuracy: 0.9296 - 16s/epoch - 37ms/step\n",
            "Epoch 180/1000\n",
            "\n",
            "Epoch 180: val_accuracy did not improve from 0.94720\n",
            "429/429 - 16s - loss: 0.2796 - accuracy: 0.9119 - val_loss: 0.2055 - val_accuracy: 0.9348 - 16s/epoch - 37ms/step\n",
            "Epoch 181/1000\n",
            "\n",
            "Epoch 181: val_accuracy did not improve from 0.94720\n",
            "429/429 - 16s - loss: 0.2766 - accuracy: 0.9148 - val_loss: 0.2044 - val_accuracy: 0.9416 - 16s/epoch - 38ms/step\n",
            "Epoch 182/1000\n",
            "\n",
            "Epoch 182: val_accuracy did not improve from 0.94720\n",
            "429/429 - 16s - loss: 0.2729 - accuracy: 0.9157 - val_loss: 0.2005 - val_accuracy: 0.9396 - 16s/epoch - 38ms/step\n",
            "Epoch 183/1000\n",
            "\n",
            "Epoch 183: val_accuracy did not improve from 0.94720\n",
            "429/429 - 17s - loss: 0.2722 - accuracy: 0.9154 - val_loss: 0.2356 - val_accuracy: 0.9320 - 17s/epoch - 39ms/step\n",
            "Epoch 184/1000\n",
            "\n",
            "Epoch 184: val_accuracy did not improve from 0.94720\n",
            "429/429 - 16s - loss: 0.2737 - accuracy: 0.9144 - val_loss: 0.2151 - val_accuracy: 0.9366 - 16s/epoch - 37ms/step\n",
            "Epoch 185/1000\n",
            "\n",
            "Epoch 185: val_accuracy did not improve from 0.94720\n",
            "429/429 - 17s - loss: 0.2769 - accuracy: 0.9134 - val_loss: 0.2246 - val_accuracy: 0.9322 - 17s/epoch - 39ms/step\n",
            "Epoch 186/1000\n",
            "\n",
            "Epoch 186: val_accuracy did not improve from 0.94720\n",
            "429/429 - 17s - loss: 0.2779 - accuracy: 0.9128 - val_loss: 0.2146 - val_accuracy: 0.9392 - 17s/epoch - 40ms/step\n",
            "Epoch 187/1000\n",
            "\n",
            "Epoch 187: val_accuracy did not improve from 0.94720\n",
            "429/429 - 21s - loss: 0.2761 - accuracy: 0.9143 - val_loss: 0.2187 - val_accuracy: 0.9316 - 21s/epoch - 50ms/step\n",
            "Epoch 188/1000\n",
            "\n",
            "Epoch 188: val_accuracy did not improve from 0.94720\n",
            "429/429 - 22s - loss: 0.2775 - accuracy: 0.9134 - val_loss: 0.2099 - val_accuracy: 0.9340 - 22s/epoch - 51ms/step\n",
            "Epoch 189/1000\n",
            "\n",
            "Epoch 189: val_accuracy did not improve from 0.94720\n",
            "429/429 - 16s - loss: 0.2763 - accuracy: 0.9131 - val_loss: 0.2483 - val_accuracy: 0.9266 - 16s/epoch - 38ms/step\n",
            "Epoch 190/1000\n",
            "\n",
            "Epoch 190: val_accuracy did not improve from 0.94720\n",
            "429/429 - 16s - loss: 0.2784 - accuracy: 0.9126 - val_loss: 0.2200 - val_accuracy: 0.9342 - 16s/epoch - 37ms/step\n",
            "Epoch 191/1000\n",
            "\n",
            "Epoch 191: val_accuracy did not improve from 0.94720\n",
            "429/429 - 16s - loss: 0.2759 - accuracy: 0.9139 - val_loss: 0.2212 - val_accuracy: 0.9358 - 16s/epoch - 37ms/step\n",
            "Epoch 192/1000\n",
            "\n",
            "Epoch 192: val_accuracy did not improve from 0.94720\n",
            "429/429 - 16s - loss: 0.2777 - accuracy: 0.9150 - val_loss: 0.2113 - val_accuracy: 0.9404 - 16s/epoch - 38ms/step\n",
            "Epoch 193/1000\n",
            "\n",
            "Epoch 193: val_accuracy did not improve from 0.94720\n",
            "429/429 - 16s - loss: 0.2738 - accuracy: 0.9151 - val_loss: 0.2102 - val_accuracy: 0.9372 - 16s/epoch - 38ms/step\n",
            "Epoch 194/1000\n",
            "\n",
            "Epoch 194: val_accuracy did not improve from 0.94720\n",
            "429/429 - 16s - loss: 0.2744 - accuracy: 0.9149 - val_loss: 0.1978 - val_accuracy: 0.9404 - 16s/epoch - 38ms/step\n",
            "Epoch 195/1000\n",
            "\n",
            "Epoch 195: val_accuracy did not improve from 0.94720\n",
            "429/429 - 16s - loss: 0.2786 - accuracy: 0.9132 - val_loss: 0.2230 - val_accuracy: 0.9300 - 16s/epoch - 37ms/step\n",
            "Epoch 196/1000\n",
            "\n",
            "Epoch 196: val_accuracy did not improve from 0.94720\n",
            "429/429 - 16s - loss: 0.2765 - accuracy: 0.9152 - val_loss: 0.2151 - val_accuracy: 0.9348 - 16s/epoch - 38ms/step\n",
            "Epoch 197/1000\n",
            "\n",
            "Epoch 197: val_accuracy did not improve from 0.94720\n",
            "429/429 - 16s - loss: 0.2737 - accuracy: 0.9145 - val_loss: 0.2238 - val_accuracy: 0.9324 - 16s/epoch - 38ms/step\n",
            "Epoch 198/1000\n",
            "\n",
            "Epoch 198: val_accuracy did not improve from 0.94720\n",
            "429/429 - 16s - loss: 0.2717 - accuracy: 0.9153 - val_loss: 0.1925 - val_accuracy: 0.9460 - 16s/epoch - 38ms/step\n",
            "Epoch 199/1000\n",
            "\n",
            "Epoch 199: val_accuracy did not improve from 0.94720\n",
            "429/429 - 17s - loss: 0.2756 - accuracy: 0.9148 - val_loss: 0.2194 - val_accuracy: 0.9320 - 17s/epoch - 39ms/step\n",
            "Epoch 200/1000\n",
            "\n",
            "Epoch 200: val_accuracy did not improve from 0.94720\n",
            "429/429 - 17s - loss: 0.2774 - accuracy: 0.9141 - val_loss: 0.2032 - val_accuracy: 0.9384 - 17s/epoch - 39ms/step\n",
            "Epoch 201/1000\n",
            "\n",
            "Epoch 201: val_accuracy did not improve from 0.94720\n",
            "429/429 - 19s - loss: 0.2742 - accuracy: 0.9147 - val_loss: 0.1998 - val_accuracy: 0.9432 - 19s/epoch - 43ms/step\n",
            "Epoch 202/1000\n",
            "\n",
            "Epoch 202: val_accuracy did not improve from 0.94720\n",
            "429/429 - 16s - loss: 0.2707 - accuracy: 0.9159 - val_loss: 0.1969 - val_accuracy: 0.9380 - 16s/epoch - 38ms/step\n",
            "Epoch 203/1000\n",
            "\n",
            "Epoch 203: val_accuracy did not improve from 0.94720\n",
            "429/429 - 16s - loss: 0.2718 - accuracy: 0.9155 - val_loss: 0.1845 - val_accuracy: 0.9460 - 16s/epoch - 37ms/step\n",
            "Epoch 204/1000\n",
            "\n",
            "Epoch 204: val_accuracy did not improve from 0.94720\n",
            "429/429 - 16s - loss: 0.2748 - accuracy: 0.9144 - val_loss: 0.2092 - val_accuracy: 0.9398 - 16s/epoch - 37ms/step\n",
            "Epoch 205/1000\n",
            "\n",
            "Epoch 205: val_accuracy did not improve from 0.94720\n",
            "429/429 - 16s - loss: 0.2692 - accuracy: 0.9150 - val_loss: 0.2092 - val_accuracy: 0.9436 - 16s/epoch - 37ms/step\n",
            "Epoch 206/1000\n",
            "\n",
            "Epoch 206: val_accuracy did not improve from 0.94720\n",
            "429/429 - 16s - loss: 0.2751 - accuracy: 0.9142 - val_loss: 0.1939 - val_accuracy: 0.9472 - 16s/epoch - 37ms/step\n",
            "Epoch 207/1000\n",
            "\n",
            "Epoch 207: val_accuracy did not improve from 0.94720\n",
            "429/429 - 16s - loss: 0.2748 - accuracy: 0.9149 - val_loss: 0.2196 - val_accuracy: 0.9366 - 16s/epoch - 37ms/step\n",
            "Epoch 208/1000\n",
            "\n",
            "Epoch 208: val_accuracy did not improve from 0.94720\n",
            "429/429 - 16s - loss: 0.2741 - accuracy: 0.9152 - val_loss: 0.2046 - val_accuracy: 0.9438 - 16s/epoch - 38ms/step\n",
            "Epoch 209/1000\n",
            "\n",
            "Epoch 209: val_accuracy did not improve from 0.94720\n",
            "429/429 - 16s - loss: 0.2762 - accuracy: 0.9147 - val_loss: 0.2198 - val_accuracy: 0.9366 - 16s/epoch - 37ms/step\n",
            "Epoch 210/1000\n",
            "\n",
            "Epoch 210: val_accuracy did not improve from 0.94720\n",
            "429/429 - 16s - loss: 0.2761 - accuracy: 0.9138 - val_loss: 0.2081 - val_accuracy: 0.9432 - 16s/epoch - 38ms/step\n",
            "Epoch 211/1000\n",
            "\n",
            "Epoch 211: val_accuracy did not improve from 0.94720\n",
            "429/429 - 16s - loss: 0.2759 - accuracy: 0.9153 - val_loss: 0.2078 - val_accuracy: 0.9424 - 16s/epoch - 38ms/step\n",
            "Epoch 212/1000\n",
            "\n",
            "Epoch 212: val_accuracy did not improve from 0.94720\n",
            "429/429 - 16s - loss: 0.2712 - accuracy: 0.9150 - val_loss: 0.2078 - val_accuracy: 0.9380 - 16s/epoch - 38ms/step\n",
            "Epoch 213/1000\n",
            "\n",
            "Epoch 213: val_accuracy did not improve from 0.94720\n",
            "429/429 - 16s - loss: 0.2741 - accuracy: 0.9129 - val_loss: 0.2130 - val_accuracy: 0.9366 - 16s/epoch - 38ms/step\n",
            "Epoch 214/1000\n",
            "\n",
            "Epoch 214: val_accuracy did not improve from 0.94720\n",
            "429/429 - 17s - loss: 0.2743 - accuracy: 0.9145 - val_loss: 0.2158 - val_accuracy: 0.9358 - 17s/epoch - 39ms/step\n",
            "Epoch 215/1000\n",
            "\n",
            "Epoch 215: val_accuracy did not improve from 0.94720\n",
            "429/429 - 17s - loss: 0.2740 - accuracy: 0.9141 - val_loss: 0.1884 - val_accuracy: 0.9424 - 17s/epoch - 40ms/step\n",
            "Epoch 216/1000\n",
            "\n",
            "Epoch 216: val_accuracy did not improve from 0.94720\n",
            "429/429 - 16s - loss: 0.2723 - accuracy: 0.9145 - val_loss: 0.1885 - val_accuracy: 0.9440 - 16s/epoch - 38ms/step\n",
            "Epoch 217/1000\n",
            "\n",
            "Epoch 217: val_accuracy did not improve from 0.94720\n",
            "429/429 - 16s - loss: 0.2756 - accuracy: 0.9137 - val_loss: 0.2013 - val_accuracy: 0.9388 - 16s/epoch - 37ms/step\n",
            "Epoch 218/1000\n",
            "\n",
            "Epoch 218: val_accuracy did not improve from 0.94720\n",
            "429/429 - 16s - loss: 0.2765 - accuracy: 0.9145 - val_loss: 0.1914 - val_accuracy: 0.9448 - 16s/epoch - 37ms/step\n",
            "Epoch 219/1000\n",
            "\n",
            "Epoch 219: val_accuracy did not improve from 0.94720\n",
            "429/429 - 16s - loss: 0.2749 - accuracy: 0.9137 - val_loss: 0.1791 - val_accuracy: 0.9466 - 16s/epoch - 37ms/step\n",
            "Epoch 220/1000\n",
            "\n",
            "Epoch 220: val_accuracy did not improve from 0.94720\n",
            "429/429 - 16s - loss: 0.2756 - accuracy: 0.9154 - val_loss: 0.2526 - val_accuracy: 0.9280 - 16s/epoch - 37ms/step\n",
            "Epoch 221/1000\n",
            "\n",
            "Epoch 221: val_accuracy did not improve from 0.94720\n",
            "429/429 - 16s - loss: 0.2733 - accuracy: 0.9144 - val_loss: 0.2191 - val_accuracy: 0.9310 - 16s/epoch - 37ms/step\n",
            "Epoch 222/1000\n",
            "\n",
            "Epoch 222: val_accuracy did not improve from 0.94720\n",
            "429/429 - 16s - loss: 0.2738 - accuracy: 0.9153 - val_loss: 0.2154 - val_accuracy: 0.9366 - 16s/epoch - 37ms/step\n",
            "Epoch 223/1000\n",
            "\n",
            "Epoch 223: val_accuracy did not improve from 0.94720\n",
            "429/429 - 16s - loss: 0.2698 - accuracy: 0.9157 - val_loss: 0.2121 - val_accuracy: 0.9348 - 16s/epoch - 37ms/step\n",
            "Epoch 224/1000\n",
            "\n",
            "Epoch 224: val_accuracy did not improve from 0.94720\n",
            "429/429 - 16s - loss: 0.2706 - accuracy: 0.9164 - val_loss: 0.2041 - val_accuracy: 0.9402 - 16s/epoch - 38ms/step\n",
            "Epoch 225/1000\n",
            "\n",
            "Epoch 225: val_accuracy did not improve from 0.94720\n",
            "429/429 - 16s - loss: 0.2723 - accuracy: 0.9164 - val_loss: 0.2429 - val_accuracy: 0.9274 - 16s/epoch - 38ms/step\n",
            "Epoch 226/1000\n",
            "\n",
            "Epoch 226: val_accuracy did not improve from 0.94720\n",
            "429/429 - 16s - loss: 0.2713 - accuracy: 0.9161 - val_loss: 0.2010 - val_accuracy: 0.9378 - 16s/epoch - 38ms/step\n",
            "Epoch 227/1000\n",
            "\n",
            "Epoch 227: val_accuracy improved from 0.94720 to 0.94980, saving model to best_model,keras\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "429/429 - 19s - loss: 0.2703 - accuracy: 0.9158 - val_loss: 0.1889 - val_accuracy: 0.9498 - 19s/epoch - 44ms/step\n",
            "Epoch 228/1000\n",
            "\n",
            "Epoch 228: val_accuracy did not improve from 0.94980\n",
            "429/429 - 22s - loss: 0.2723 - accuracy: 0.9157 - val_loss: 0.1954 - val_accuracy: 0.9432 - 22s/epoch - 51ms/step\n",
            "Epoch 229/1000\n",
            "\n",
            "Epoch 229: val_accuracy did not improve from 0.94980\n",
            "429/429 - 21s - loss: 0.2774 - accuracy: 0.9147 - val_loss: 0.2048 - val_accuracy: 0.9362 - 21s/epoch - 48ms/step\n",
            "Epoch 230/1000\n",
            "\n",
            "Epoch 230: val_accuracy did not improve from 0.94980\n",
            "429/429 - 18s - loss: 0.2727 - accuracy: 0.9161 - val_loss: 0.2100 - val_accuracy: 0.9420 - 18s/epoch - 43ms/step\n",
            "Epoch 231/1000\n",
            "\n",
            "Epoch 231: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2730 - accuracy: 0.9154 - val_loss: 0.2026 - val_accuracy: 0.9402 - 16s/epoch - 38ms/step\n",
            "Epoch 232/1000\n",
            "\n",
            "Epoch 232: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2734 - accuracy: 0.9159 - val_loss: 0.2606 - val_accuracy: 0.9196 - 16s/epoch - 37ms/step\n",
            "Epoch 233/1000\n",
            "\n",
            "Epoch 233: val_accuracy did not improve from 0.94980\n",
            "429/429 - 17s - loss: 0.2726 - accuracy: 0.9153 - val_loss: 0.2312 - val_accuracy: 0.9272 - 17s/epoch - 39ms/step\n",
            "Epoch 234/1000\n",
            "\n",
            "Epoch 234: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2736 - accuracy: 0.9157 - val_loss: 0.2261 - val_accuracy: 0.9296 - 16s/epoch - 38ms/step\n",
            "Epoch 235/1000\n",
            "\n",
            "Epoch 235: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2707 - accuracy: 0.9153 - val_loss: 0.2072 - val_accuracy: 0.9356 - 16s/epoch - 37ms/step\n",
            "Epoch 236/1000\n",
            "\n",
            "Epoch 236: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2705 - accuracy: 0.9153 - val_loss: 0.2219 - val_accuracy: 0.9324 - 16s/epoch - 38ms/step\n",
            "Epoch 237/1000\n",
            "\n",
            "Epoch 237: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2736 - accuracy: 0.9152 - val_loss: 0.2036 - val_accuracy: 0.9402 - 16s/epoch - 38ms/step\n",
            "Epoch 238/1000\n",
            "\n",
            "Epoch 238: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2728 - accuracy: 0.9153 - val_loss: 0.2190 - val_accuracy: 0.9312 - 16s/epoch - 37ms/step\n",
            "Epoch 239/1000\n",
            "\n",
            "Epoch 239: val_accuracy did not improve from 0.94980\n",
            "429/429 - 17s - loss: 0.2664 - accuracy: 0.9155 - val_loss: 0.2254 - val_accuracy: 0.9312 - 17s/epoch - 39ms/step\n",
            "Epoch 240/1000\n",
            "\n",
            "Epoch 240: val_accuracy did not improve from 0.94980\n",
            "429/429 - 18s - loss: 0.2699 - accuracy: 0.9148 - val_loss: 0.2109 - val_accuracy: 0.9362 - 18s/epoch - 42ms/step\n",
            "Epoch 241/1000\n",
            "\n",
            "Epoch 241: val_accuracy did not improve from 0.94980\n",
            "429/429 - 19s - loss: 0.2712 - accuracy: 0.9168 - val_loss: 0.2075 - val_accuracy: 0.9374 - 19s/epoch - 45ms/step\n",
            "Epoch 242/1000\n",
            "\n",
            "Epoch 242: val_accuracy did not improve from 0.94980\n",
            "429/429 - 17s - loss: 0.2716 - accuracy: 0.9155 - val_loss: 0.2085 - val_accuracy: 0.9388 - 17s/epoch - 39ms/step\n",
            "Epoch 243/1000\n",
            "\n",
            "Epoch 243: val_accuracy did not improve from 0.94980\n",
            "429/429 - 17s - loss: 0.2744 - accuracy: 0.9136 - val_loss: 0.2085 - val_accuracy: 0.9340 - 17s/epoch - 40ms/step\n",
            "Epoch 244/1000\n",
            "\n",
            "Epoch 244: val_accuracy did not improve from 0.94980\n",
            "429/429 - 17s - loss: 0.2745 - accuracy: 0.9145 - val_loss: 0.1878 - val_accuracy: 0.9458 - 17s/epoch - 39ms/step\n",
            "Epoch 245/1000\n",
            "\n",
            "Epoch 245: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2724 - accuracy: 0.9146 - val_loss: 0.1913 - val_accuracy: 0.9464 - 16s/epoch - 37ms/step\n",
            "Epoch 246/1000\n",
            "\n",
            "Epoch 246: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2736 - accuracy: 0.9150 - val_loss: 0.1880 - val_accuracy: 0.9446 - 16s/epoch - 38ms/step\n",
            "Epoch 247/1000\n",
            "\n",
            "Epoch 247: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2683 - accuracy: 0.9169 - val_loss: 0.2079 - val_accuracy: 0.9372 - 16s/epoch - 37ms/step\n",
            "Epoch 248/1000\n",
            "\n",
            "Epoch 248: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2737 - accuracy: 0.9144 - val_loss: 0.2040 - val_accuracy: 0.9430 - 16s/epoch - 37ms/step\n",
            "Epoch 249/1000\n",
            "\n",
            "Epoch 249: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2700 - accuracy: 0.9155 - val_loss: 0.2145 - val_accuracy: 0.9372 - 16s/epoch - 37ms/step\n",
            "Epoch 250/1000\n",
            "\n",
            "Epoch 250: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2702 - accuracy: 0.9167 - val_loss: 0.1837 - val_accuracy: 0.9462 - 16s/epoch - 37ms/step\n",
            "Epoch 251/1000\n",
            "\n",
            "Epoch 251: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2712 - accuracy: 0.9139 - val_loss: 0.1919 - val_accuracy: 0.9454 - 16s/epoch - 37ms/step\n",
            "Epoch 252/1000\n",
            "\n",
            "Epoch 252: val_accuracy did not improve from 0.94980\n",
            "429/429 - 17s - loss: 0.2736 - accuracy: 0.9155 - val_loss: 0.1849 - val_accuracy: 0.9466 - 17s/epoch - 39ms/step\n",
            "Epoch 253/1000\n",
            "\n",
            "Epoch 253: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2705 - accuracy: 0.9163 - val_loss: 0.1919 - val_accuracy: 0.9436 - 16s/epoch - 38ms/step\n",
            "Epoch 254/1000\n",
            "\n",
            "Epoch 254: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2713 - accuracy: 0.9162 - val_loss: 0.1948 - val_accuracy: 0.9406 - 16s/epoch - 38ms/step\n",
            "Epoch 255/1000\n",
            "\n",
            "Epoch 255: val_accuracy did not improve from 0.94980\n",
            "429/429 - 17s - loss: 0.2754 - accuracy: 0.9148 - val_loss: 0.2113 - val_accuracy: 0.9362 - 17s/epoch - 39ms/step\n",
            "Epoch 256/1000\n",
            "\n",
            "Epoch 256: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2743 - accuracy: 0.9136 - val_loss: 0.2151 - val_accuracy: 0.9332 - 16s/epoch - 38ms/step\n",
            "Epoch 257/1000\n",
            "\n",
            "Epoch 257: val_accuracy did not improve from 0.94980\n",
            "429/429 - 17s - loss: 0.2681 - accuracy: 0.9180 - val_loss: 0.2277 - val_accuracy: 0.9316 - 17s/epoch - 39ms/step\n",
            "Epoch 258/1000\n",
            "\n",
            "Epoch 258: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2710 - accuracy: 0.9172 - val_loss: 0.2037 - val_accuracy: 0.9414 - 16s/epoch - 37ms/step\n",
            "Epoch 259/1000\n",
            "\n",
            "Epoch 259: val_accuracy did not improve from 0.94980\n",
            "429/429 - 17s - loss: 0.2692 - accuracy: 0.9176 - val_loss: 0.2129 - val_accuracy: 0.9370 - 17s/epoch - 39ms/step\n",
            "Epoch 260/1000\n",
            "\n",
            "Epoch 260: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2705 - accuracy: 0.9150 - val_loss: 0.2268 - val_accuracy: 0.9286 - 16s/epoch - 36ms/step\n",
            "Epoch 261/1000\n",
            "\n",
            "Epoch 261: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2681 - accuracy: 0.9162 - val_loss: 0.2326 - val_accuracy: 0.9326 - 16s/epoch - 37ms/step\n",
            "Epoch 262/1000\n",
            "\n",
            "Epoch 262: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2699 - accuracy: 0.9154 - val_loss: 0.2191 - val_accuracy: 0.9346 - 16s/epoch - 37ms/step\n",
            "Epoch 263/1000\n",
            "\n",
            "Epoch 263: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2691 - accuracy: 0.9161 - val_loss: 0.2079 - val_accuracy: 0.9384 - 16s/epoch - 37ms/step\n",
            "Epoch 264/1000\n",
            "\n",
            "Epoch 264: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2746 - accuracy: 0.9141 - val_loss: 0.2135 - val_accuracy: 0.9376 - 16s/epoch - 37ms/step\n",
            "Epoch 265/1000\n",
            "\n",
            "Epoch 265: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2727 - accuracy: 0.9157 - val_loss: 0.2227 - val_accuracy: 0.9338 - 16s/epoch - 37ms/step\n",
            "Epoch 266/1000\n",
            "\n",
            "Epoch 266: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2702 - accuracy: 0.9152 - val_loss: 0.1881 - val_accuracy: 0.9450 - 16s/epoch - 38ms/step\n",
            "Epoch 267/1000\n",
            "\n",
            "Epoch 267: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2723 - accuracy: 0.9165 - val_loss: 0.1969 - val_accuracy: 0.9398 - 16s/epoch - 38ms/step\n",
            "Epoch 268/1000\n",
            "\n",
            "Epoch 268: val_accuracy did not improve from 0.94980\n",
            "429/429 - 17s - loss: 0.2743 - accuracy: 0.9151 - val_loss: 0.2274 - val_accuracy: 0.9298 - 17s/epoch - 39ms/step\n",
            "Epoch 269/1000\n",
            "\n",
            "Epoch 269: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2714 - accuracy: 0.9170 - val_loss: 0.2067 - val_accuracy: 0.9366 - 16s/epoch - 38ms/step\n",
            "Epoch 270/1000\n",
            "\n",
            "Epoch 270: val_accuracy did not improve from 0.94980\n",
            "429/429 - 17s - loss: 0.2695 - accuracy: 0.9156 - val_loss: 0.2104 - val_accuracy: 0.9370 - 17s/epoch - 39ms/step\n",
            "Epoch 271/1000\n",
            "\n",
            "Epoch 271: val_accuracy did not improve from 0.94980\n",
            "429/429 - 17s - loss: 0.2704 - accuracy: 0.9166 - val_loss: 0.2586 - val_accuracy: 0.9240 - 17s/epoch - 39ms/step\n",
            "Epoch 272/1000\n",
            "\n",
            "Epoch 272: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2687 - accuracy: 0.9174 - val_loss: 0.2285 - val_accuracy: 0.9278 - 16s/epoch - 37ms/step\n",
            "Epoch 273/1000\n",
            "\n",
            "Epoch 273: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2692 - accuracy: 0.9172 - val_loss: 0.2071 - val_accuracy: 0.9344 - 16s/epoch - 37ms/step\n",
            "Epoch 274/1000\n",
            "\n",
            "Epoch 274: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2664 - accuracy: 0.9178 - val_loss: 0.2214 - val_accuracy: 0.9336 - 16s/epoch - 37ms/step\n",
            "Epoch 275/1000\n",
            "\n",
            "Epoch 275: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2728 - accuracy: 0.9151 - val_loss: 0.2272 - val_accuracy: 0.9340 - 16s/epoch - 37ms/step\n",
            "Epoch 276/1000\n",
            "\n",
            "Epoch 276: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2664 - accuracy: 0.9178 - val_loss: 0.2500 - val_accuracy: 0.9330 - 16s/epoch - 37ms/step\n",
            "Epoch 277/1000\n",
            "\n",
            "Epoch 277: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2690 - accuracy: 0.9158 - val_loss: 0.2339 - val_accuracy: 0.9276 - 16s/epoch - 37ms/step\n",
            "Epoch 278/1000\n",
            "\n",
            "Epoch 278: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2741 - accuracy: 0.9142 - val_loss: 0.1826 - val_accuracy: 0.9494 - 16s/epoch - 37ms/step\n",
            "Epoch 279/1000\n",
            "\n",
            "Epoch 279: val_accuracy did not improve from 0.94980\n",
            "429/429 - 31s - loss: 0.2681 - accuracy: 0.9160 - val_loss: 0.2252 - val_accuracy: 0.9352 - 31s/epoch - 71ms/step\n",
            "Epoch 280/1000\n",
            "\n",
            "Epoch 280: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2703 - accuracy: 0.9156 - val_loss: 0.1951 - val_accuracy: 0.9434 - 16s/epoch - 38ms/step\n",
            "Epoch 281/1000\n",
            "\n",
            "Epoch 281: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2678 - accuracy: 0.9175 - val_loss: 0.1945 - val_accuracy: 0.9486 - 16s/epoch - 38ms/step\n",
            "Epoch 282/1000\n",
            "\n",
            "Epoch 282: val_accuracy did not improve from 0.94980\n",
            "429/429 - 17s - loss: 0.2720 - accuracy: 0.9153 - val_loss: 0.2115 - val_accuracy: 0.9372 - 17s/epoch - 41ms/step\n",
            "Epoch 283/1000\n",
            "\n",
            "Epoch 283: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2717 - accuracy: 0.9166 - val_loss: 0.2064 - val_accuracy: 0.9418 - 16s/epoch - 38ms/step\n",
            "Epoch 284/1000\n",
            "\n",
            "Epoch 284: val_accuracy did not improve from 0.94980\n",
            "429/429 - 17s - loss: 0.2698 - accuracy: 0.9162 - val_loss: 0.2027 - val_accuracy: 0.9386 - 17s/epoch - 39ms/step\n",
            "Epoch 285/1000\n",
            "\n",
            "Epoch 285: val_accuracy did not improve from 0.94980\n",
            "429/429 - 17s - loss: 0.2706 - accuracy: 0.9158 - val_loss: 0.2060 - val_accuracy: 0.9360 - 17s/epoch - 39ms/step\n",
            "Epoch 286/1000\n",
            "\n",
            "Epoch 286: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2693 - accuracy: 0.9181 - val_loss: 0.2125 - val_accuracy: 0.9340 - 16s/epoch - 38ms/step\n",
            "Epoch 287/1000\n",
            "\n",
            "Epoch 287: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2702 - accuracy: 0.9168 - val_loss: 0.1951 - val_accuracy: 0.9412 - 16s/epoch - 37ms/step\n",
            "Epoch 288/1000\n",
            "\n",
            "Epoch 288: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2675 - accuracy: 0.9171 - val_loss: 0.2109 - val_accuracy: 0.9384 - 16s/epoch - 38ms/step\n",
            "Epoch 289/1000\n",
            "\n",
            "Epoch 289: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2668 - accuracy: 0.9165 - val_loss: 0.1988 - val_accuracy: 0.9472 - 16s/epoch - 37ms/step\n",
            "Epoch 290/1000\n",
            "\n",
            "Epoch 290: val_accuracy did not improve from 0.94980\n",
            "429/429 - 16s - loss: 0.2693 - accuracy: 0.9168 - val_loss: 0.1959 - val_accuracy: 0.9424 - 16s/epoch - 37ms/step\n",
            "Epoch 291/1000\n",
            "\n",
            "Epoch 291: val_accuracy improved from 0.94980 to 0.95040, saving model to best_model,keras\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "429/429 - 18s - loss: 0.2701 - accuracy: 0.9153 - val_loss: 0.1792 - val_accuracy: 0.9504 - 18s/epoch - 43ms/step\n",
            "Epoch 292/1000\n",
            "\n",
            "Epoch 292: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2695 - accuracy: 0.9164 - val_loss: 0.2055 - val_accuracy: 0.9416 - 16s/epoch - 38ms/step\n",
            "Epoch 293/1000\n",
            "\n",
            "Epoch 293: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2680 - accuracy: 0.9167 - val_loss: 0.2038 - val_accuracy: 0.9414 - 16s/epoch - 37ms/step\n",
            "Epoch 294/1000\n",
            "\n",
            "Epoch 294: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2702 - accuracy: 0.9159 - val_loss: 0.1971 - val_accuracy: 0.9436 - 16s/epoch - 38ms/step\n",
            "Epoch 295/1000\n",
            "\n",
            "Epoch 295: val_accuracy did not improve from 0.95040\n",
            "429/429 - 17s - loss: 0.2710 - accuracy: 0.9166 - val_loss: 0.1857 - val_accuracy: 0.9484 - 17s/epoch - 39ms/step\n",
            "Epoch 296/1000\n",
            "\n",
            "Epoch 296: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2717 - accuracy: 0.9158 - val_loss: 0.1860 - val_accuracy: 0.9490 - 16s/epoch - 38ms/step\n",
            "Epoch 297/1000\n",
            "\n",
            "Epoch 297: val_accuracy did not improve from 0.95040\n",
            "429/429 - 17s - loss: 0.2656 - accuracy: 0.9164 - val_loss: 0.2119 - val_accuracy: 0.9366 - 17s/epoch - 39ms/step\n",
            "Epoch 298/1000\n",
            "\n",
            "Epoch 298: val_accuracy did not improve from 0.95040\n",
            "429/429 - 17s - loss: 0.2718 - accuracy: 0.9153 - val_loss: 0.2013 - val_accuracy: 0.9406 - 17s/epoch - 39ms/step\n",
            "Epoch 299/1000\n",
            "\n",
            "Epoch 299: val_accuracy did not improve from 0.95040\n",
            "429/429 - 17s - loss: 0.2685 - accuracy: 0.9177 - val_loss: 0.2143 - val_accuracy: 0.9414 - 17s/epoch - 39ms/step\n",
            "Epoch 300/1000\n",
            "\n",
            "Epoch 300: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2661 - accuracy: 0.9169 - val_loss: 0.1940 - val_accuracy: 0.9428 - 16s/epoch - 38ms/step\n",
            "Epoch 301/1000\n",
            "\n",
            "Epoch 301: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2729 - accuracy: 0.9146 - val_loss: 0.1977 - val_accuracy: 0.9366 - 16s/epoch - 38ms/step\n",
            "Epoch 302/1000\n",
            "\n",
            "Epoch 302: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2690 - accuracy: 0.9166 - val_loss: 0.1995 - val_accuracy: 0.9368 - 16s/epoch - 38ms/step\n",
            "Epoch 303/1000\n",
            "\n",
            "Epoch 303: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2705 - accuracy: 0.9165 - val_loss: 0.1960 - val_accuracy: 0.9440 - 16s/epoch - 37ms/step\n",
            "Epoch 304/1000\n",
            "\n",
            "Epoch 304: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2697 - accuracy: 0.9165 - val_loss: 0.1969 - val_accuracy: 0.9428 - 16s/epoch - 37ms/step\n",
            "Epoch 305/1000\n",
            "\n",
            "Epoch 305: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2656 - accuracy: 0.9185 - val_loss: 0.2117 - val_accuracy: 0.9376 - 16s/epoch - 37ms/step\n",
            "Epoch 306/1000\n",
            "\n",
            "Epoch 306: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2690 - accuracy: 0.9173 - val_loss: 0.2166 - val_accuracy: 0.9330 - 16s/epoch - 37ms/step\n",
            "Epoch 307/1000\n",
            "\n",
            "Epoch 307: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2698 - accuracy: 0.9163 - val_loss: 0.1947 - val_accuracy: 0.9420 - 16s/epoch - 38ms/step\n",
            "Epoch 308/1000\n",
            "\n",
            "Epoch 308: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2713 - accuracy: 0.9159 - val_loss: 0.2067 - val_accuracy: 0.9410 - 16s/epoch - 38ms/step\n",
            "Epoch 309/1000\n",
            "\n",
            "Epoch 309: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2689 - accuracy: 0.9167 - val_loss: 0.2081 - val_accuracy: 0.9402 - 16s/epoch - 38ms/step\n",
            "Epoch 310/1000\n",
            "\n",
            "Epoch 310: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2676 - accuracy: 0.9163 - val_loss: 0.1956 - val_accuracy: 0.9458 - 16s/epoch - 38ms/step\n",
            "Epoch 311/1000\n",
            "\n",
            "Epoch 311: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2665 - accuracy: 0.9166 - val_loss: 0.2212 - val_accuracy: 0.9374 - 16s/epoch - 38ms/step\n",
            "Epoch 312/1000\n",
            "\n",
            "Epoch 312: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2651 - accuracy: 0.9187 - val_loss: 0.1964 - val_accuracy: 0.9408 - 16s/epoch - 38ms/step\n",
            "Epoch 313/1000\n",
            "\n",
            "Epoch 313: val_accuracy did not improve from 0.95040\n",
            "429/429 - 17s - loss: 0.2687 - accuracy: 0.9162 - val_loss: 0.1969 - val_accuracy: 0.9422 - 17s/epoch - 39ms/step\n",
            "Epoch 314/1000\n",
            "\n",
            "Epoch 314: val_accuracy did not improve from 0.95040\n",
            "429/429 - 17s - loss: 0.2690 - accuracy: 0.9163 - val_loss: 0.2270 - val_accuracy: 0.9342 - 17s/epoch - 39ms/step\n",
            "Epoch 315/1000\n",
            "\n",
            "Epoch 315: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2697 - accuracy: 0.9156 - val_loss: 0.2136 - val_accuracy: 0.9394 - 16s/epoch - 37ms/step\n",
            "Epoch 316/1000\n",
            "\n",
            "Epoch 316: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2661 - accuracy: 0.9171 - val_loss: 0.1960 - val_accuracy: 0.9438 - 16s/epoch - 37ms/step\n",
            "Epoch 317/1000\n",
            "\n",
            "Epoch 317: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2716 - accuracy: 0.9149 - val_loss: 0.2124 - val_accuracy: 0.9364 - 16s/epoch - 37ms/step\n",
            "Epoch 318/1000\n",
            "\n",
            "Epoch 318: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2694 - accuracy: 0.9163 - val_loss: 0.2045 - val_accuracy: 0.9410 - 16s/epoch - 37ms/step\n",
            "Epoch 319/1000\n",
            "\n",
            "Epoch 319: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2687 - accuracy: 0.9170 - val_loss: 0.2049 - val_accuracy: 0.9420 - 16s/epoch - 37ms/step\n",
            "Epoch 320/1000\n",
            "\n",
            "Epoch 320: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2690 - accuracy: 0.9177 - val_loss: 0.2152 - val_accuracy: 0.9362 - 16s/epoch - 38ms/step\n",
            "Epoch 321/1000\n",
            "\n",
            "Epoch 321: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2705 - accuracy: 0.9166 - val_loss: 0.1954 - val_accuracy: 0.9462 - 16s/epoch - 37ms/step\n",
            "Epoch 322/1000\n",
            "\n",
            "Epoch 322: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2646 - accuracy: 0.9161 - val_loss: 0.1967 - val_accuracy: 0.9400 - 16s/epoch - 38ms/step\n",
            "Epoch 323/1000\n",
            "\n",
            "Epoch 323: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2684 - accuracy: 0.9155 - val_loss: 0.1928 - val_accuracy: 0.9434 - 16s/epoch - 38ms/step\n",
            "Epoch 324/1000\n",
            "\n",
            "Epoch 324: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2678 - accuracy: 0.9169 - val_loss: 0.1921 - val_accuracy: 0.9432 - 16s/epoch - 38ms/step\n",
            "Epoch 325/1000\n",
            "\n",
            "Epoch 325: val_accuracy did not improve from 0.95040\n",
            "429/429 - 17s - loss: 0.2656 - accuracy: 0.9174 - val_loss: 0.2243 - val_accuracy: 0.9322 - 17s/epoch - 39ms/step\n",
            "Epoch 326/1000\n",
            "\n",
            "Epoch 326: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2700 - accuracy: 0.9160 - val_loss: 0.1882 - val_accuracy: 0.9478 - 16s/epoch - 38ms/step\n",
            "Epoch 327/1000\n",
            "\n",
            "Epoch 327: val_accuracy did not improve from 0.95040\n",
            "429/429 - 17s - loss: 0.2678 - accuracy: 0.9170 - val_loss: 0.1828 - val_accuracy: 0.9486 - 17s/epoch - 39ms/step\n",
            "Epoch 328/1000\n",
            "\n",
            "Epoch 328: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2673 - accuracy: 0.9176 - val_loss: 0.1925 - val_accuracy: 0.9450 - 16s/epoch - 38ms/step\n",
            "Epoch 329/1000\n",
            "\n",
            "Epoch 329: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2703 - accuracy: 0.9165 - val_loss: 0.1883 - val_accuracy: 0.9496 - 16s/epoch - 37ms/step\n",
            "Epoch 330/1000\n",
            "\n",
            "Epoch 330: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2693 - accuracy: 0.9170 - val_loss: 0.2234 - val_accuracy: 0.9356 - 16s/epoch - 36ms/step\n",
            "Epoch 331/1000\n",
            "\n",
            "Epoch 331: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2678 - accuracy: 0.9167 - val_loss: 0.2086 - val_accuracy: 0.9382 - 16s/epoch - 36ms/step\n",
            "Epoch 332/1000\n",
            "\n",
            "Epoch 332: val_accuracy did not improve from 0.95040\n",
            "429/429 - 17s - loss: 0.2672 - accuracy: 0.9164 - val_loss: 0.2224 - val_accuracy: 0.9318 - 17s/epoch - 39ms/step\n",
            "Epoch 333/1000\n",
            "\n",
            "Epoch 333: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2677 - accuracy: 0.9168 - val_loss: 0.2381 - val_accuracy: 0.9278 - 16s/epoch - 37ms/step\n",
            "Epoch 334/1000\n",
            "\n",
            "Epoch 334: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2676 - accuracy: 0.9168 - val_loss: 0.1974 - val_accuracy: 0.9396 - 16s/epoch - 37ms/step\n",
            "Epoch 335/1000\n",
            "\n",
            "Epoch 335: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2669 - accuracy: 0.9172 - val_loss: 0.2228 - val_accuracy: 0.9322 - 16s/epoch - 37ms/step\n",
            "Epoch 336/1000\n",
            "\n",
            "Epoch 336: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2683 - accuracy: 0.9168 - val_loss: 0.2047 - val_accuracy: 0.9396 - 16s/epoch - 38ms/step\n",
            "Epoch 337/1000\n",
            "\n",
            "Epoch 337: val_accuracy did not improve from 0.95040\n",
            "429/429 - 17s - loss: 0.2685 - accuracy: 0.9158 - val_loss: 0.2135 - val_accuracy: 0.9388 - 17s/epoch - 39ms/step\n",
            "Epoch 338/1000\n",
            "\n",
            "Epoch 338: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2676 - accuracy: 0.9166 - val_loss: 0.1989 - val_accuracy: 0.9414 - 16s/epoch - 38ms/step\n",
            "Epoch 339/1000\n",
            "\n",
            "Epoch 339: val_accuracy did not improve from 0.95040\n",
            "429/429 - 19s - loss: 0.2685 - accuracy: 0.9179 - val_loss: 0.1918 - val_accuracy: 0.9422 - 19s/epoch - 45ms/step\n",
            "Epoch 340/1000\n",
            "\n",
            "Epoch 340: val_accuracy did not improve from 0.95040\n",
            "429/429 - 19s - loss: 0.2659 - accuracy: 0.9173 - val_loss: 0.2076 - val_accuracy: 0.9390 - 19s/epoch - 45ms/step\n",
            "Epoch 341/1000\n",
            "\n",
            "Epoch 341: val_accuracy did not improve from 0.95040\n",
            "429/429 - 18s - loss: 0.2690 - accuracy: 0.9163 - val_loss: 0.2223 - val_accuracy: 0.9318 - 18s/epoch - 43ms/step\n",
            "Epoch 342/1000\n",
            "\n",
            "Epoch 342: val_accuracy did not improve from 0.95040\n",
            "429/429 - 17s - loss: 0.2668 - accuracy: 0.9181 - val_loss: 0.2247 - val_accuracy: 0.9288 - 17s/epoch - 39ms/step\n",
            "Epoch 343/1000\n",
            "\n",
            "Epoch 343: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2662 - accuracy: 0.9179 - val_loss: 0.2003 - val_accuracy: 0.9422 - 16s/epoch - 37ms/step\n",
            "Epoch 344/1000\n",
            "\n",
            "Epoch 344: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2672 - accuracy: 0.9167 - val_loss: 0.2116 - val_accuracy: 0.9378 - 16s/epoch - 37ms/step\n",
            "Epoch 345/1000\n",
            "\n",
            "Epoch 345: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2713 - accuracy: 0.9156 - val_loss: 0.2067 - val_accuracy: 0.9404 - 16s/epoch - 37ms/step\n",
            "Epoch 346/1000\n",
            "\n",
            "Epoch 346: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2691 - accuracy: 0.9174 - val_loss: 0.2003 - val_accuracy: 0.9418 - 16s/epoch - 38ms/step\n",
            "Epoch 347/1000\n",
            "\n",
            "Epoch 347: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2652 - accuracy: 0.9180 - val_loss: 0.2030 - val_accuracy: 0.9406 - 16s/epoch - 37ms/step\n",
            "Epoch 348/1000\n",
            "\n",
            "Epoch 348: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2685 - accuracy: 0.9177 - val_loss: 0.2138 - val_accuracy: 0.9372 - 16s/epoch - 37ms/step\n",
            "Epoch 349/1000\n",
            "\n",
            "Epoch 349: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2701 - accuracy: 0.9164 - val_loss: 0.1915 - val_accuracy: 0.9430 - 16s/epoch - 37ms/step\n",
            "Epoch 350/1000\n",
            "\n",
            "Epoch 350: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2678 - accuracy: 0.9164 - val_loss: 0.2148 - val_accuracy: 0.9354 - 16s/epoch - 37ms/step\n",
            "Epoch 351/1000\n",
            "\n",
            "Epoch 351: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2688 - accuracy: 0.9156 - val_loss: 0.1991 - val_accuracy: 0.9408 - 16s/epoch - 38ms/step\n",
            "Epoch 352/1000\n",
            "\n",
            "Epoch 352: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2643 - accuracy: 0.9181 - val_loss: 0.2042 - val_accuracy: 0.9432 - 16s/epoch - 38ms/step\n",
            "Epoch 353/1000\n",
            "\n",
            "Epoch 353: val_accuracy did not improve from 0.95040\n",
            "429/429 - 17s - loss: 0.2674 - accuracy: 0.9177 - val_loss: 0.2314 - val_accuracy: 0.9278 - 17s/epoch - 39ms/step\n",
            "Epoch 354/1000\n",
            "\n",
            "Epoch 354: val_accuracy did not improve from 0.95040\n",
            "429/429 - 17s - loss: 0.2649 - accuracy: 0.9175 - val_loss: 0.1918 - val_accuracy: 0.9440 - 17s/epoch - 39ms/step\n",
            "Epoch 355/1000\n",
            "\n",
            "Epoch 355: val_accuracy did not improve from 0.95040\n",
            "429/429 - 17s - loss: 0.2645 - accuracy: 0.9181 - val_loss: 0.1982 - val_accuracy: 0.9452 - 17s/epoch - 39ms/step\n",
            "Epoch 356/1000\n",
            "\n",
            "Epoch 356: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2665 - accuracy: 0.9161 - val_loss: 0.1997 - val_accuracy: 0.9390 - 16s/epoch - 38ms/step\n",
            "Epoch 357/1000\n",
            "\n",
            "Epoch 357: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2666 - accuracy: 0.9171 - val_loss: 0.2139 - val_accuracy: 0.9372 - 16s/epoch - 37ms/step\n",
            "Epoch 358/1000\n",
            "\n",
            "Epoch 358: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2658 - accuracy: 0.9189 - val_loss: 0.2020 - val_accuracy: 0.9436 - 16s/epoch - 37ms/step\n",
            "Epoch 359/1000\n",
            "\n",
            "Epoch 359: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2676 - accuracy: 0.9168 - val_loss: 0.2030 - val_accuracy: 0.9398 - 16s/epoch - 36ms/step\n",
            "Epoch 360/1000\n",
            "\n",
            "Epoch 360: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2645 - accuracy: 0.9179 - val_loss: 0.1864 - val_accuracy: 0.9468 - 16s/epoch - 37ms/step\n",
            "Epoch 361/1000\n",
            "\n",
            "Epoch 361: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2696 - accuracy: 0.9162 - val_loss: 0.2036 - val_accuracy: 0.9418 - 16s/epoch - 37ms/step\n",
            "Epoch 362/1000\n",
            "\n",
            "Epoch 362: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2656 - accuracy: 0.9177 - val_loss: 0.1958 - val_accuracy: 0.9412 - 16s/epoch - 36ms/step\n",
            "Epoch 363/1000\n",
            "\n",
            "Epoch 363: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2698 - accuracy: 0.9169 - val_loss: 0.1893 - val_accuracy: 0.9444 - 16s/epoch - 37ms/step\n",
            "Epoch 364/1000\n",
            "\n",
            "Epoch 364: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2671 - accuracy: 0.9168 - val_loss: 0.1941 - val_accuracy: 0.9440 - 16s/epoch - 38ms/step\n",
            "Epoch 365/1000\n",
            "\n",
            "Epoch 365: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2675 - accuracy: 0.9172 - val_loss: 0.1909 - val_accuracy: 0.9492 - 16s/epoch - 37ms/step\n",
            "Epoch 366/1000\n",
            "\n",
            "Epoch 366: val_accuracy did not improve from 0.95040\n",
            "429/429 - 17s - loss: 0.2652 - accuracy: 0.9189 - val_loss: 0.1942 - val_accuracy: 0.9456 - 17s/epoch - 39ms/step\n",
            "Epoch 367/1000\n",
            "\n",
            "Epoch 367: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2640 - accuracy: 0.9178 - val_loss: 0.1773 - val_accuracy: 0.9486 - 16s/epoch - 38ms/step\n",
            "Epoch 368/1000\n",
            "\n",
            "Epoch 368: val_accuracy did not improve from 0.95040\n",
            "429/429 - 17s - loss: 0.2654 - accuracy: 0.9185 - val_loss: 0.2016 - val_accuracy: 0.9418 - 17s/epoch - 39ms/step\n",
            "Epoch 369/1000\n",
            "\n",
            "Epoch 369: val_accuracy did not improve from 0.95040\n",
            "429/429 - 17s - loss: 0.2685 - accuracy: 0.9167 - val_loss: 0.2109 - val_accuracy: 0.9356 - 17s/epoch - 39ms/step\n",
            "Epoch 370/1000\n",
            "\n",
            "Epoch 370: val_accuracy did not improve from 0.95040\n",
            "429/429 - 17s - loss: 0.2658 - accuracy: 0.9182 - val_loss: 0.1865 - val_accuracy: 0.9448 - 17s/epoch - 41ms/step\n",
            "Epoch 371/1000\n",
            "\n",
            "Epoch 371: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2675 - accuracy: 0.9173 - val_loss: 0.1867 - val_accuracy: 0.9466 - 16s/epoch - 37ms/step\n",
            "Epoch 372/1000\n",
            "\n",
            "Epoch 372: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2691 - accuracy: 0.9167 - val_loss: 0.2219 - val_accuracy: 0.9366 - 16s/epoch - 37ms/step\n",
            "Epoch 373/1000\n",
            "\n",
            "Epoch 373: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2665 - accuracy: 0.9168 - val_loss: 0.2107 - val_accuracy: 0.9414 - 16s/epoch - 36ms/step\n",
            "Epoch 374/1000\n",
            "\n",
            "Epoch 374: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2680 - accuracy: 0.9170 - val_loss: 0.2094 - val_accuracy: 0.9348 - 16s/epoch - 37ms/step\n",
            "Epoch 375/1000\n",
            "\n",
            "Epoch 375: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2674 - accuracy: 0.9182 - val_loss: 0.2023 - val_accuracy: 0.9424 - 16s/epoch - 37ms/step\n",
            "Epoch 376/1000\n",
            "\n",
            "Epoch 376: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2676 - accuracy: 0.9176 - val_loss: 0.2095 - val_accuracy: 0.9362 - 16s/epoch - 37ms/step\n",
            "Epoch 377/1000\n",
            "\n",
            "Epoch 377: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2677 - accuracy: 0.9166 - val_loss: 0.2115 - val_accuracy: 0.9352 - 16s/epoch - 37ms/step\n",
            "Epoch 378/1000\n",
            "\n",
            "Epoch 378: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2690 - accuracy: 0.9153 - val_loss: 0.2217 - val_accuracy: 0.9286 - 16s/epoch - 37ms/step\n",
            "Epoch 379/1000\n",
            "\n",
            "Epoch 379: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2664 - accuracy: 0.9172 - val_loss: 0.2073 - val_accuracy: 0.9416 - 16s/epoch - 38ms/step\n",
            "Epoch 380/1000\n",
            "\n",
            "Epoch 380: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2710 - accuracy: 0.9173 - val_loss: 0.1961 - val_accuracy: 0.9438 - 16s/epoch - 37ms/step\n",
            "Epoch 381/1000\n",
            "\n",
            "Epoch 381: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2666 - accuracy: 0.9176 - val_loss: 0.2262 - val_accuracy: 0.9302 - 16s/epoch - 38ms/step\n",
            "Epoch 382/1000\n",
            "\n",
            "Epoch 382: val_accuracy did not improve from 0.95040\n",
            "429/429 - 17s - loss: 0.2649 - accuracy: 0.9175 - val_loss: 0.1865 - val_accuracy: 0.9448 - 17s/epoch - 39ms/step\n",
            "Epoch 383/1000\n",
            "\n",
            "Epoch 383: val_accuracy did not improve from 0.95040\n",
            "429/429 - 17s - loss: 0.2651 - accuracy: 0.9179 - val_loss: 0.2050 - val_accuracy: 0.9380 - 17s/epoch - 39ms/step\n",
            "Epoch 384/1000\n",
            "\n",
            "Epoch 384: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2658 - accuracy: 0.9162 - val_loss: 0.1917 - val_accuracy: 0.9430 - 16s/epoch - 37ms/step\n",
            "Epoch 385/1000\n",
            "\n",
            "Epoch 385: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2661 - accuracy: 0.9176 - val_loss: 0.2169 - val_accuracy: 0.9364 - 16s/epoch - 37ms/step\n",
            "Epoch 386/1000\n",
            "\n",
            "Epoch 386: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2671 - accuracy: 0.9182 - val_loss: 0.1880 - val_accuracy: 0.9452 - 16s/epoch - 37ms/step\n",
            "Epoch 387/1000\n",
            "\n",
            "Epoch 387: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2676 - accuracy: 0.9166 - val_loss: 0.2037 - val_accuracy: 0.9402 - 16s/epoch - 37ms/step\n",
            "Epoch 388/1000\n",
            "\n",
            "Epoch 388: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2663 - accuracy: 0.9178 - val_loss: 0.1915 - val_accuracy: 0.9454 - 16s/epoch - 37ms/step\n",
            "Epoch 389/1000\n",
            "\n",
            "Epoch 389: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2653 - accuracy: 0.9180 - val_loss: 0.1925 - val_accuracy: 0.9420 - 16s/epoch - 36ms/step\n",
            "Epoch 390/1000\n",
            "\n",
            "Epoch 390: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2672 - accuracy: 0.9172 - val_loss: 0.2091 - val_accuracy: 0.9390 - 16s/epoch - 37ms/step\n",
            "Epoch 391/1000\n",
            "\n",
            "Epoch 391: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2667 - accuracy: 0.9161 - val_loss: 0.1968 - val_accuracy: 0.9418 - 16s/epoch - 38ms/step\n",
            "Epoch 392/1000\n",
            "\n",
            "Epoch 392: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2697 - accuracy: 0.9176 - val_loss: 0.1956 - val_accuracy: 0.9404 - 16s/epoch - 38ms/step\n",
            "Epoch 393/1000\n",
            "\n",
            "Epoch 393: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2686 - accuracy: 0.9169 - val_loss: 0.2046 - val_accuracy: 0.9402 - 16s/epoch - 38ms/step\n",
            "Epoch 394/1000\n",
            "\n",
            "Epoch 394: val_accuracy did not improve from 0.95040\n",
            "429/429 - 15s - loss: 0.2669 - accuracy: 0.9157 - val_loss: 0.1913 - val_accuracy: 0.9478 - 15s/epoch - 35ms/step\n",
            "Epoch 395/1000\n",
            "\n",
            "Epoch 395: val_accuracy did not improve from 0.95040\n",
            "429/429 - 15s - loss: 0.2650 - accuracy: 0.9174 - val_loss: 0.2206 - val_accuracy: 0.9334 - 15s/epoch - 35ms/step\n",
            "Epoch 396/1000\n",
            "\n",
            "Epoch 396: val_accuracy did not improve from 0.95040\n",
            "429/429 - 15s - loss: 0.2647 - accuracy: 0.9170 - val_loss: 0.1964 - val_accuracy: 0.9414 - 15s/epoch - 35ms/step\n",
            "Epoch 397/1000\n",
            "\n",
            "Epoch 397: val_accuracy did not improve from 0.95040\n",
            "429/429 - 15s - loss: 0.2682 - accuracy: 0.9163 - val_loss: 0.2013 - val_accuracy: 0.9414 - 15s/epoch - 35ms/step\n",
            "Epoch 398/1000\n",
            "\n",
            "Epoch 398: val_accuracy did not improve from 0.95040\n",
            "429/429 - 17s - loss: 0.2639 - accuracy: 0.9171 - val_loss: 0.2102 - val_accuracy: 0.9352 - 17s/epoch - 39ms/step\n",
            "Epoch 399/1000\n",
            "\n",
            "Epoch 399: val_accuracy did not improve from 0.95040\n",
            "429/429 - 17s - loss: 0.2639 - accuracy: 0.9191 - val_loss: 0.2108 - val_accuracy: 0.9376 - 17s/epoch - 39ms/step\n",
            "Epoch 400/1000\n",
            "\n",
            "Epoch 400: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2637 - accuracy: 0.9194 - val_loss: 0.2266 - val_accuracy: 0.9302 - 16s/epoch - 38ms/step\n",
            "Epoch 401/1000\n",
            "\n",
            "Epoch 401: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2691 - accuracy: 0.9171 - val_loss: 0.2022 - val_accuracy: 0.9436 - 16s/epoch - 38ms/step\n",
            "Epoch 402/1000\n",
            "\n",
            "Epoch 402: val_accuracy did not improve from 0.95040\n",
            "429/429 - 17s - loss: 0.2675 - accuracy: 0.9179 - val_loss: 0.2107 - val_accuracy: 0.9358 - 17s/epoch - 39ms/step\n",
            "Epoch 403/1000\n",
            "\n",
            "Epoch 403: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2672 - accuracy: 0.9179 - val_loss: 0.2157 - val_accuracy: 0.9362 - 16s/epoch - 36ms/step\n",
            "Epoch 404/1000\n",
            "\n",
            "Epoch 404: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2648 - accuracy: 0.9185 - val_loss: 0.2074 - val_accuracy: 0.9382 - 16s/epoch - 37ms/step\n",
            "Epoch 405/1000\n",
            "\n",
            "Epoch 405: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2652 - accuracy: 0.9172 - val_loss: 0.2060 - val_accuracy: 0.9382 - 16s/epoch - 36ms/step\n",
            "Epoch 406/1000\n",
            "\n",
            "Epoch 406: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2660 - accuracy: 0.9188 - val_loss: 0.1904 - val_accuracy: 0.9416 - 16s/epoch - 37ms/step\n",
            "Epoch 407/1000\n",
            "\n",
            "Epoch 407: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2631 - accuracy: 0.9179 - val_loss: 0.1830 - val_accuracy: 0.9458 - 16s/epoch - 37ms/step\n",
            "Epoch 408/1000\n",
            "\n",
            "Epoch 408: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2617 - accuracy: 0.9194 - val_loss: 0.1897 - val_accuracy: 0.9468 - 16s/epoch - 38ms/step\n",
            "Epoch 409/1000\n",
            "\n",
            "Epoch 409: val_accuracy did not improve from 0.95040\n",
            "429/429 - 15s - loss: 0.2654 - accuracy: 0.9179 - val_loss: 0.1843 - val_accuracy: 0.9458 - 15s/epoch - 35ms/step\n",
            "Epoch 410/1000\n",
            "\n",
            "Epoch 410: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2657 - accuracy: 0.9181 - val_loss: 0.2131 - val_accuracy: 0.9350 - 16s/epoch - 38ms/step\n",
            "Epoch 411/1000\n",
            "\n",
            "Epoch 411: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2641 - accuracy: 0.9181 - val_loss: 0.2268 - val_accuracy: 0.9328 - 16s/epoch - 37ms/step\n",
            "Epoch 412/1000\n",
            "\n",
            "Epoch 412: val_accuracy did not improve from 0.95040\n",
            "429/429 - 17s - loss: 0.2641 - accuracy: 0.9193 - val_loss: 0.1933 - val_accuracy: 0.9452 - 17s/epoch - 39ms/step\n",
            "Epoch 413/1000\n",
            "\n",
            "Epoch 413: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2668 - accuracy: 0.9174 - val_loss: 0.2509 - val_accuracy: 0.9152 - 16s/epoch - 38ms/step\n",
            "Epoch 414/1000\n",
            "\n",
            "Epoch 414: val_accuracy did not improve from 0.95040\n",
            "429/429 - 17s - loss: 0.2645 - accuracy: 0.9183 - val_loss: 0.2020 - val_accuracy: 0.9386 - 17s/epoch - 39ms/step\n",
            "Epoch 415/1000\n",
            "\n",
            "Epoch 415: val_accuracy did not improve from 0.95040\n",
            "429/429 - 15s - loss: 0.2686 - accuracy: 0.9180 - val_loss: 0.2347 - val_accuracy: 0.9278 - 15s/epoch - 35ms/step\n",
            "Epoch 416/1000\n",
            "\n",
            "Epoch 416: val_accuracy did not improve from 0.95040\n",
            "429/429 - 15s - loss: 0.2689 - accuracy: 0.9168 - val_loss: 0.2110 - val_accuracy: 0.9392 - 15s/epoch - 35ms/step\n",
            "Epoch 417/1000\n",
            "\n",
            "Epoch 417: val_accuracy did not improve from 0.95040\n",
            "429/429 - 15s - loss: 0.2686 - accuracy: 0.9170 - val_loss: 0.2241 - val_accuracy: 0.9294 - 15s/epoch - 35ms/step\n",
            "Epoch 418/1000\n",
            "\n",
            "Epoch 418: val_accuracy did not improve from 0.95040\n",
            "429/429 - 15s - loss: 0.2689 - accuracy: 0.9167 - val_loss: 0.2253 - val_accuracy: 0.9340 - 15s/epoch - 35ms/step\n",
            "Epoch 419/1000\n",
            "\n",
            "Epoch 419: val_accuracy did not improve from 0.95040\n",
            "429/429 - 15s - loss: 0.2654 - accuracy: 0.9178 - val_loss: 0.1959 - val_accuracy: 0.9432 - 15s/epoch - 35ms/step\n",
            "Epoch 420/1000\n",
            "\n",
            "Epoch 420: val_accuracy did not improve from 0.95040\n",
            "429/429 - 18s - loss: 0.2663 - accuracy: 0.9170 - val_loss: 0.2159 - val_accuracy: 0.9368 - 18s/epoch - 42ms/step\n",
            "Epoch 421/1000\n",
            "\n",
            "Epoch 421: val_accuracy did not improve from 0.95040\n",
            "429/429 - 18s - loss: 0.2651 - accuracy: 0.9167 - val_loss: 0.1995 - val_accuracy: 0.9344 - 18s/epoch - 43ms/step\n",
            "Epoch 422/1000\n",
            "\n",
            "Epoch 422: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2706 - accuracy: 0.9163 - val_loss: 0.2087 - val_accuracy: 0.9374 - 16s/epoch - 36ms/step\n",
            "Epoch 423/1000\n",
            "\n",
            "Epoch 423: val_accuracy did not improve from 0.95040\n",
            "429/429 - 15s - loss: 0.2669 - accuracy: 0.9168 - val_loss: 0.1878 - val_accuracy: 0.9448 - 15s/epoch - 35ms/step\n",
            "Epoch 424/1000\n",
            "\n",
            "Epoch 424: val_accuracy did not improve from 0.95040\n",
            "429/429 - 15s - loss: 0.2640 - accuracy: 0.9190 - val_loss: 0.2087 - val_accuracy: 0.9384 - 15s/epoch - 35ms/step\n",
            "Epoch 425/1000\n",
            "\n",
            "Epoch 425: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2635 - accuracy: 0.9173 - val_loss: 0.2242 - val_accuracy: 0.9304 - 16s/epoch - 38ms/step\n",
            "Epoch 426/1000\n",
            "\n",
            "Epoch 426: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2656 - accuracy: 0.9181 - val_loss: 0.2118 - val_accuracy: 0.9346 - 16s/epoch - 38ms/step\n",
            "Epoch 427/1000\n",
            "\n",
            "Epoch 427: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2666 - accuracy: 0.9166 - val_loss: 0.1828 - val_accuracy: 0.9446 - 16s/epoch - 37ms/step\n",
            "Epoch 428/1000\n",
            "\n",
            "Epoch 428: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2657 - accuracy: 0.9190 - val_loss: 0.2008 - val_accuracy: 0.9380 - 16s/epoch - 37ms/step\n",
            "Epoch 429/1000\n",
            "\n",
            "Epoch 429: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2649 - accuracy: 0.9180 - val_loss: 0.2065 - val_accuracy: 0.9352 - 16s/epoch - 37ms/step\n",
            "Epoch 430/1000\n",
            "\n",
            "Epoch 430: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2621 - accuracy: 0.9184 - val_loss: 0.1860 - val_accuracy: 0.9436 - 16s/epoch - 36ms/step\n",
            "Epoch 431/1000\n",
            "\n",
            "Epoch 431: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2641 - accuracy: 0.9189 - val_loss: 0.2251 - val_accuracy: 0.9346 - 16s/epoch - 37ms/step\n",
            "Epoch 432/1000\n",
            "\n",
            "Epoch 432: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2672 - accuracy: 0.9165 - val_loss: 0.1891 - val_accuracy: 0.9426 - 16s/epoch - 37ms/step\n",
            "Epoch 433/1000\n",
            "\n",
            "Epoch 433: val_accuracy did not improve from 0.95040\n",
            "429/429 - 15s - loss: 0.2631 - accuracy: 0.9178 - val_loss: 0.2111 - val_accuracy: 0.9362 - 15s/epoch - 36ms/step\n",
            "Epoch 434/1000\n",
            "\n",
            "Epoch 434: val_accuracy did not improve from 0.95040\n",
            "429/429 - 15s - loss: 0.2633 - accuracy: 0.9185 - val_loss: 0.1935 - val_accuracy: 0.9422 - 15s/epoch - 35ms/step\n",
            "Epoch 435/1000\n",
            "\n",
            "Epoch 435: val_accuracy did not improve from 0.95040\n",
            "429/429 - 15s - loss: 0.2652 - accuracy: 0.9171 - val_loss: 0.2177 - val_accuracy: 0.9320 - 15s/epoch - 35ms/step\n",
            "Epoch 436/1000\n",
            "\n",
            "Epoch 436: val_accuracy did not improve from 0.95040\n",
            "429/429 - 15s - loss: 0.2667 - accuracy: 0.9176 - val_loss: 0.1966 - val_accuracy: 0.9398 - 15s/epoch - 35ms/step\n",
            "Epoch 437/1000\n",
            "\n",
            "Epoch 437: val_accuracy did not improve from 0.95040\n",
            "429/429 - 15s - loss: 0.2671 - accuracy: 0.9192 - val_loss: 0.2263 - val_accuracy: 0.9306 - 15s/epoch - 36ms/step\n",
            "Epoch 438/1000\n",
            "\n",
            "Epoch 438: val_accuracy did not improve from 0.95040\n",
            "429/429 - 15s - loss: 0.2620 - accuracy: 0.9187 - val_loss: 0.1894 - val_accuracy: 0.9434 - 15s/epoch - 35ms/step\n",
            "Epoch 439/1000\n",
            "\n",
            "Epoch 439: val_accuracy did not improve from 0.95040\n",
            "429/429 - 17s - loss: 0.2657 - accuracy: 0.9176 - val_loss: 0.2283 - val_accuracy: 0.9342 - 17s/epoch - 39ms/step\n",
            "Epoch 440/1000\n",
            "\n",
            "Epoch 440: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2672 - accuracy: 0.9168 - val_loss: 0.2061 - val_accuracy: 0.9386 - 16s/epoch - 38ms/step\n",
            "Epoch 441/1000\n",
            "\n",
            "Epoch 441: val_accuracy did not improve from 0.95040\n",
            "429/429 - 16s - loss: 0.2652 - accuracy: 0.9173 - val_loss: 0.1900 - val_accuracy: 0.9408 - 16s/epoch - 37ms/step\n",
            "Epoch 442/1000\n",
            "\n",
            "Epoch 442: val_accuracy improved from 0.95040 to 0.95080, saving model to best_model,keras\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "429/429 - 18s - loss: 0.2673 - accuracy: 0.9173 - val_loss: 0.1836 - val_accuracy: 0.9508 - 18s/epoch - 42ms/step\n",
            "Epoch 443/1000\n",
            "\n",
            "Epoch 443: val_accuracy did not improve from 0.95080\n",
            "429/429 - 17s - loss: 0.2663 - accuracy: 0.9173 - val_loss: 0.2016 - val_accuracy: 0.9408 - 17s/epoch - 40ms/step\n",
            "Epoch 444/1000\n",
            "\n",
            "Epoch 444: val_accuracy did not improve from 0.95080\n",
            "429/429 - 17s - loss: 0.2670 - accuracy: 0.9167 - val_loss: 0.1911 - val_accuracy: 0.9468 - 17s/epoch - 39ms/step\n",
            "Epoch 445/1000\n",
            "\n",
            "Epoch 445: val_accuracy did not improve from 0.95080\n",
            "429/429 - 17s - loss: 0.2651 - accuracy: 0.9165 - val_loss: 0.2011 - val_accuracy: 0.9422 - 17s/epoch - 39ms/step\n",
            "Epoch 446/1000\n",
            "\n",
            "Epoch 446: val_accuracy did not improve from 0.95080\n",
            "429/429 - 17s - loss: 0.2642 - accuracy: 0.9185 - val_loss: 0.2174 - val_accuracy: 0.9354 - 17s/epoch - 39ms/step\n",
            "Epoch 447/1000\n",
            "\n",
            "Epoch 447: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2669 - accuracy: 0.9174 - val_loss: 0.2447 - val_accuracy: 0.9248 - 16s/epoch - 37ms/step\n",
            "Epoch 448/1000\n",
            "\n",
            "Epoch 448: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2691 - accuracy: 0.9172 - val_loss: 0.2050 - val_accuracy: 0.9400 - 16s/epoch - 37ms/step\n",
            "Epoch 449/1000\n",
            "\n",
            "Epoch 449: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2659 - accuracy: 0.9173 - val_loss: 0.1874 - val_accuracy: 0.9460 - 16s/epoch - 38ms/step\n",
            "Epoch 450/1000\n",
            "\n",
            "Epoch 450: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2621 - accuracy: 0.9193 - val_loss: 0.2067 - val_accuracy: 0.9368 - 16s/epoch - 37ms/step\n",
            "Epoch 451/1000\n",
            "\n",
            "Epoch 451: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2665 - accuracy: 0.9170 - val_loss: 0.1906 - val_accuracy: 0.9438 - 16s/epoch - 37ms/step\n",
            "Epoch 452/1000\n",
            "\n",
            "Epoch 452: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2658 - accuracy: 0.9182 - val_loss: 0.2212 - val_accuracy: 0.9332 - 16s/epoch - 37ms/step\n",
            "Epoch 453/1000\n",
            "\n",
            "Epoch 453: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2651 - accuracy: 0.9175 - val_loss: 0.1888 - val_accuracy: 0.9428 - 16s/epoch - 37ms/step\n",
            "Epoch 454/1000\n",
            "\n",
            "Epoch 454: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2666 - accuracy: 0.9178 - val_loss: 0.2087 - val_accuracy: 0.9368 - 16s/epoch - 38ms/step\n",
            "Epoch 455/1000\n",
            "\n",
            "Epoch 455: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2659 - accuracy: 0.9183 - val_loss: 0.2069 - val_accuracy: 0.9346 - 16s/epoch - 38ms/step\n",
            "Epoch 456/1000\n",
            "\n",
            "Epoch 456: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2647 - accuracy: 0.9178 - val_loss: 0.1898 - val_accuracy: 0.9442 - 16s/epoch - 38ms/step\n",
            "Epoch 457/1000\n",
            "\n",
            "Epoch 457: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2679 - accuracy: 0.9161 - val_loss: 0.2066 - val_accuracy: 0.9384 - 16s/epoch - 38ms/step\n",
            "Epoch 458/1000\n",
            "\n",
            "Epoch 458: val_accuracy did not improve from 0.95080\n",
            "429/429 - 17s - loss: 0.2652 - accuracy: 0.9184 - val_loss: 0.2062 - val_accuracy: 0.9382 - 17s/epoch - 39ms/step\n",
            "Epoch 459/1000\n",
            "\n",
            "Epoch 459: val_accuracy did not improve from 0.95080\n",
            "429/429 - 17s - loss: 0.2668 - accuracy: 0.9164 - val_loss: 0.2065 - val_accuracy: 0.9372 - 17s/epoch - 39ms/step\n",
            "Epoch 460/1000\n",
            "\n",
            "Epoch 460: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2623 - accuracy: 0.9199 - val_loss: 0.2143 - val_accuracy: 0.9356 - 16s/epoch - 38ms/step\n",
            "Epoch 461/1000\n",
            "\n",
            "Epoch 461: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2668 - accuracy: 0.9166 - val_loss: 0.1936 - val_accuracy: 0.9446 - 16s/epoch - 38ms/step\n",
            "Epoch 462/1000\n",
            "\n",
            "Epoch 462: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2674 - accuracy: 0.9179 - val_loss: 0.1999 - val_accuracy: 0.9464 - 16s/epoch - 37ms/step\n",
            "Epoch 463/1000\n",
            "\n",
            "Epoch 463: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2633 - accuracy: 0.9179 - val_loss: 0.1936 - val_accuracy: 0.9460 - 16s/epoch - 37ms/step\n",
            "Epoch 464/1000\n",
            "\n",
            "Epoch 464: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2645 - accuracy: 0.9181 - val_loss: 0.1919 - val_accuracy: 0.9446 - 16s/epoch - 37ms/step\n",
            "Epoch 465/1000\n",
            "\n",
            "Epoch 465: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2681 - accuracy: 0.9166 - val_loss: 0.1891 - val_accuracy: 0.9468 - 16s/epoch - 38ms/step\n",
            "Epoch 466/1000\n",
            "\n",
            "Epoch 466: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2663 - accuracy: 0.9164 - val_loss: 0.2033 - val_accuracy: 0.9420 - 16s/epoch - 38ms/step\n",
            "Epoch 467/1000\n",
            "\n",
            "Epoch 467: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2635 - accuracy: 0.9183 - val_loss: 0.1950 - val_accuracy: 0.9422 - 16s/epoch - 37ms/step\n",
            "Epoch 468/1000\n",
            "\n",
            "Epoch 468: val_accuracy did not improve from 0.95080\n",
            "429/429 - 17s - loss: 0.2623 - accuracy: 0.9194 - val_loss: 0.1868 - val_accuracy: 0.9470 - 17s/epoch - 39ms/step\n",
            "Epoch 469/1000\n",
            "\n",
            "Epoch 469: val_accuracy did not improve from 0.95080\n",
            "429/429 - 15s - loss: 0.2610 - accuracy: 0.9192 - val_loss: 0.2001 - val_accuracy: 0.9424 - 15s/epoch - 35ms/step\n",
            "Epoch 470/1000\n",
            "\n",
            "Epoch 470: val_accuracy did not improve from 0.95080\n",
            "429/429 - 15s - loss: 0.2676 - accuracy: 0.9169 - val_loss: 0.2045 - val_accuracy: 0.9402 - 15s/epoch - 35ms/step\n",
            "Epoch 471/1000\n",
            "\n",
            "Epoch 471: val_accuracy did not improve from 0.95080\n",
            "429/429 - 15s - loss: 0.2681 - accuracy: 0.9174 - val_loss: 0.2019 - val_accuracy: 0.9418 - 15s/epoch - 35ms/step\n",
            "Epoch 472/1000\n",
            "\n",
            "Epoch 472: val_accuracy did not improve from 0.95080\n",
            "429/429 - 15s - loss: 0.2661 - accuracy: 0.9171 - val_loss: 0.2304 - val_accuracy: 0.9286 - 15s/epoch - 35ms/step\n",
            "Epoch 473/1000\n",
            "\n",
            "Epoch 473: val_accuracy did not improve from 0.95080\n",
            "429/429 - 15s - loss: 0.2661 - accuracy: 0.9181 - val_loss: 0.1794 - val_accuracy: 0.9484 - 15s/epoch - 35ms/step\n",
            "Epoch 474/1000\n",
            "\n",
            "Epoch 474: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2598 - accuracy: 0.9194 - val_loss: 0.1957 - val_accuracy: 0.9454 - 16s/epoch - 38ms/step\n",
            "Epoch 475/1000\n",
            "\n",
            "Epoch 475: val_accuracy did not improve from 0.95080\n",
            "429/429 - 15s - loss: 0.2653 - accuracy: 0.9167 - val_loss: 0.2022 - val_accuracy: 0.9380 - 15s/epoch - 35ms/step\n",
            "Epoch 476/1000\n",
            "\n",
            "Epoch 476: val_accuracy did not improve from 0.95080\n",
            "429/429 - 17s - loss: 0.2645 - accuracy: 0.9187 - val_loss: 0.1915 - val_accuracy: 0.9438 - 17s/epoch - 39ms/step\n",
            "Epoch 477/1000\n",
            "\n",
            "Epoch 477: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2634 - accuracy: 0.9190 - val_loss: 0.2091 - val_accuracy: 0.9336 - 16s/epoch - 38ms/step\n",
            "Epoch 478/1000\n",
            "\n",
            "Epoch 478: val_accuracy did not improve from 0.95080\n",
            "429/429 - 17s - loss: 0.2658 - accuracy: 0.9171 - val_loss: 0.1959 - val_accuracy: 0.9422 - 17s/epoch - 38ms/step\n",
            "Epoch 479/1000\n",
            "\n",
            "Epoch 479: val_accuracy did not improve from 0.95080\n",
            "429/429 - 17s - loss: 0.2629 - accuracy: 0.9200 - val_loss: 0.2260 - val_accuracy: 0.9338 - 17s/epoch - 39ms/step\n",
            "Epoch 480/1000\n",
            "\n",
            "Epoch 480: val_accuracy did not improve from 0.95080\n",
            "429/429 - 17s - loss: 0.2636 - accuracy: 0.9181 - val_loss: 0.2105 - val_accuracy: 0.9376 - 17s/epoch - 39ms/step\n",
            "Epoch 481/1000\n",
            "\n",
            "Epoch 481: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2613 - accuracy: 0.9182 - val_loss: 0.2199 - val_accuracy: 0.9350 - 16s/epoch - 38ms/step\n",
            "Epoch 482/1000\n",
            "\n",
            "Epoch 482: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2659 - accuracy: 0.9185 - val_loss: 0.1845 - val_accuracy: 0.9480 - 16s/epoch - 37ms/step\n",
            "Epoch 483/1000\n",
            "\n",
            "Epoch 483: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2657 - accuracy: 0.9177 - val_loss: 0.2088 - val_accuracy: 0.9370 - 16s/epoch - 37ms/step\n",
            "Epoch 484/1000\n",
            "\n",
            "Epoch 484: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2642 - accuracy: 0.9179 - val_loss: 0.1920 - val_accuracy: 0.9410 - 16s/epoch - 38ms/step\n",
            "Epoch 485/1000\n",
            "\n",
            "Epoch 485: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2634 - accuracy: 0.9195 - val_loss: 0.2035 - val_accuracy: 0.9400 - 16s/epoch - 37ms/step\n",
            "Epoch 486/1000\n",
            "\n",
            "Epoch 486: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2656 - accuracy: 0.9181 - val_loss: 0.2150 - val_accuracy: 0.9362 - 16s/epoch - 37ms/step\n",
            "Epoch 487/1000\n",
            "\n",
            "Epoch 487: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2666 - accuracy: 0.9176 - val_loss: 0.2085 - val_accuracy: 0.9398 - 16s/epoch - 37ms/step\n",
            "Epoch 488/1000\n",
            "\n",
            "Epoch 488: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2634 - accuracy: 0.9183 - val_loss: 0.1986 - val_accuracy: 0.9382 - 16s/epoch - 38ms/step\n",
            "Epoch 489/1000\n",
            "\n",
            "Epoch 489: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2606 - accuracy: 0.9191 - val_loss: 0.2245 - val_accuracy: 0.9334 - 16s/epoch - 38ms/step\n",
            "Epoch 490/1000\n",
            "\n",
            "Epoch 490: val_accuracy did not improve from 0.95080\n",
            "429/429 - 17s - loss: 0.2613 - accuracy: 0.9185 - val_loss: 0.2471 - val_accuracy: 0.9276 - 17s/epoch - 40ms/step\n",
            "Epoch 491/1000\n",
            "\n",
            "Epoch 491: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2648 - accuracy: 0.9193 - val_loss: 0.1942 - val_accuracy: 0.9450 - 16s/epoch - 38ms/step\n",
            "Epoch 492/1000\n",
            "\n",
            "Epoch 492: val_accuracy did not improve from 0.95080\n",
            "429/429 - 17s - loss: 0.2665 - accuracy: 0.9177 - val_loss: 0.2041 - val_accuracy: 0.9398 - 17s/epoch - 39ms/step\n",
            "Epoch 493/1000\n",
            "\n",
            "Epoch 493: val_accuracy did not improve from 0.95080\n",
            "429/429 - 20s - loss: 0.2651 - accuracy: 0.9184 - val_loss: 0.1890 - val_accuracy: 0.9430 - 20s/epoch - 46ms/step\n",
            "Epoch 494/1000\n",
            "\n",
            "Epoch 494: val_accuracy did not improve from 0.95080\n",
            "429/429 - 17s - loss: 0.2643 - accuracy: 0.9190 - val_loss: 0.2070 - val_accuracy: 0.9364 - 17s/epoch - 40ms/step\n",
            "Epoch 495/1000\n",
            "\n",
            "Epoch 495: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2611 - accuracy: 0.9192 - val_loss: 0.1895 - val_accuracy: 0.9424 - 16s/epoch - 37ms/step\n",
            "Epoch 496/1000\n",
            "\n",
            "Epoch 496: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2666 - accuracy: 0.9173 - val_loss: 0.1945 - val_accuracy: 0.9438 - 16s/epoch - 37ms/step\n",
            "Epoch 497/1000\n",
            "\n",
            "Epoch 497: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2660 - accuracy: 0.9179 - val_loss: 0.1931 - val_accuracy: 0.9436 - 16s/epoch - 37ms/step\n",
            "Epoch 498/1000\n",
            "\n",
            "Epoch 498: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2655 - accuracy: 0.9183 - val_loss: 0.1979 - val_accuracy: 0.9418 - 16s/epoch - 37ms/step\n",
            "Epoch 499/1000\n",
            "\n",
            "Epoch 499: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2647 - accuracy: 0.9189 - val_loss: 0.2082 - val_accuracy: 0.9394 - 16s/epoch - 37ms/step\n",
            "Epoch 500/1000\n",
            "\n",
            "Epoch 500: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2670 - accuracy: 0.9176 - val_loss: 0.2208 - val_accuracy: 0.9352 - 16s/epoch - 37ms/step\n",
            "Epoch 501/1000\n",
            "\n",
            "Epoch 501: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2630 - accuracy: 0.9170 - val_loss: 0.1949 - val_accuracy: 0.9406 - 16s/epoch - 38ms/step\n",
            "Epoch 502/1000\n",
            "\n",
            "Epoch 502: val_accuracy did not improve from 0.95080\n",
            "429/429 - 17s - loss: 0.2667 - accuracy: 0.9172 - val_loss: 0.2030 - val_accuracy: 0.9364 - 17s/epoch - 39ms/step\n",
            "Epoch 503/1000\n",
            "\n",
            "Epoch 503: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2673 - accuracy: 0.9167 - val_loss: 0.1900 - val_accuracy: 0.9466 - 16s/epoch - 38ms/step\n",
            "Epoch 504/1000\n",
            "\n",
            "Epoch 504: val_accuracy did not improve from 0.95080\n",
            "429/429 - 17s - loss: 0.2633 - accuracy: 0.9185 - val_loss: 0.1865 - val_accuracy: 0.9454 - 17s/epoch - 40ms/step\n",
            "Epoch 505/1000\n",
            "\n",
            "Epoch 505: val_accuracy did not improve from 0.95080\n",
            "429/429 - 17s - loss: 0.2697 - accuracy: 0.9175 - val_loss: 0.2030 - val_accuracy: 0.9384 - 17s/epoch - 40ms/step\n",
            "Epoch 506/1000\n",
            "\n",
            "Epoch 506: val_accuracy did not improve from 0.95080\n",
            "429/429 - 21s - loss: 0.2628 - accuracy: 0.9198 - val_loss: 0.2075 - val_accuracy: 0.9370 - 21s/epoch - 48ms/step\n",
            "Epoch 507/1000\n",
            "\n",
            "Epoch 507: val_accuracy did not improve from 0.95080\n",
            "429/429 - 17s - loss: 0.2639 - accuracy: 0.9176 - val_loss: 0.2093 - val_accuracy: 0.9378 - 17s/epoch - 40ms/step\n",
            "Epoch 508/1000\n",
            "\n",
            "Epoch 508: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2653 - accuracy: 0.9169 - val_loss: 0.1955 - val_accuracy: 0.9456 - 16s/epoch - 38ms/step\n",
            "Epoch 509/1000\n",
            "\n",
            "Epoch 509: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2626 - accuracy: 0.9183 - val_loss: 0.1990 - val_accuracy: 0.9402 - 16s/epoch - 37ms/step\n",
            "Epoch 510/1000\n",
            "\n",
            "Epoch 510: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2640 - accuracy: 0.9172 - val_loss: 0.1870 - val_accuracy: 0.9476 - 16s/epoch - 38ms/step\n",
            "Epoch 511/1000\n",
            "\n",
            "Epoch 511: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2641 - accuracy: 0.9199 - val_loss: 0.1873 - val_accuracy: 0.9444 - 16s/epoch - 37ms/step\n",
            "Epoch 512/1000\n",
            "\n",
            "Epoch 512: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2645 - accuracy: 0.9186 - val_loss: 0.2090 - val_accuracy: 0.9368 - 16s/epoch - 36ms/step\n",
            "Epoch 513/1000\n",
            "\n",
            "Epoch 513: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2634 - accuracy: 0.9188 - val_loss: 0.1965 - val_accuracy: 0.9400 - 16s/epoch - 37ms/step\n",
            "Epoch 514/1000\n",
            "\n",
            "Epoch 514: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2635 - accuracy: 0.9201 - val_loss: 0.1872 - val_accuracy: 0.9430 - 16s/epoch - 37ms/step\n",
            "Epoch 515/1000\n",
            "\n",
            "Epoch 515: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2592 - accuracy: 0.9205 - val_loss: 0.2053 - val_accuracy: 0.9388 - 16s/epoch - 37ms/step\n",
            "Epoch 516/1000\n",
            "\n",
            "Epoch 516: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2640 - accuracy: 0.9192 - val_loss: 0.1951 - val_accuracy: 0.9426 - 16s/epoch - 37ms/step\n",
            "Epoch 517/1000\n",
            "\n",
            "Epoch 517: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2628 - accuracy: 0.9211 - val_loss: 0.2101 - val_accuracy: 0.9392 - 16s/epoch - 38ms/step\n",
            "Epoch 518/1000\n",
            "\n",
            "Epoch 518: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2625 - accuracy: 0.9189 - val_loss: 0.1978 - val_accuracy: 0.9394 - 16s/epoch - 38ms/step\n",
            "Epoch 519/1000\n",
            "\n",
            "Epoch 519: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2666 - accuracy: 0.9169 - val_loss: 0.1935 - val_accuracy: 0.9414 - 16s/epoch - 38ms/step\n",
            "Epoch 520/1000\n",
            "\n",
            "Epoch 520: val_accuracy did not improve from 0.95080\n",
            "429/429 - 17s - loss: 0.2637 - accuracy: 0.9186 - val_loss: 0.2199 - val_accuracy: 0.9338 - 17s/epoch - 40ms/step\n",
            "Epoch 521/1000\n",
            "\n",
            "Epoch 521: val_accuracy did not improve from 0.95080\n",
            "429/429 - 17s - loss: 0.2636 - accuracy: 0.9185 - val_loss: 0.2009 - val_accuracy: 0.9396 - 17s/epoch - 39ms/step\n",
            "Epoch 522/1000\n",
            "\n",
            "Epoch 522: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2635 - accuracy: 0.9178 - val_loss: 0.2000 - val_accuracy: 0.9412 - 16s/epoch - 38ms/step\n",
            "Epoch 523/1000\n",
            "\n",
            "Epoch 523: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2642 - accuracy: 0.9176 - val_loss: 0.2104 - val_accuracy: 0.9360 - 16s/epoch - 37ms/step\n",
            "Epoch 524/1000\n",
            "\n",
            "Epoch 524: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2672 - accuracy: 0.9159 - val_loss: 0.2151 - val_accuracy: 0.9362 - 16s/epoch - 37ms/step\n",
            "Epoch 525/1000\n",
            "\n",
            "Epoch 525: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2648 - accuracy: 0.9189 - val_loss: 0.1886 - val_accuracy: 0.9436 - 16s/epoch - 37ms/step\n",
            "Epoch 526/1000\n",
            "\n",
            "Epoch 526: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2628 - accuracy: 0.9180 - val_loss: 0.2309 - val_accuracy: 0.9304 - 16s/epoch - 37ms/step\n",
            "Epoch 527/1000\n",
            "\n",
            "Epoch 527: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2656 - accuracy: 0.9184 - val_loss: 0.2220 - val_accuracy: 0.9382 - 16s/epoch - 37ms/step\n",
            "Epoch 528/1000\n",
            "\n",
            "Epoch 528: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2637 - accuracy: 0.9181 - val_loss: 0.2057 - val_accuracy: 0.9404 - 16s/epoch - 37ms/step\n",
            "Epoch 529/1000\n",
            "\n",
            "Epoch 529: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2613 - accuracy: 0.9184 - val_loss: 0.2375 - val_accuracy: 0.9294 - 16s/epoch - 37ms/step\n",
            "Epoch 530/1000\n",
            "\n",
            "Epoch 530: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2656 - accuracy: 0.9181 - val_loss: 0.2142 - val_accuracy: 0.9324 - 16s/epoch - 37ms/step\n",
            "Epoch 531/1000\n",
            "\n",
            "Epoch 531: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2625 - accuracy: 0.9203 - val_loss: 0.2084 - val_accuracy: 0.9404 - 16s/epoch - 37ms/step\n",
            "Epoch 532/1000\n",
            "\n",
            "Epoch 532: val_accuracy did not improve from 0.95080\n",
            "429/429 - 17s - loss: 0.2652 - accuracy: 0.9182 - val_loss: 0.2247 - val_accuracy: 0.9314 - 17s/epoch - 39ms/step\n",
            "Epoch 533/1000\n",
            "\n",
            "Epoch 533: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2642 - accuracy: 0.9188 - val_loss: 0.2107 - val_accuracy: 0.9324 - 16s/epoch - 38ms/step\n",
            "Epoch 534/1000\n",
            "\n",
            "Epoch 534: val_accuracy did not improve from 0.95080\n",
            "429/429 - 17s - loss: 0.2639 - accuracy: 0.9180 - val_loss: 0.1999 - val_accuracy: 0.9422 - 17s/epoch - 39ms/step\n",
            "Epoch 535/1000\n",
            "\n",
            "Epoch 535: val_accuracy did not improve from 0.95080\n",
            "429/429 - 17s - loss: 0.2658 - accuracy: 0.9171 - val_loss: 0.2106 - val_accuracy: 0.9342 - 17s/epoch - 39ms/step\n",
            "Epoch 536/1000\n",
            "\n",
            "Epoch 536: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2627 - accuracy: 0.9196 - val_loss: 0.1958 - val_accuracy: 0.9434 - 16s/epoch - 38ms/step\n",
            "Epoch 537/1000\n",
            "\n",
            "Epoch 537: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2677 - accuracy: 0.9180 - val_loss: 0.1890 - val_accuracy: 0.9446 - 16s/epoch - 37ms/step\n",
            "Epoch 538/1000\n",
            "\n",
            "Epoch 538: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2628 - accuracy: 0.9191 - val_loss: 0.1964 - val_accuracy: 0.9406 - 16s/epoch - 37ms/step\n",
            "Epoch 539/1000\n",
            "\n",
            "Epoch 539: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2629 - accuracy: 0.9185 - val_loss: 0.1841 - val_accuracy: 0.9484 - 16s/epoch - 37ms/step\n",
            "Epoch 540/1000\n",
            "\n",
            "Epoch 540: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2678 - accuracy: 0.9168 - val_loss: 0.2102 - val_accuracy: 0.9390 - 16s/epoch - 38ms/step\n",
            "Epoch 541/1000\n",
            "\n",
            "Epoch 541: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2615 - accuracy: 0.9194 - val_loss: 0.1870 - val_accuracy: 0.9472 - 16s/epoch - 37ms/step\n",
            "Epoch 542/1000\n",
            "\n",
            "Epoch 542: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2643 - accuracy: 0.9176 - val_loss: 0.1923 - val_accuracy: 0.9414 - 16s/epoch - 37ms/step\n",
            "Epoch 543/1000\n",
            "\n",
            "Epoch 543: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2643 - accuracy: 0.9196 - val_loss: 0.1869 - val_accuracy: 0.9424 - 16s/epoch - 37ms/step\n",
            "Epoch 544/1000\n",
            "\n",
            "Epoch 544: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2646 - accuracy: 0.9192 - val_loss: 0.2144 - val_accuracy: 0.9302 - 16s/epoch - 38ms/step\n",
            "Epoch 545/1000\n",
            "\n",
            "Epoch 545: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2666 - accuracy: 0.9180 - val_loss: 0.2011 - val_accuracy: 0.9370 - 16s/epoch - 37ms/step\n",
            "Epoch 546/1000\n",
            "\n",
            "Epoch 546: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2627 - accuracy: 0.9191 - val_loss: 0.1985 - val_accuracy: 0.9388 - 16s/epoch - 38ms/step\n",
            "Epoch 547/1000\n",
            "\n",
            "Epoch 547: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2643 - accuracy: 0.9180 - val_loss: 0.1849 - val_accuracy: 0.9486 - 16s/epoch - 38ms/step\n",
            "Epoch 548/1000\n",
            "\n",
            "Epoch 548: val_accuracy did not improve from 0.95080\n",
            "429/429 - 17s - loss: 0.2613 - accuracy: 0.9192 - val_loss: 0.2110 - val_accuracy: 0.9352 - 17s/epoch - 41ms/step\n",
            "Epoch 549/1000\n",
            "\n",
            "Epoch 549: val_accuracy did not improve from 0.95080\n",
            "429/429 - 17s - loss: 0.2641 - accuracy: 0.9182 - val_loss: 0.2034 - val_accuracy: 0.9392 - 17s/epoch - 39ms/step\n",
            "Epoch 550/1000\n",
            "\n",
            "Epoch 550: val_accuracy did not improve from 0.95080\n",
            "429/429 - 17s - loss: 0.2632 - accuracy: 0.9177 - val_loss: 0.2552 - val_accuracy: 0.9224 - 17s/epoch - 39ms/step\n",
            "Epoch 551/1000\n",
            "\n",
            "Epoch 551: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2612 - accuracy: 0.9198 - val_loss: 0.2427 - val_accuracy: 0.9282 - 16s/epoch - 37ms/step\n",
            "Epoch 552/1000\n",
            "\n",
            "Epoch 552: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2633 - accuracy: 0.9174 - val_loss: 0.2045 - val_accuracy: 0.9420 - 16s/epoch - 37ms/step\n",
            "Epoch 553/1000\n",
            "\n",
            "Epoch 553: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2653 - accuracy: 0.9172 - val_loss: 0.2001 - val_accuracy: 0.9348 - 16s/epoch - 36ms/step\n",
            "Epoch 554/1000\n",
            "\n",
            "Epoch 554: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2639 - accuracy: 0.9197 - val_loss: 0.2079 - val_accuracy: 0.9396 - 16s/epoch - 37ms/step\n",
            "Epoch 555/1000\n",
            "\n",
            "Epoch 555: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2638 - accuracy: 0.9173 - val_loss: 0.2133 - val_accuracy: 0.9340 - 16s/epoch - 38ms/step\n",
            "Epoch 556/1000\n",
            "\n",
            "Epoch 556: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2650 - accuracy: 0.9174 - val_loss: 0.1928 - val_accuracy: 0.9428 - 16s/epoch - 37ms/step\n",
            "Epoch 557/1000\n",
            "\n",
            "Epoch 557: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2627 - accuracy: 0.9192 - val_loss: 0.2188 - val_accuracy: 0.9322 - 16s/epoch - 37ms/step\n",
            "Epoch 558/1000\n",
            "\n",
            "Epoch 558: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2636 - accuracy: 0.9193 - val_loss: 0.2042 - val_accuracy: 0.9396 - 16s/epoch - 38ms/step\n",
            "Epoch 559/1000\n",
            "\n",
            "Epoch 559: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2661 - accuracy: 0.9196 - val_loss: 0.2052 - val_accuracy: 0.9414 - 16s/epoch - 38ms/step\n",
            "Epoch 560/1000\n",
            "\n",
            "Epoch 560: val_accuracy did not improve from 0.95080\n",
            "429/429 - 17s - loss: 0.2637 - accuracy: 0.9183 - val_loss: 0.2017 - val_accuracy: 0.9414 - 17s/epoch - 39ms/step\n",
            "Epoch 561/1000\n",
            "\n",
            "Epoch 561: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2653 - accuracy: 0.9182 - val_loss: 0.1970 - val_accuracy: 0.9436 - 16s/epoch - 38ms/step\n",
            "Epoch 562/1000\n",
            "\n",
            "Epoch 562: val_accuracy did not improve from 0.95080\n",
            "429/429 - 19s - loss: 0.2627 - accuracy: 0.9180 - val_loss: 0.2249 - val_accuracy: 0.9308 - 19s/epoch - 45ms/step\n",
            "Epoch 563/1000\n",
            "\n",
            "Epoch 563: val_accuracy did not improve from 0.95080\n",
            "429/429 - 17s - loss: 0.2660 - accuracy: 0.9174 - val_loss: 0.2046 - val_accuracy: 0.9408 - 17s/epoch - 40ms/step\n",
            "Epoch 564/1000\n",
            "\n",
            "Epoch 564: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2632 - accuracy: 0.9191 - val_loss: 0.2072 - val_accuracy: 0.9398 - 16s/epoch - 38ms/step\n",
            "Epoch 565/1000\n",
            "\n",
            "Epoch 565: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2650 - accuracy: 0.9162 - val_loss: 0.2093 - val_accuracy: 0.9352 - 16s/epoch - 38ms/step\n",
            "Epoch 566/1000\n",
            "\n",
            "Epoch 566: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2640 - accuracy: 0.9192 - val_loss: 0.1810 - val_accuracy: 0.9468 - 16s/epoch - 36ms/step\n",
            "Epoch 567/1000\n",
            "\n",
            "Epoch 567: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2643 - accuracy: 0.9196 - val_loss: 0.2053 - val_accuracy: 0.9414 - 16s/epoch - 37ms/step\n",
            "Epoch 568/1000\n",
            "\n",
            "Epoch 568: val_accuracy did not improve from 0.95080\n",
            "429/429 - 15s - loss: 0.2650 - accuracy: 0.9174 - val_loss: 0.2227 - val_accuracy: 0.9350 - 15s/epoch - 36ms/step\n",
            "Epoch 569/1000\n",
            "\n",
            "Epoch 569: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2583 - accuracy: 0.9207 - val_loss: 0.2043 - val_accuracy: 0.9430 - 16s/epoch - 37ms/step\n",
            "Epoch 570/1000\n",
            "\n",
            "Epoch 570: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2642 - accuracy: 0.9187 - val_loss: 0.2051 - val_accuracy: 0.9362 - 16s/epoch - 37ms/step\n",
            "Epoch 571/1000\n",
            "\n",
            "Epoch 571: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2649 - accuracy: 0.9175 - val_loss: 0.2012 - val_accuracy: 0.9390 - 16s/epoch - 37ms/step\n",
            "Epoch 572/1000\n",
            "\n",
            "Epoch 572: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2691 - accuracy: 0.9175 - val_loss: 0.2351 - val_accuracy: 0.9282 - 16s/epoch - 37ms/step\n",
            "Epoch 573/1000\n",
            "\n",
            "Epoch 573: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2629 - accuracy: 0.9190 - val_loss: 0.2062 - val_accuracy: 0.9378 - 16s/epoch - 38ms/step\n",
            "Epoch 574/1000\n",
            "\n",
            "Epoch 574: val_accuracy did not improve from 0.95080\n",
            "429/429 - 24s - loss: 0.2639 - accuracy: 0.9177 - val_loss: 0.1864 - val_accuracy: 0.9450 - 24s/epoch - 56ms/step\n",
            "Epoch 575/1000\n",
            "\n",
            "Epoch 575: val_accuracy did not improve from 0.95080\n",
            "429/429 - 20s - loss: 0.2612 - accuracy: 0.9200 - val_loss: 0.1931 - val_accuracy: 0.9452 - 20s/epoch - 47ms/step\n",
            "Epoch 576/1000\n",
            "\n",
            "Epoch 576: val_accuracy did not improve from 0.95080\n",
            "429/429 - 17s - loss: 0.2631 - accuracy: 0.9185 - val_loss: 0.2005 - val_accuracy: 0.9378 - 17s/epoch - 39ms/step\n",
            "Epoch 577/1000\n",
            "\n",
            "Epoch 577: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2633 - accuracy: 0.9197 - val_loss: 0.1940 - val_accuracy: 0.9470 - 16s/epoch - 38ms/step\n",
            "Epoch 578/1000\n",
            "\n",
            "Epoch 578: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2636 - accuracy: 0.9180 - val_loss: 0.2058 - val_accuracy: 0.9408 - 16s/epoch - 37ms/step\n",
            "Epoch 579/1000\n",
            "\n",
            "Epoch 579: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2647 - accuracy: 0.9182 - val_loss: 0.1919 - val_accuracy: 0.9430 - 16s/epoch - 37ms/step\n",
            "Epoch 580/1000\n",
            "\n",
            "Epoch 580: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2637 - accuracy: 0.9191 - val_loss: 0.2121 - val_accuracy: 0.9366 - 16s/epoch - 36ms/step\n",
            "Epoch 581/1000\n",
            "\n",
            "Epoch 581: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2657 - accuracy: 0.9163 - val_loss: 0.1881 - val_accuracy: 0.9458 - 16s/epoch - 37ms/step\n",
            "Epoch 582/1000\n",
            "\n",
            "Epoch 582: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2651 - accuracy: 0.9181 - val_loss: 0.2175 - val_accuracy: 0.9366 - 16s/epoch - 37ms/step\n",
            "Epoch 583/1000\n",
            "\n",
            "Epoch 583: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2648 - accuracy: 0.9193 - val_loss: 0.1869 - val_accuracy: 0.9448 - 16s/epoch - 37ms/step\n",
            "Epoch 584/1000\n",
            "\n",
            "Epoch 584: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2638 - accuracy: 0.9190 - val_loss: 0.2057 - val_accuracy: 0.9398 - 16s/epoch - 37ms/step\n",
            "Epoch 585/1000\n",
            "\n",
            "Epoch 585: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2626 - accuracy: 0.9182 - val_loss: 0.2168 - val_accuracy: 0.9344 - 16s/epoch - 37ms/step\n",
            "Epoch 586/1000\n",
            "\n",
            "Epoch 586: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2597 - accuracy: 0.9202 - val_loss: 0.1920 - val_accuracy: 0.9448 - 16s/epoch - 38ms/step\n",
            "Epoch 587/1000\n",
            "\n",
            "Epoch 587: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2606 - accuracy: 0.9196 - val_loss: 0.2088 - val_accuracy: 0.9404 - 16s/epoch - 38ms/step\n",
            "Epoch 588/1000\n",
            "\n",
            "Epoch 588: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2640 - accuracy: 0.9182 - val_loss: 0.1842 - val_accuracy: 0.9486 - 16s/epoch - 38ms/step\n",
            "Epoch 589/1000\n",
            "\n",
            "Epoch 589: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2640 - accuracy: 0.9196 - val_loss: 0.1879 - val_accuracy: 0.9462 - 16s/epoch - 38ms/step\n",
            "Epoch 590/1000\n",
            "\n",
            "Epoch 590: val_accuracy did not improve from 0.95080\n",
            "429/429 - 17s - loss: 0.2641 - accuracy: 0.9187 - val_loss: 0.1921 - val_accuracy: 0.9460 - 17s/epoch - 39ms/step\n",
            "Epoch 591/1000\n",
            "\n",
            "Epoch 591: val_accuracy did not improve from 0.95080\n",
            "429/429 - 19s - loss: 0.2651 - accuracy: 0.9183 - val_loss: 0.2019 - val_accuracy: 0.9430 - 19s/epoch - 44ms/step\n",
            "Epoch 592/1000\n",
            "\n",
            "Epoch 592: val_accuracy did not improve from 0.95080\n",
            "429/429 - 17s - loss: 0.2615 - accuracy: 0.9206 - val_loss: 0.2073 - val_accuracy: 0.9358 - 17s/epoch - 39ms/step\n",
            "Epoch 593/1000\n",
            "\n",
            "Epoch 593: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2667 - accuracy: 0.9166 - val_loss: 0.1973 - val_accuracy: 0.9416 - 16s/epoch - 37ms/step\n",
            "Epoch 594/1000\n",
            "\n",
            "Epoch 594: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2615 - accuracy: 0.9193 - val_loss: 0.2013 - val_accuracy: 0.9386 - 16s/epoch - 37ms/step\n",
            "Epoch 595/1000\n",
            "\n",
            "Epoch 595: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2646 - accuracy: 0.9184 - val_loss: 0.1974 - val_accuracy: 0.9400 - 16s/epoch - 36ms/step\n",
            "Epoch 596/1000\n",
            "\n",
            "Epoch 596: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2636 - accuracy: 0.9172 - val_loss: 0.1915 - val_accuracy: 0.9440 - 16s/epoch - 37ms/step\n",
            "Epoch 597/1000\n",
            "\n",
            "Epoch 597: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2621 - accuracy: 0.9185 - val_loss: 0.2070 - val_accuracy: 0.9418 - 16s/epoch - 38ms/step\n",
            "Epoch 598/1000\n",
            "\n",
            "Epoch 598: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2624 - accuracy: 0.9195 - val_loss: 0.2013 - val_accuracy: 0.9406 - 16s/epoch - 37ms/step\n",
            "Epoch 599/1000\n",
            "\n",
            "Epoch 599: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2628 - accuracy: 0.9187 - val_loss: 0.2218 - val_accuracy: 0.9344 - 16s/epoch - 38ms/step\n",
            "Epoch 600/1000\n",
            "\n",
            "Epoch 600: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2667 - accuracy: 0.9178 - val_loss: 0.1894 - val_accuracy: 0.9498 - 16s/epoch - 38ms/step\n",
            "Epoch 601/1000\n",
            "\n",
            "Epoch 601: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2671 - accuracy: 0.9170 - val_loss: 0.1930 - val_accuracy: 0.9484 - 16s/epoch - 38ms/step\n",
            "Epoch 602/1000\n",
            "\n",
            "Epoch 602: val_accuracy did not improve from 0.95080\n",
            "429/429 - 17s - loss: 0.2643 - accuracy: 0.9200 - val_loss: 0.1845 - val_accuracy: 0.9504 - 17s/epoch - 39ms/step\n",
            "Epoch 603/1000\n",
            "\n",
            "Epoch 603: val_accuracy did not improve from 0.95080\n",
            "429/429 - 17s - loss: 0.2635 - accuracy: 0.9188 - val_loss: 0.1943 - val_accuracy: 0.9440 - 17s/epoch - 39ms/step\n",
            "Epoch 604/1000\n",
            "\n",
            "Epoch 604: val_accuracy did not improve from 0.95080\n",
            "429/429 - 17s - loss: 0.2663 - accuracy: 0.9186 - val_loss: 0.1770 - val_accuracy: 0.9496 - 17s/epoch - 40ms/step\n",
            "Epoch 605/1000\n",
            "\n",
            "Epoch 605: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2632 - accuracy: 0.9198 - val_loss: 0.1904 - val_accuracy: 0.9426 - 16s/epoch - 38ms/step\n",
            "Epoch 606/1000\n",
            "\n",
            "Epoch 606: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2615 - accuracy: 0.9189 - val_loss: 0.1943 - val_accuracy: 0.9460 - 16s/epoch - 37ms/step\n",
            "Epoch 607/1000\n",
            "\n",
            "Epoch 607: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2634 - accuracy: 0.9174 - val_loss: 0.2077 - val_accuracy: 0.9388 - 16s/epoch - 36ms/step\n",
            "Epoch 608/1000\n",
            "\n",
            "Epoch 608: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2634 - accuracy: 0.9182 - val_loss: 0.2088 - val_accuracy: 0.9348 - 16s/epoch - 36ms/step\n",
            "Epoch 609/1000\n",
            "\n",
            "Epoch 609: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2618 - accuracy: 0.9197 - val_loss: 0.1936 - val_accuracy: 0.9418 - 16s/epoch - 37ms/step\n",
            "Epoch 610/1000\n",
            "\n",
            "Epoch 610: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2598 - accuracy: 0.9190 - val_loss: 0.2012 - val_accuracy: 0.9370 - 16s/epoch - 36ms/step\n",
            "Epoch 611/1000\n",
            "\n",
            "Epoch 611: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2612 - accuracy: 0.9194 - val_loss: 0.2491 - val_accuracy: 0.9256 - 16s/epoch - 37ms/step\n",
            "Epoch 612/1000\n",
            "\n",
            "Epoch 612: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2620 - accuracy: 0.9200 - val_loss: 0.2013 - val_accuracy: 0.9428 - 16s/epoch - 38ms/step\n",
            "Epoch 613/1000\n",
            "\n",
            "Epoch 613: val_accuracy did not improve from 0.95080\n",
            "429/429 - 17s - loss: 0.2621 - accuracy: 0.9185 - val_loss: 0.1921 - val_accuracy: 0.9434 - 17s/epoch - 40ms/step\n",
            "Epoch 614/1000\n",
            "\n",
            "Epoch 614: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2631 - accuracy: 0.9187 - val_loss: 0.2145 - val_accuracy: 0.9366 - 16s/epoch - 38ms/step\n",
            "Epoch 615/1000\n",
            "\n",
            "Epoch 615: val_accuracy did not improve from 0.95080\n",
            "429/429 - 17s - loss: 0.2620 - accuracy: 0.9197 - val_loss: 0.1943 - val_accuracy: 0.9410 - 17s/epoch - 39ms/step\n",
            "Epoch 616/1000\n",
            "\n",
            "Epoch 616: val_accuracy did not improve from 0.95080\n",
            "429/429 - 17s - loss: 0.2626 - accuracy: 0.9180 - val_loss: 0.2065 - val_accuracy: 0.9358 - 17s/epoch - 39ms/step\n",
            "Epoch 617/1000\n",
            "\n",
            "Epoch 617: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2661 - accuracy: 0.9192 - val_loss: 0.1974 - val_accuracy: 0.9376 - 16s/epoch - 38ms/step\n",
            "Epoch 618/1000\n",
            "\n",
            "Epoch 618: val_accuracy did not improve from 0.95080\n",
            "429/429 - 17s - loss: 0.2672 - accuracy: 0.9162 - val_loss: 0.1916 - val_accuracy: 0.9436 - 17s/epoch - 40ms/step\n",
            "Epoch 619/1000\n",
            "\n",
            "Epoch 619: val_accuracy did not improve from 0.95080\n",
            "429/429 - 18s - loss: 0.2623 - accuracy: 0.9188 - val_loss: 0.1904 - val_accuracy: 0.9484 - 18s/epoch - 43ms/step\n",
            "Epoch 620/1000\n",
            "\n",
            "Epoch 620: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2614 - accuracy: 0.9187 - val_loss: 0.2013 - val_accuracy: 0.9432 - 16s/epoch - 37ms/step\n",
            "Epoch 621/1000\n",
            "\n",
            "Epoch 621: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2599 - accuracy: 0.9205 - val_loss: 0.2055 - val_accuracy: 0.9390 - 16s/epoch - 38ms/step\n",
            "Epoch 622/1000\n",
            "\n",
            "Epoch 622: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2658 - accuracy: 0.9176 - val_loss: 0.2088 - val_accuracy: 0.9390 - 16s/epoch - 37ms/step\n",
            "Epoch 623/1000\n",
            "\n",
            "Epoch 623: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2638 - accuracy: 0.9188 - val_loss: 0.2144 - val_accuracy: 0.9384 - 16s/epoch - 37ms/step\n",
            "Epoch 624/1000\n",
            "\n",
            "Epoch 624: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2658 - accuracy: 0.9180 - val_loss: 0.1946 - val_accuracy: 0.9438 - 16s/epoch - 37ms/step\n",
            "Epoch 625/1000\n",
            "\n",
            "Epoch 625: val_accuracy did not improve from 0.95080\n",
            "429/429 - 16s - loss: 0.2633 - accuracy: 0.9175 - val_loss: 0.1887 - val_accuracy: 0.9450 - 16s/epoch - 37ms/step\n",
            "Epoch 626/1000\n",
            "\n",
            "Epoch 626: val_accuracy improved from 0.95080 to 0.95140, saving model to best_model,keras\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "429/429 - 17s - loss: 0.2643 - accuracy: 0.9193 - val_loss: 0.1841 - val_accuracy: 0.9514 - 17s/epoch - 41ms/step\n",
            "Epoch 627/1000\n",
            "\n",
            "Epoch 627: val_accuracy did not improve from 0.95140\n",
            "429/429 - 17s - loss: 0.2618 - accuracy: 0.9198 - val_loss: 0.2068 - val_accuracy: 0.9340 - 17s/epoch - 39ms/step\n",
            "Epoch 628/1000\n",
            "\n",
            "Epoch 628: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2631 - accuracy: 0.9195 - val_loss: 0.2043 - val_accuracy: 0.9420 - 16s/epoch - 38ms/step\n",
            "Epoch 629/1000\n",
            "\n",
            "Epoch 629: val_accuracy did not improve from 0.95140\n",
            "429/429 - 17s - loss: 0.2628 - accuracy: 0.9176 - val_loss: 0.2020 - val_accuracy: 0.9410 - 17s/epoch - 40ms/step\n",
            "Epoch 630/1000\n",
            "\n",
            "Epoch 630: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2625 - accuracy: 0.9203 - val_loss: 0.2029 - val_accuracy: 0.9426 - 16s/epoch - 38ms/step\n",
            "Epoch 631/1000\n",
            "\n",
            "Epoch 631: val_accuracy did not improve from 0.95140\n",
            "429/429 - 17s - loss: 0.2659 - accuracy: 0.9186 - val_loss: 0.2139 - val_accuracy: 0.9366 - 17s/epoch - 40ms/step\n",
            "Epoch 632/1000\n",
            "\n",
            "Epoch 632: val_accuracy did not improve from 0.95140\n",
            "429/429 - 17s - loss: 0.2625 - accuracy: 0.9194 - val_loss: 0.2234 - val_accuracy: 0.9314 - 17s/epoch - 41ms/step\n",
            "Epoch 633/1000\n",
            "\n",
            "Epoch 633: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2622 - accuracy: 0.9175 - val_loss: 0.2060 - val_accuracy: 0.9378 - 16s/epoch - 38ms/step\n",
            "Epoch 634/1000\n",
            "\n",
            "Epoch 634: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2616 - accuracy: 0.9195 - val_loss: 0.2033 - val_accuracy: 0.9376 - 16s/epoch - 38ms/step\n",
            "Epoch 635/1000\n",
            "\n",
            "Epoch 635: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2653 - accuracy: 0.9173 - val_loss: 0.1953 - val_accuracy: 0.9422 - 16s/epoch - 37ms/step\n",
            "Epoch 636/1000\n",
            "\n",
            "Epoch 636: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2612 - accuracy: 0.9195 - val_loss: 0.2051 - val_accuracy: 0.9382 - 16s/epoch - 37ms/step\n",
            "Epoch 637/1000\n",
            "\n",
            "Epoch 637: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2659 - accuracy: 0.9184 - val_loss: 0.1877 - val_accuracy: 0.9474 - 16s/epoch - 37ms/step\n",
            "Epoch 638/1000\n",
            "\n",
            "Epoch 638: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2667 - accuracy: 0.9196 - val_loss: 0.2386 - val_accuracy: 0.9292 - 16s/epoch - 37ms/step\n",
            "Epoch 639/1000\n",
            "\n",
            "Epoch 639: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2648 - accuracy: 0.9194 - val_loss: 0.2216 - val_accuracy: 0.9336 - 16s/epoch - 37ms/step\n",
            "Epoch 640/1000\n",
            "\n",
            "Epoch 640: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2613 - accuracy: 0.9191 - val_loss: 0.2127 - val_accuracy: 0.9394 - 16s/epoch - 37ms/step\n",
            "Epoch 641/1000\n",
            "\n",
            "Epoch 641: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2636 - accuracy: 0.9183 - val_loss: 0.1961 - val_accuracy: 0.9432 - 16s/epoch - 38ms/step\n",
            "Epoch 642/1000\n",
            "\n",
            "Epoch 642: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2648 - accuracy: 0.9178 - val_loss: 0.2234 - val_accuracy: 0.9350 - 16s/epoch - 38ms/step\n",
            "Epoch 643/1000\n",
            "\n",
            "Epoch 643: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2655 - accuracy: 0.9190 - val_loss: 0.1985 - val_accuracy: 0.9402 - 16s/epoch - 38ms/step\n",
            "Epoch 644/1000\n",
            "\n",
            "Epoch 644: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2632 - accuracy: 0.9173 - val_loss: 0.1901 - val_accuracy: 0.9408 - 16s/epoch - 38ms/step\n",
            "Epoch 645/1000\n",
            "\n",
            "Epoch 645: val_accuracy did not improve from 0.95140\n",
            "429/429 - 17s - loss: 0.2651 - accuracy: 0.9178 - val_loss: 0.1917 - val_accuracy: 0.9438 - 17s/epoch - 39ms/step\n",
            "Epoch 646/1000\n",
            "\n",
            "Epoch 646: val_accuracy did not improve from 0.95140\n",
            "429/429 - 17s - loss: 0.2616 - accuracy: 0.9198 - val_loss: 0.2012 - val_accuracy: 0.9408 - 17s/epoch - 40ms/step\n",
            "Epoch 647/1000\n",
            "\n",
            "Epoch 647: val_accuracy did not improve from 0.95140\n",
            "429/429 - 17s - loss: 0.2640 - accuracy: 0.9191 - val_loss: 0.2098 - val_accuracy: 0.9396 - 17s/epoch - 39ms/step\n",
            "Epoch 648/1000\n",
            "\n",
            "Epoch 648: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2614 - accuracy: 0.9178 - val_loss: 0.1995 - val_accuracy: 0.9416 - 16s/epoch - 38ms/step\n",
            "Epoch 649/1000\n",
            "\n",
            "Epoch 649: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2621 - accuracy: 0.9189 - val_loss: 0.1980 - val_accuracy: 0.9416 - 16s/epoch - 37ms/step\n",
            "Epoch 650/1000\n",
            "\n",
            "Epoch 650: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2605 - accuracy: 0.9184 - val_loss: 0.2024 - val_accuracy: 0.9364 - 16s/epoch - 37ms/step\n",
            "Epoch 651/1000\n",
            "\n",
            "Epoch 651: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2656 - accuracy: 0.9191 - val_loss: 0.1875 - val_accuracy: 0.9422 - 16s/epoch - 37ms/step\n",
            "Epoch 652/1000\n",
            "\n",
            "Epoch 652: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2658 - accuracy: 0.9186 - val_loss: 0.2276 - val_accuracy: 0.9286 - 16s/epoch - 36ms/step\n",
            "Epoch 653/1000\n",
            "\n",
            "Epoch 653: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2649 - accuracy: 0.9183 - val_loss: 0.1990 - val_accuracy: 0.9372 - 16s/epoch - 38ms/step\n",
            "Epoch 654/1000\n",
            "\n",
            "Epoch 654: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2623 - accuracy: 0.9190 - val_loss: 0.1985 - val_accuracy: 0.9430 - 16s/epoch - 37ms/step\n",
            "Epoch 655/1000\n",
            "\n",
            "Epoch 655: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2614 - accuracy: 0.9182 - val_loss: 0.1966 - val_accuracy: 0.9386 - 16s/epoch - 37ms/step\n",
            "Epoch 656/1000\n",
            "\n",
            "Epoch 656: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2644 - accuracy: 0.9179 - val_loss: 0.1934 - val_accuracy: 0.9410 - 16s/epoch - 38ms/step\n",
            "Epoch 657/1000\n",
            "\n",
            "Epoch 657: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2632 - accuracy: 0.9185 - val_loss: 0.2026 - val_accuracy: 0.9384 - 16s/epoch - 38ms/step\n",
            "Epoch 658/1000\n",
            "\n",
            "Epoch 658: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2621 - accuracy: 0.9198 - val_loss: 0.2136 - val_accuracy: 0.9368 - 16s/epoch - 38ms/step\n",
            "Epoch 659/1000\n",
            "\n",
            "Epoch 659: val_accuracy did not improve from 0.95140\n",
            "429/429 - 17s - loss: 0.2647 - accuracy: 0.9170 - val_loss: 0.1829 - val_accuracy: 0.9474 - 17s/epoch - 41ms/step\n",
            "Epoch 660/1000\n",
            "\n",
            "Epoch 660: val_accuracy did not improve from 0.95140\n",
            "429/429 - 17s - loss: 0.2639 - accuracy: 0.9184 - val_loss: 0.1949 - val_accuracy: 0.9386 - 17s/epoch - 40ms/step\n",
            "Epoch 661/1000\n",
            "\n",
            "Epoch 661: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2599 - accuracy: 0.9204 - val_loss: 0.2074 - val_accuracy: 0.9432 - 16s/epoch - 38ms/step\n",
            "Epoch 662/1000\n",
            "\n",
            "Epoch 662: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2620 - accuracy: 0.9190 - val_loss: 0.2027 - val_accuracy: 0.9390 - 16s/epoch - 37ms/step\n",
            "Epoch 663/1000\n",
            "\n",
            "Epoch 663: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2669 - accuracy: 0.9172 - val_loss: 0.1864 - val_accuracy: 0.9500 - 16s/epoch - 37ms/step\n",
            "Epoch 664/1000\n",
            "\n",
            "Epoch 664: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2654 - accuracy: 0.9176 - val_loss: 0.1803 - val_accuracy: 0.9458 - 16s/epoch - 37ms/step\n",
            "Epoch 665/1000\n",
            "\n",
            "Epoch 665: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2641 - accuracy: 0.9196 - val_loss: 0.2018 - val_accuracy: 0.9400 - 16s/epoch - 37ms/step\n",
            "Epoch 666/1000\n",
            "\n",
            "Epoch 666: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2671 - accuracy: 0.9178 - val_loss: 0.1903 - val_accuracy: 0.9412 - 16s/epoch - 37ms/step\n",
            "Epoch 667/1000\n",
            "\n",
            "Epoch 667: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2626 - accuracy: 0.9198 - val_loss: 0.1863 - val_accuracy: 0.9458 - 16s/epoch - 37ms/step\n",
            "Epoch 668/1000\n",
            "\n",
            "Epoch 668: val_accuracy did not improve from 0.95140\n",
            "429/429 - 17s - loss: 0.2648 - accuracy: 0.9179 - val_loss: 0.2047 - val_accuracy: 0.9408 - 17s/epoch - 39ms/step\n",
            "Epoch 669/1000\n",
            "\n",
            "Epoch 669: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2628 - accuracy: 0.9196 - val_loss: 0.2000 - val_accuracy: 0.9414 - 16s/epoch - 38ms/step\n",
            "Epoch 670/1000\n",
            "\n",
            "Epoch 670: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2629 - accuracy: 0.9198 - val_loss: 0.2049 - val_accuracy: 0.9388 - 16s/epoch - 38ms/step\n",
            "Epoch 671/1000\n",
            "\n",
            "Epoch 671: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2666 - accuracy: 0.9179 - val_loss: 0.1889 - val_accuracy: 0.9464 - 16s/epoch - 38ms/step\n",
            "Epoch 672/1000\n",
            "\n",
            "Epoch 672: val_accuracy did not improve from 0.95140\n",
            "429/429 - 17s - loss: 0.2618 - accuracy: 0.9189 - val_loss: 0.1938 - val_accuracy: 0.9440 - 17s/epoch - 40ms/step\n",
            "Epoch 673/1000\n",
            "\n",
            "Epoch 673: val_accuracy did not improve from 0.95140\n",
            "429/429 - 17s - loss: 0.2668 - accuracy: 0.9168 - val_loss: 0.2042 - val_accuracy: 0.9384 - 17s/epoch - 39ms/step\n",
            "Epoch 674/1000\n",
            "\n",
            "Epoch 674: val_accuracy did not improve from 0.95140\n",
            "429/429 - 18s - loss: 0.2634 - accuracy: 0.9195 - val_loss: 0.1797 - val_accuracy: 0.9450 - 18s/epoch - 42ms/step\n",
            "Epoch 675/1000\n",
            "\n",
            "Epoch 675: val_accuracy did not improve from 0.95140\n",
            "429/429 - 17s - loss: 0.2628 - accuracy: 0.9191 - val_loss: 0.1802 - val_accuracy: 0.9474 - 17s/epoch - 39ms/step\n",
            "Epoch 676/1000\n",
            "\n",
            "Epoch 676: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2631 - accuracy: 0.9174 - val_loss: 0.1881 - val_accuracy: 0.9440 - 16s/epoch - 37ms/step\n",
            "Epoch 677/1000\n",
            "\n",
            "Epoch 677: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2621 - accuracy: 0.9191 - val_loss: 0.2196 - val_accuracy: 0.9378 - 16s/epoch - 37ms/step\n",
            "Epoch 678/1000\n",
            "\n",
            "Epoch 678: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2618 - accuracy: 0.9192 - val_loss: 0.1931 - val_accuracy: 0.9448 - 16s/epoch - 37ms/step\n",
            "Epoch 679/1000\n",
            "\n",
            "Epoch 679: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2640 - accuracy: 0.9190 - val_loss: 0.1785 - val_accuracy: 0.9500 - 16s/epoch - 37ms/step\n",
            "Epoch 680/1000\n",
            "\n",
            "Epoch 680: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2637 - accuracy: 0.9185 - val_loss: 0.1993 - val_accuracy: 0.9414 - 16s/epoch - 36ms/step\n",
            "Epoch 681/1000\n",
            "\n",
            "Epoch 681: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2626 - accuracy: 0.9195 - val_loss: 0.1851 - val_accuracy: 0.9460 - 16s/epoch - 37ms/step\n",
            "Epoch 682/1000\n",
            "\n",
            "Epoch 682: val_accuracy did not improve from 0.95140\n",
            "429/429 - 16s - loss: 0.2637 - accuracy: 0.9189 - val_loss: 0.2083 - val_accuracy: 0.9360 - 16s/epoch - 37ms/step\n",
            "Epoch 683/1000\n",
            "\n",
            "Epoch 683: val_accuracy did not improve from 0.95140\n",
            "429/429 - 17s - loss: 0.2653 - accuracy: 0.9180 - val_loss: 0.2335 - val_accuracy: 0.9294 - 17s/epoch - 39ms/step\n",
            "Epoch 684/1000\n",
            "\n",
            "Epoch 684: val_accuracy improved from 0.95140 to 0.95240, saving model to best_model,keras\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model,keras\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "429/429 - 20s - loss: 0.2593 - accuracy: 0.9207 - val_loss: 0.1656 - val_accuracy: 0.9524 - 20s/epoch - 47ms/step\n",
            "Epoch 685/1000\n",
            "\n",
            "Epoch 685: val_accuracy did not improve from 0.95240\n",
            "429/429 - 19s - loss: 0.2589 - accuracy: 0.9199 - val_loss: 0.1896 - val_accuracy: 0.9434 - 19s/epoch - 44ms/step\n",
            "Epoch 686/1000\n",
            "\n",
            "Epoch 686: val_accuracy did not improve from 0.95240\n",
            "429/429 - 21s - loss: 0.2636 - accuracy: 0.9173 - val_loss: 0.2032 - val_accuracy: 0.9394 - 21s/epoch - 49ms/step\n",
            "Epoch 687/1000\n",
            "\n",
            "Epoch 687: val_accuracy did not improve from 0.95240\n",
            "429/429 - 22s - loss: 0.2597 - accuracy: 0.9193 - val_loss: 0.2126 - val_accuracy: 0.9338 - 22s/epoch - 52ms/step\n",
            "Epoch 688/1000\n",
            "\n",
            "Epoch 688: val_accuracy did not improve from 0.95240\n",
            "429/429 - 21s - loss: 0.2639 - accuracy: 0.9190 - val_loss: 0.1926 - val_accuracy: 0.9454 - 21s/epoch - 49ms/step\n",
            "Epoch 689/1000\n",
            "\n",
            "Epoch 689: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2630 - accuracy: 0.9183 - val_loss: 0.2038 - val_accuracy: 0.9358 - 17s/epoch - 40ms/step\n",
            "Epoch 690/1000\n",
            "\n",
            "Epoch 690: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2627 - accuracy: 0.9194 - val_loss: 0.1990 - val_accuracy: 0.9402 - 16s/epoch - 38ms/step\n",
            "Epoch 691/1000\n",
            "\n",
            "Epoch 691: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2629 - accuracy: 0.9188 - val_loss: 0.2111 - val_accuracy: 0.9374 - 16s/epoch - 37ms/step\n",
            "Epoch 692/1000\n",
            "\n",
            "Epoch 692: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2636 - accuracy: 0.9182 - val_loss: 0.1967 - val_accuracy: 0.9374 - 16s/epoch - 37ms/step\n",
            "Epoch 693/1000\n",
            "\n",
            "Epoch 693: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2610 - accuracy: 0.9197 - val_loss: 0.1976 - val_accuracy: 0.9418 - 16s/epoch - 37ms/step\n",
            "Epoch 694/1000\n",
            "\n",
            "Epoch 694: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2602 - accuracy: 0.9195 - val_loss: 0.2032 - val_accuracy: 0.9442 - 16s/epoch - 38ms/step\n",
            "Epoch 695/1000\n",
            "\n",
            "Epoch 695: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2606 - accuracy: 0.9215 - val_loss: 0.2020 - val_accuracy: 0.9418 - 17s/epoch - 40ms/step\n",
            "Epoch 696/1000\n",
            "\n",
            "Epoch 696: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2637 - accuracy: 0.9190 - val_loss: 0.2007 - val_accuracy: 0.9392 - 16s/epoch - 38ms/step\n",
            "Epoch 697/1000\n",
            "\n",
            "Epoch 697: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2621 - accuracy: 0.9185 - val_loss: 0.2036 - val_accuracy: 0.9376 - 16s/epoch - 38ms/step\n",
            "Epoch 698/1000\n",
            "\n",
            "Epoch 698: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2650 - accuracy: 0.9187 - val_loss: 0.1836 - val_accuracy: 0.9472 - 16s/epoch - 38ms/step\n",
            "Epoch 699/1000\n",
            "\n",
            "Epoch 699: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2659 - accuracy: 0.9181 - val_loss: 0.2023 - val_accuracy: 0.9386 - 17s/epoch - 39ms/step\n",
            "Epoch 700/1000\n",
            "\n",
            "Epoch 700: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2635 - accuracy: 0.9193 - val_loss: 0.2035 - val_accuracy: 0.9402 - 17s/epoch - 39ms/step\n",
            "Epoch 701/1000\n",
            "\n",
            "Epoch 701: val_accuracy did not improve from 0.95240\n",
            "429/429 - 18s - loss: 0.2638 - accuracy: 0.9194 - val_loss: 0.1980 - val_accuracy: 0.9436 - 18s/epoch - 43ms/step\n",
            "Epoch 702/1000\n",
            "\n",
            "Epoch 702: val_accuracy did not improve from 0.95240\n",
            "429/429 - 18s - loss: 0.2641 - accuracy: 0.9184 - val_loss: 0.1958 - val_accuracy: 0.9432 - 18s/epoch - 41ms/step\n",
            "Epoch 703/1000\n",
            "\n",
            "Epoch 703: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2630 - accuracy: 0.9178 - val_loss: 0.1923 - val_accuracy: 0.9444 - 17s/epoch - 39ms/step\n",
            "Epoch 704/1000\n",
            "\n",
            "Epoch 704: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2636 - accuracy: 0.9182 - val_loss: 0.2100 - val_accuracy: 0.9378 - 16s/epoch - 38ms/step\n",
            "Epoch 705/1000\n",
            "\n",
            "Epoch 705: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2595 - accuracy: 0.9207 - val_loss: 0.2138 - val_accuracy: 0.9388 - 16s/epoch - 37ms/step\n",
            "Epoch 706/1000\n",
            "\n",
            "Epoch 706: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2602 - accuracy: 0.9195 - val_loss: 0.2020 - val_accuracy: 0.9400 - 16s/epoch - 38ms/step\n",
            "Epoch 707/1000\n",
            "\n",
            "Epoch 707: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2649 - accuracy: 0.9189 - val_loss: 0.2042 - val_accuracy: 0.9370 - 16s/epoch - 38ms/step\n",
            "Epoch 708/1000\n",
            "\n",
            "Epoch 708: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2642 - accuracy: 0.9178 - val_loss: 0.2067 - val_accuracy: 0.9400 - 16s/epoch - 38ms/step\n",
            "Epoch 709/1000\n",
            "\n",
            "Epoch 709: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2604 - accuracy: 0.9201 - val_loss: 0.2140 - val_accuracy: 0.9316 - 16s/epoch - 37ms/step\n",
            "Epoch 710/1000\n",
            "\n",
            "Epoch 710: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2628 - accuracy: 0.9203 - val_loss: 0.2240 - val_accuracy: 0.9320 - 16s/epoch - 38ms/step\n",
            "Epoch 711/1000\n",
            "\n",
            "Epoch 711: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2611 - accuracy: 0.9185 - val_loss: 0.2098 - val_accuracy: 0.9384 - 16s/epoch - 38ms/step\n",
            "Epoch 712/1000\n",
            "\n",
            "Epoch 712: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2631 - accuracy: 0.9190 - val_loss: 0.1928 - val_accuracy: 0.9448 - 17s/epoch - 39ms/step\n",
            "Epoch 713/1000\n",
            "\n",
            "Epoch 713: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2628 - accuracy: 0.9187 - val_loss: 0.2091 - val_accuracy: 0.9352 - 17s/epoch - 39ms/step\n",
            "Epoch 714/1000\n",
            "\n",
            "Epoch 714: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2642 - accuracy: 0.9177 - val_loss: 0.2089 - val_accuracy: 0.9388 - 17s/epoch - 39ms/step\n",
            "Epoch 715/1000\n",
            "\n",
            "Epoch 715: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2618 - accuracy: 0.9189 - val_loss: 0.2137 - val_accuracy: 0.9376 - 17s/epoch - 40ms/step\n",
            "Epoch 716/1000\n",
            "\n",
            "Epoch 716: val_accuracy did not improve from 0.95240\n",
            "429/429 - 22s - loss: 0.2594 - accuracy: 0.9185 - val_loss: 0.2120 - val_accuracy: 0.9318 - 22s/epoch - 51ms/step\n",
            "Epoch 717/1000\n",
            "\n",
            "Epoch 717: val_accuracy did not improve from 0.95240\n",
            "429/429 - 18s - loss: 0.2666 - accuracy: 0.9173 - val_loss: 0.1864 - val_accuracy: 0.9476 - 18s/epoch - 43ms/step\n",
            "Epoch 718/1000\n",
            "\n",
            "Epoch 718: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2585 - accuracy: 0.9201 - val_loss: 0.2007 - val_accuracy: 0.9416 - 16s/epoch - 37ms/step\n",
            "Epoch 719/1000\n",
            "\n",
            "Epoch 719: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2617 - accuracy: 0.9193 - val_loss: 0.1905 - val_accuracy: 0.9426 - 16s/epoch - 38ms/step\n",
            "Epoch 720/1000\n",
            "\n",
            "Epoch 720: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2613 - accuracy: 0.9190 - val_loss: 0.2159 - val_accuracy: 0.9412 - 16s/epoch - 36ms/step\n",
            "Epoch 721/1000\n",
            "\n",
            "Epoch 721: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2624 - accuracy: 0.9177 - val_loss: 0.1915 - val_accuracy: 0.9420 - 17s/epoch - 39ms/step\n",
            "Epoch 722/1000\n",
            "\n",
            "Epoch 722: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2643 - accuracy: 0.9186 - val_loss: 0.2021 - val_accuracy: 0.9364 - 16s/epoch - 37ms/step\n",
            "Epoch 723/1000\n",
            "\n",
            "Epoch 723: val_accuracy did not improve from 0.95240\n",
            "429/429 - 19s - loss: 0.2607 - accuracy: 0.9209 - val_loss: 0.2076 - val_accuracy: 0.9368 - 19s/epoch - 43ms/step\n",
            "Epoch 724/1000\n",
            "\n",
            "Epoch 724: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2641 - accuracy: 0.9174 - val_loss: 0.2138 - val_accuracy: 0.9366 - 17s/epoch - 39ms/step\n",
            "Epoch 725/1000\n",
            "\n",
            "Epoch 725: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2624 - accuracy: 0.9191 - val_loss: 0.2095 - val_accuracy: 0.9376 - 16s/epoch - 37ms/step\n",
            "Epoch 726/1000\n",
            "\n",
            "Epoch 726: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2575 - accuracy: 0.9202 - val_loss: 0.2153 - val_accuracy: 0.9384 - 16s/epoch - 38ms/step\n",
            "Epoch 727/1000\n",
            "\n",
            "Epoch 727: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2621 - accuracy: 0.9192 - val_loss: 0.1957 - val_accuracy: 0.9438 - 17s/epoch - 39ms/step\n",
            "Epoch 728/1000\n",
            "\n",
            "Epoch 728: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2632 - accuracy: 0.9186 - val_loss: 0.2162 - val_accuracy: 0.9372 - 17s/epoch - 39ms/step\n",
            "Epoch 729/1000\n",
            "\n",
            "Epoch 729: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2612 - accuracy: 0.9194 - val_loss: 0.2046 - val_accuracy: 0.9398 - 17s/epoch - 39ms/step\n",
            "Epoch 730/1000\n",
            "\n",
            "Epoch 730: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2627 - accuracy: 0.9187 - val_loss: 0.2068 - val_accuracy: 0.9400 - 17s/epoch - 40ms/step\n",
            "Epoch 731/1000\n",
            "\n",
            "Epoch 731: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2593 - accuracy: 0.9203 - val_loss: 0.2180 - val_accuracy: 0.9338 - 16s/epoch - 38ms/step\n",
            "Epoch 732/1000\n",
            "\n",
            "Epoch 732: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2622 - accuracy: 0.9179 - val_loss: 0.2074 - val_accuracy: 0.9410 - 16s/epoch - 37ms/step\n",
            "Epoch 733/1000\n",
            "\n",
            "Epoch 733: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2635 - accuracy: 0.9200 - val_loss: 0.2034 - val_accuracy: 0.9398 - 16s/epoch - 37ms/step\n",
            "Epoch 734/1000\n",
            "\n",
            "Epoch 734: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2624 - accuracy: 0.9190 - val_loss: 0.2039 - val_accuracy: 0.9388 - 16s/epoch - 37ms/step\n",
            "Epoch 735/1000\n",
            "\n",
            "Epoch 735: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2593 - accuracy: 0.9208 - val_loss: 0.2013 - val_accuracy: 0.9376 - 16s/epoch - 37ms/step\n",
            "Epoch 736/1000\n",
            "\n",
            "Epoch 736: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2618 - accuracy: 0.9189 - val_loss: 0.2225 - val_accuracy: 0.9366 - 16s/epoch - 37ms/step\n",
            "Epoch 737/1000\n",
            "\n",
            "Epoch 737: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2607 - accuracy: 0.9194 - val_loss: 0.2323 - val_accuracy: 0.9300 - 16s/epoch - 37ms/step\n",
            "Epoch 738/1000\n",
            "\n",
            "Epoch 738: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2632 - accuracy: 0.9186 - val_loss: 0.2262 - val_accuracy: 0.9298 - 16s/epoch - 38ms/step\n",
            "Epoch 739/1000\n",
            "\n",
            "Epoch 739: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2647 - accuracy: 0.9181 - val_loss: 0.2083 - val_accuracy: 0.9374 - 16s/epoch - 38ms/step\n",
            "Epoch 740/1000\n",
            "\n",
            "Epoch 740: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2629 - accuracy: 0.9201 - val_loss: 0.2044 - val_accuracy: 0.9392 - 16s/epoch - 38ms/step\n",
            "Epoch 741/1000\n",
            "\n",
            "Epoch 741: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2605 - accuracy: 0.9196 - val_loss: 0.2199 - val_accuracy: 0.9364 - 17s/epoch - 39ms/step\n",
            "Epoch 742/1000\n",
            "\n",
            "Epoch 742: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2621 - accuracy: 0.9195 - val_loss: 0.2081 - val_accuracy: 0.9380 - 17s/epoch - 39ms/step\n",
            "Epoch 743/1000\n",
            "\n",
            "Epoch 743: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2637 - accuracy: 0.9191 - val_loss: 0.2094 - val_accuracy: 0.9356 - 17s/epoch - 40ms/step\n",
            "Epoch 744/1000\n",
            "\n",
            "Epoch 744: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2593 - accuracy: 0.9205 - val_loss: 0.2048 - val_accuracy: 0.9374 - 17s/epoch - 40ms/step\n",
            "Epoch 745/1000\n",
            "\n",
            "Epoch 745: val_accuracy did not improve from 0.95240\n",
            "429/429 - 23s - loss: 0.2613 - accuracy: 0.9194 - val_loss: 0.2120 - val_accuracy: 0.9356 - 23s/epoch - 54ms/step\n",
            "Epoch 746/1000\n",
            "\n",
            "Epoch 746: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2616 - accuracy: 0.9186 - val_loss: 0.2005 - val_accuracy: 0.9422 - 16s/epoch - 37ms/step\n",
            "Epoch 747/1000\n",
            "\n",
            "Epoch 747: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2641 - accuracy: 0.9193 - val_loss: 0.1986 - val_accuracy: 0.9432 - 16s/epoch - 37ms/step\n",
            "Epoch 748/1000\n",
            "\n",
            "Epoch 748: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2585 - accuracy: 0.9205 - val_loss: 0.2089 - val_accuracy: 0.9396 - 16s/epoch - 37ms/step\n",
            "Epoch 749/1000\n",
            "\n",
            "Epoch 749: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2638 - accuracy: 0.9182 - val_loss: 0.1846 - val_accuracy: 0.9468 - 16s/epoch - 37ms/step\n",
            "Epoch 750/1000\n",
            "\n",
            "Epoch 750: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2642 - accuracy: 0.9179 - val_loss: 0.1859 - val_accuracy: 0.9444 - 16s/epoch - 38ms/step\n",
            "Epoch 751/1000\n",
            "\n",
            "Epoch 751: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2613 - accuracy: 0.9192 - val_loss: 0.2049 - val_accuracy: 0.9376 - 16s/epoch - 38ms/step\n",
            "Epoch 752/1000\n",
            "\n",
            "Epoch 752: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2661 - accuracy: 0.9174 - val_loss: 0.1943 - val_accuracy: 0.9432 - 16s/epoch - 37ms/step\n",
            "Epoch 753/1000\n",
            "\n",
            "Epoch 753: val_accuracy did not improve from 0.95240\n",
            "429/429 - 18s - loss: 0.2606 - accuracy: 0.9205 - val_loss: 0.2028 - val_accuracy: 0.9378 - 18s/epoch - 42ms/step\n",
            "Epoch 754/1000\n",
            "\n",
            "Epoch 754: val_accuracy did not improve from 0.95240\n",
            "429/429 - 29s - loss: 0.2626 - accuracy: 0.9185 - val_loss: 0.2091 - val_accuracy: 0.9390 - 29s/epoch - 68ms/step\n",
            "Epoch 755/1000\n",
            "\n",
            "Epoch 755: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2642 - accuracy: 0.9184 - val_loss: 0.1977 - val_accuracy: 0.9388 - 17s/epoch - 40ms/step\n",
            "Epoch 756/1000\n",
            "\n",
            "Epoch 756: val_accuracy did not improve from 0.95240\n",
            "429/429 - 18s - loss: 0.2636 - accuracy: 0.9189 - val_loss: 0.1931 - val_accuracy: 0.9406 - 18s/epoch - 41ms/step\n",
            "Epoch 757/1000\n",
            "\n",
            "Epoch 757: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2644 - accuracy: 0.9176 - val_loss: 0.1957 - val_accuracy: 0.9438 - 17s/epoch - 39ms/step\n",
            "Epoch 758/1000\n",
            "\n",
            "Epoch 758: val_accuracy did not improve from 0.95240\n",
            "429/429 - 18s - loss: 0.2672 - accuracy: 0.9173 - val_loss: 0.2058 - val_accuracy: 0.9350 - 18s/epoch - 42ms/step\n",
            "Epoch 759/1000\n",
            "\n",
            "Epoch 759: val_accuracy did not improve from 0.95240\n",
            "429/429 - 19s - loss: 0.2592 - accuracy: 0.9200 - val_loss: 0.2086 - val_accuracy: 0.9408 - 19s/epoch - 44ms/step\n",
            "Epoch 760/1000\n",
            "\n",
            "Epoch 760: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2605 - accuracy: 0.9188 - val_loss: 0.2039 - val_accuracy: 0.9420 - 16s/epoch - 37ms/step\n",
            "Epoch 761/1000\n",
            "\n",
            "Epoch 761: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2614 - accuracy: 0.9199 - val_loss: 0.1808 - val_accuracy: 0.9500 - 16s/epoch - 37ms/step\n",
            "Epoch 762/1000\n",
            "\n",
            "Epoch 762: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2612 - accuracy: 0.9192 - val_loss: 0.2165 - val_accuracy: 0.9336 - 16s/epoch - 37ms/step\n",
            "Epoch 763/1000\n",
            "\n",
            "Epoch 763: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2645 - accuracy: 0.9181 - val_loss: 0.1947 - val_accuracy: 0.9454 - 16s/epoch - 36ms/step\n",
            "Epoch 764/1000\n",
            "\n",
            "Epoch 764: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2655 - accuracy: 0.9183 - val_loss: 0.2075 - val_accuracy: 0.9416 - 16s/epoch - 37ms/step\n",
            "Epoch 765/1000\n",
            "\n",
            "Epoch 765: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2655 - accuracy: 0.9187 - val_loss: 0.1918 - val_accuracy: 0.9438 - 16s/epoch - 38ms/step\n",
            "Epoch 766/1000\n",
            "\n",
            "Epoch 766: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2614 - accuracy: 0.9198 - val_loss: 0.1858 - val_accuracy: 0.9476 - 17s/epoch - 39ms/step\n",
            "Epoch 767/1000\n",
            "\n",
            "Epoch 767: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2615 - accuracy: 0.9179 - val_loss: 0.1877 - val_accuracy: 0.9466 - 16s/epoch - 38ms/step\n",
            "Epoch 768/1000\n",
            "\n",
            "Epoch 768: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2612 - accuracy: 0.9196 - val_loss: 0.2032 - val_accuracy: 0.9418 - 16s/epoch - 38ms/step\n",
            "Epoch 769/1000\n",
            "\n",
            "Epoch 769: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2643 - accuracy: 0.9195 - val_loss: 0.1936 - val_accuracy: 0.9468 - 16s/epoch - 38ms/step\n",
            "Epoch 770/1000\n",
            "\n",
            "Epoch 770: val_accuracy did not improve from 0.95240\n",
            "429/429 - 18s - loss: 0.2616 - accuracy: 0.9184 - val_loss: 0.1846 - val_accuracy: 0.9466 - 18s/epoch - 42ms/step\n",
            "Epoch 771/1000\n",
            "\n",
            "Epoch 771: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2597 - accuracy: 0.9195 - val_loss: 0.1891 - val_accuracy: 0.9442 - 17s/epoch - 39ms/step\n",
            "Epoch 772/1000\n",
            "\n",
            "Epoch 772: val_accuracy did not improve from 0.95240\n",
            "429/429 - 18s - loss: 0.2613 - accuracy: 0.9188 - val_loss: 0.1892 - val_accuracy: 0.9438 - 18s/epoch - 41ms/step\n",
            "Epoch 773/1000\n",
            "\n",
            "Epoch 773: val_accuracy did not improve from 0.95240\n",
            "429/429 - 20s - loss: 0.2606 - accuracy: 0.9197 - val_loss: 0.1965 - val_accuracy: 0.9412 - 20s/epoch - 46ms/step\n",
            "Epoch 774/1000\n",
            "\n",
            "Epoch 774: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2618 - accuracy: 0.9194 - val_loss: 0.1863 - val_accuracy: 0.9448 - 16s/epoch - 38ms/step\n",
            "Epoch 775/1000\n",
            "\n",
            "Epoch 775: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2646 - accuracy: 0.9179 - val_loss: 0.1960 - val_accuracy: 0.9438 - 16s/epoch - 38ms/step\n",
            "Epoch 776/1000\n",
            "\n",
            "Epoch 776: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2616 - accuracy: 0.9180 - val_loss: 0.1916 - val_accuracy: 0.9466 - 16s/epoch - 37ms/step\n",
            "Epoch 777/1000\n",
            "\n",
            "Epoch 777: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2652 - accuracy: 0.9183 - val_loss: 0.2155 - val_accuracy: 0.9362 - 16s/epoch - 37ms/step\n",
            "Epoch 778/1000\n",
            "\n",
            "Epoch 778: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2571 - accuracy: 0.9196 - val_loss: 0.1850 - val_accuracy: 0.9464 - 16s/epoch - 38ms/step\n",
            "Epoch 779/1000\n",
            "\n",
            "Epoch 779: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2617 - accuracy: 0.9184 - val_loss: 0.1888 - val_accuracy: 0.9438 - 16s/epoch - 37ms/step\n",
            "Epoch 780/1000\n",
            "\n",
            "Epoch 780: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2612 - accuracy: 0.9198 - val_loss: 0.2161 - val_accuracy: 0.9382 - 16s/epoch - 38ms/step\n",
            "Epoch 781/1000\n",
            "\n",
            "Epoch 781: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2607 - accuracy: 0.9200 - val_loss: 0.2006 - val_accuracy: 0.9422 - 16s/epoch - 38ms/step\n",
            "Epoch 782/1000\n",
            "\n",
            "Epoch 782: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2656 - accuracy: 0.9180 - val_loss: 0.1910 - val_accuracy: 0.9438 - 16s/epoch - 38ms/step\n",
            "Epoch 783/1000\n",
            "\n",
            "Epoch 783: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2662 - accuracy: 0.9169 - val_loss: 0.1832 - val_accuracy: 0.9434 - 16s/epoch - 38ms/step\n",
            "Epoch 784/1000\n",
            "\n",
            "Epoch 784: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2624 - accuracy: 0.9186 - val_loss: 0.1842 - val_accuracy: 0.9500 - 17s/epoch - 39ms/step\n",
            "Epoch 785/1000\n",
            "\n",
            "Epoch 785: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2619 - accuracy: 0.9190 - val_loss: 0.1941 - val_accuracy: 0.9452 - 16s/epoch - 38ms/step\n",
            "Epoch 786/1000\n",
            "\n",
            "Epoch 786: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2616 - accuracy: 0.9199 - val_loss: 0.2254 - val_accuracy: 0.9378 - 17s/epoch - 40ms/step\n",
            "Epoch 787/1000\n",
            "\n",
            "Epoch 787: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2622 - accuracy: 0.9183 - val_loss: 0.2114 - val_accuracy: 0.9374 - 17s/epoch - 39ms/step\n",
            "Epoch 788/1000\n",
            "\n",
            "Epoch 788: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2618 - accuracy: 0.9193 - val_loss: 0.1920 - val_accuracy: 0.9426 - 16s/epoch - 37ms/step\n",
            "Epoch 789/1000\n",
            "\n",
            "Epoch 789: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2614 - accuracy: 0.9200 - val_loss: 0.1899 - val_accuracy: 0.9458 - 16s/epoch - 37ms/step\n",
            "Epoch 790/1000\n",
            "\n",
            "Epoch 790: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2610 - accuracy: 0.9196 - val_loss: 0.1943 - val_accuracy: 0.9444 - 16s/epoch - 37ms/step\n",
            "Epoch 791/1000\n",
            "\n",
            "Epoch 791: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2633 - accuracy: 0.9182 - val_loss: 0.2109 - val_accuracy: 0.9370 - 16s/epoch - 38ms/step\n",
            "Epoch 792/1000\n",
            "\n",
            "Epoch 792: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2626 - accuracy: 0.9174 - val_loss: 0.2159 - val_accuracy: 0.9334 - 16s/epoch - 37ms/step\n",
            "Epoch 793/1000\n",
            "\n",
            "Epoch 793: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2638 - accuracy: 0.9173 - val_loss: 0.1996 - val_accuracy: 0.9438 - 16s/epoch - 37ms/step\n",
            "Epoch 794/1000\n",
            "\n",
            "Epoch 794: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2599 - accuracy: 0.9187 - val_loss: 0.1773 - val_accuracy: 0.9498 - 16s/epoch - 37ms/step\n",
            "Epoch 795/1000\n",
            "\n",
            "Epoch 795: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2639 - accuracy: 0.9192 - val_loss: 0.2095 - val_accuracy: 0.9364 - 16s/epoch - 37ms/step\n",
            "Epoch 796/1000\n",
            "\n",
            "Epoch 796: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2607 - accuracy: 0.9195 - val_loss: 0.1974 - val_accuracy: 0.9394 - 16s/epoch - 38ms/step\n",
            "Epoch 797/1000\n",
            "\n",
            "Epoch 797: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2645 - accuracy: 0.9195 - val_loss: 0.2008 - val_accuracy: 0.9410 - 16s/epoch - 38ms/step\n",
            "Epoch 798/1000\n",
            "\n",
            "Epoch 798: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2622 - accuracy: 0.9191 - val_loss: 0.1802 - val_accuracy: 0.9494 - 17s/epoch - 39ms/step\n",
            "Epoch 799/1000\n",
            "\n",
            "Epoch 799: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2583 - accuracy: 0.9205 - val_loss: 0.1899 - val_accuracy: 0.9434 - 16s/epoch - 38ms/step\n",
            "Epoch 800/1000\n",
            "\n",
            "Epoch 800: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2658 - accuracy: 0.9192 - val_loss: 0.2179 - val_accuracy: 0.9356 - 16s/epoch - 38ms/step\n",
            "Epoch 801/1000\n",
            "\n",
            "Epoch 801: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2612 - accuracy: 0.9195 - val_loss: 0.2000 - val_accuracy: 0.9402 - 16s/epoch - 38ms/step\n",
            "Epoch 802/1000\n",
            "\n",
            "Epoch 802: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2634 - accuracy: 0.9194 - val_loss: 0.2041 - val_accuracy: 0.9396 - 16s/epoch - 37ms/step\n",
            "Epoch 803/1000\n",
            "\n",
            "Epoch 803: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2635 - accuracy: 0.9178 - val_loss: 0.2178 - val_accuracy: 0.9366 - 16s/epoch - 37ms/step\n",
            "Epoch 804/1000\n",
            "\n",
            "Epoch 804: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2603 - accuracy: 0.9191 - val_loss: 0.1850 - val_accuracy: 0.9450 - 16s/epoch - 37ms/step\n",
            "Epoch 805/1000\n",
            "\n",
            "Epoch 805: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2626 - accuracy: 0.9190 - val_loss: 0.1963 - val_accuracy: 0.9418 - 16s/epoch - 36ms/step\n",
            "Epoch 806/1000\n",
            "\n",
            "Epoch 806: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2610 - accuracy: 0.9195 - val_loss: 0.2012 - val_accuracy: 0.9394 - 16s/epoch - 37ms/step\n",
            "Epoch 807/1000\n",
            "\n",
            "Epoch 807: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2590 - accuracy: 0.9190 - val_loss: 0.1755 - val_accuracy: 0.9508 - 16s/epoch - 38ms/step\n",
            "Epoch 808/1000\n",
            "\n",
            "Epoch 808: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2618 - accuracy: 0.9190 - val_loss: 0.2068 - val_accuracy: 0.9396 - 16s/epoch - 38ms/step\n",
            "Epoch 809/1000\n",
            "\n",
            "Epoch 809: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2600 - accuracy: 0.9186 - val_loss: 0.1862 - val_accuracy: 0.9466 - 17s/epoch - 39ms/step\n",
            "Epoch 810/1000\n",
            "\n",
            "Epoch 810: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2640 - accuracy: 0.9179 - val_loss: 0.1966 - val_accuracy: 0.9442 - 17s/epoch - 38ms/step\n",
            "Epoch 811/1000\n",
            "\n",
            "Epoch 811: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2638 - accuracy: 0.9186 - val_loss: 0.1993 - val_accuracy: 0.9418 - 16s/epoch - 38ms/step\n",
            "Epoch 812/1000\n",
            "\n",
            "Epoch 812: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2632 - accuracy: 0.9175 - val_loss: 0.1975 - val_accuracy: 0.9426 - 16s/epoch - 38ms/step\n",
            "Epoch 813/1000\n",
            "\n",
            "Epoch 813: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2615 - accuracy: 0.9183 - val_loss: 0.1904 - val_accuracy: 0.9422 - 17s/epoch - 39ms/step\n",
            "Epoch 814/1000\n",
            "\n",
            "Epoch 814: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2648 - accuracy: 0.9181 - val_loss: 0.1966 - val_accuracy: 0.9446 - 17s/epoch - 41ms/step\n",
            "Epoch 815/1000\n",
            "\n",
            "Epoch 815: val_accuracy did not improve from 0.95240\n",
            "429/429 - 18s - loss: 0.2595 - accuracy: 0.9205 - val_loss: 0.1981 - val_accuracy: 0.9422 - 18s/epoch - 42ms/step\n",
            "Epoch 816/1000\n",
            "\n",
            "Epoch 816: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2659 - accuracy: 0.9189 - val_loss: 0.1973 - val_accuracy: 0.9424 - 16s/epoch - 37ms/step\n",
            "Epoch 817/1000\n",
            "\n",
            "Epoch 817: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2588 - accuracy: 0.9200 - val_loss: 0.1989 - val_accuracy: 0.9432 - 16s/epoch - 37ms/step\n",
            "Epoch 818/1000\n",
            "\n",
            "Epoch 818: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2608 - accuracy: 0.9192 - val_loss: 0.1917 - val_accuracy: 0.9460 - 16s/epoch - 37ms/step\n",
            "Epoch 819/1000\n",
            "\n",
            "Epoch 819: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2620 - accuracy: 0.9182 - val_loss: 0.2030 - val_accuracy: 0.9384 - 17s/epoch - 39ms/step\n",
            "Epoch 820/1000\n",
            "\n",
            "Epoch 820: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2650 - accuracy: 0.9183 - val_loss: 0.2064 - val_accuracy: 0.9378 - 16s/epoch - 37ms/step\n",
            "Epoch 821/1000\n",
            "\n",
            "Epoch 821: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2621 - accuracy: 0.9190 - val_loss: 0.1890 - val_accuracy: 0.9442 - 16s/epoch - 38ms/step\n",
            "Epoch 822/1000\n",
            "\n",
            "Epoch 822: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2614 - accuracy: 0.9189 - val_loss: 0.1880 - val_accuracy: 0.9456 - 16s/epoch - 37ms/step\n",
            "Epoch 823/1000\n",
            "\n",
            "Epoch 823: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2561 - accuracy: 0.9211 - val_loss: 0.1853 - val_accuracy: 0.9462 - 16s/epoch - 38ms/step\n",
            "Epoch 824/1000\n",
            "\n",
            "Epoch 824: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2628 - accuracy: 0.9194 - val_loss: 0.1978 - val_accuracy: 0.9408 - 16s/epoch - 38ms/step\n",
            "Epoch 825/1000\n",
            "\n",
            "Epoch 825: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2610 - accuracy: 0.9193 - val_loss: 0.1967 - val_accuracy: 0.9394 - 17s/epoch - 40ms/step\n",
            "Epoch 826/1000\n",
            "\n",
            "Epoch 826: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2619 - accuracy: 0.9200 - val_loss: 0.1863 - val_accuracy: 0.9438 - 17s/epoch - 39ms/step\n",
            "Epoch 827/1000\n",
            "\n",
            "Epoch 827: val_accuracy did not improve from 0.95240\n",
            "429/429 - 22s - loss: 0.2646 - accuracy: 0.9177 - val_loss: 0.1923 - val_accuracy: 0.9432 - 22s/epoch - 51ms/step\n",
            "Epoch 828/1000\n",
            "\n",
            "Epoch 828: val_accuracy did not improve from 0.95240\n",
            "429/429 - 20s - loss: 0.2640 - accuracy: 0.9190 - val_loss: 0.2033 - val_accuracy: 0.9420 - 20s/epoch - 47ms/step\n",
            "Epoch 829/1000\n",
            "\n",
            "Epoch 829: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2629 - accuracy: 0.9182 - val_loss: 0.2108 - val_accuracy: 0.9390 - 17s/epoch - 39ms/step\n",
            "Epoch 830/1000\n",
            "\n",
            "Epoch 830: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2652 - accuracy: 0.9186 - val_loss: 0.1892 - val_accuracy: 0.9424 - 16s/epoch - 37ms/step\n",
            "Epoch 831/1000\n",
            "\n",
            "Epoch 831: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2605 - accuracy: 0.9197 - val_loss: 0.1881 - val_accuracy: 0.9460 - 16s/epoch - 37ms/step\n",
            "Epoch 832/1000\n",
            "\n",
            "Epoch 832: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2611 - accuracy: 0.9198 - val_loss: 0.1958 - val_accuracy: 0.9434 - 16s/epoch - 37ms/step\n",
            "Epoch 833/1000\n",
            "\n",
            "Epoch 833: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2627 - accuracy: 0.9181 - val_loss: 0.2036 - val_accuracy: 0.9396 - 16s/epoch - 37ms/step\n",
            "Epoch 834/1000\n",
            "\n",
            "Epoch 834: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2654 - accuracy: 0.9182 - val_loss: 0.1898 - val_accuracy: 0.9466 - 16s/epoch - 37ms/step\n",
            "Epoch 835/1000\n",
            "\n",
            "Epoch 835: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2591 - accuracy: 0.9196 - val_loss: 0.2007 - val_accuracy: 0.9424 - 16s/epoch - 38ms/step\n",
            "Epoch 836/1000\n",
            "\n",
            "Epoch 836: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2637 - accuracy: 0.9181 - val_loss: 0.1801 - val_accuracy: 0.9500 - 16s/epoch - 38ms/step\n",
            "Epoch 837/1000\n",
            "\n",
            "Epoch 837: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2616 - accuracy: 0.9186 - val_loss: 0.1899 - val_accuracy: 0.9454 - 16s/epoch - 37ms/step\n",
            "Epoch 838/1000\n",
            "\n",
            "Epoch 838: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2641 - accuracy: 0.9184 - val_loss: 0.1810 - val_accuracy: 0.9486 - 16s/epoch - 38ms/step\n",
            "Epoch 839/1000\n",
            "\n",
            "Epoch 839: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2600 - accuracy: 0.9196 - val_loss: 0.1912 - val_accuracy: 0.9446 - 16s/epoch - 38ms/step\n",
            "Epoch 840/1000\n",
            "\n",
            "Epoch 840: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2569 - accuracy: 0.9202 - val_loss: 0.1915 - val_accuracy: 0.9450 - 16s/epoch - 38ms/step\n",
            "Epoch 841/1000\n",
            "\n",
            "Epoch 841: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2622 - accuracy: 0.9200 - val_loss: 0.2092 - val_accuracy: 0.9398 - 16s/epoch - 38ms/step\n",
            "Epoch 842/1000\n",
            "\n",
            "Epoch 842: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2637 - accuracy: 0.9180 - val_loss: 0.2118 - val_accuracy: 0.9372 - 17s/epoch - 40ms/step\n",
            "Epoch 843/1000\n",
            "\n",
            "Epoch 843: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2608 - accuracy: 0.9201 - val_loss: 0.1806 - val_accuracy: 0.9492 - 17s/epoch - 40ms/step\n",
            "Epoch 844/1000\n",
            "\n",
            "Epoch 844: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2605 - accuracy: 0.9202 - val_loss: 0.2070 - val_accuracy: 0.9362 - 16s/epoch - 37ms/step\n",
            "Epoch 845/1000\n",
            "\n",
            "Epoch 845: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2637 - accuracy: 0.9179 - val_loss: 0.2358 - val_accuracy: 0.9284 - 16s/epoch - 37ms/step\n",
            "Epoch 846/1000\n",
            "\n",
            "Epoch 846: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2639 - accuracy: 0.9192 - val_loss: 0.2089 - val_accuracy: 0.9398 - 16s/epoch - 37ms/step\n",
            "Epoch 847/1000\n",
            "\n",
            "Epoch 847: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2584 - accuracy: 0.9205 - val_loss: 0.1985 - val_accuracy: 0.9414 - 16s/epoch - 38ms/step\n",
            "Epoch 848/1000\n",
            "\n",
            "Epoch 848: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2620 - accuracy: 0.9194 - val_loss: 0.1939 - val_accuracy: 0.9448 - 16s/epoch - 38ms/step\n",
            "Epoch 849/1000\n",
            "\n",
            "Epoch 849: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2612 - accuracy: 0.9204 - val_loss: 0.2064 - val_accuracy: 0.9374 - 16s/epoch - 37ms/step\n",
            "Epoch 850/1000\n",
            "\n",
            "Epoch 850: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2596 - accuracy: 0.9201 - val_loss: 0.1940 - val_accuracy: 0.9444 - 16s/epoch - 38ms/step\n",
            "Epoch 851/1000\n",
            "\n",
            "Epoch 851: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2580 - accuracy: 0.9201 - val_loss: 0.1994 - val_accuracy: 0.9408 - 16s/epoch - 38ms/step\n",
            "Epoch 852/1000\n",
            "\n",
            "Epoch 852: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2634 - accuracy: 0.9181 - val_loss: 0.2047 - val_accuracy: 0.9396 - 17s/epoch - 39ms/step\n",
            "Epoch 853/1000\n",
            "\n",
            "Epoch 853: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2603 - accuracy: 0.9189 - val_loss: 0.2035 - val_accuracy: 0.9410 - 16s/epoch - 38ms/step\n",
            "Epoch 854/1000\n",
            "\n",
            "Epoch 854: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2659 - accuracy: 0.9177 - val_loss: 0.1840 - val_accuracy: 0.9480 - 17s/epoch - 39ms/step\n",
            "Epoch 855/1000\n",
            "\n",
            "Epoch 855: val_accuracy did not improve from 0.95240\n",
            "429/429 - 21s - loss: 0.2607 - accuracy: 0.9185 - val_loss: 0.1916 - val_accuracy: 0.9446 - 21s/epoch - 49ms/step\n",
            "Epoch 856/1000\n",
            "\n",
            "Epoch 856: val_accuracy did not improve from 0.95240\n",
            "429/429 - 49s - loss: 0.2611 - accuracy: 0.9192 - val_loss: 0.1855 - val_accuracy: 0.9448 - 49s/epoch - 115ms/step\n",
            "Epoch 857/1000\n",
            "\n",
            "Epoch 857: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2613 - accuracy: 0.9195 - val_loss: 0.2116 - val_accuracy: 0.9374 - 17s/epoch - 41ms/step\n",
            "Epoch 858/1000\n",
            "\n",
            "Epoch 858: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2630 - accuracy: 0.9198 - val_loss: 0.1828 - val_accuracy: 0.9446 - 15s/epoch - 34ms/step\n",
            "Epoch 859/1000\n",
            "\n",
            "Epoch 859: val_accuracy did not improve from 0.95240\n",
            "429/429 - 14s - loss: 0.2635 - accuracy: 0.9185 - val_loss: 0.2035 - val_accuracy: 0.9402 - 14s/epoch - 34ms/step\n",
            "Epoch 860/1000\n",
            "\n",
            "Epoch 860: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2626 - accuracy: 0.9186 - val_loss: 0.2046 - val_accuracy: 0.9434 - 15s/epoch - 34ms/step\n",
            "Epoch 861/1000\n",
            "\n",
            "Epoch 861: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2643 - accuracy: 0.9188 - val_loss: 0.2144 - val_accuracy: 0.9394 - 15s/epoch - 35ms/step\n",
            "Epoch 862/1000\n",
            "\n",
            "Epoch 862: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2638 - accuracy: 0.9191 - val_loss: 0.1868 - val_accuracy: 0.9474 - 15s/epoch - 35ms/step\n",
            "Epoch 863/1000\n",
            "\n",
            "Epoch 863: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2601 - accuracy: 0.9187 - val_loss: 0.1998 - val_accuracy: 0.9392 - 15s/epoch - 36ms/step\n",
            "Epoch 864/1000\n",
            "\n",
            "Epoch 864: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2613 - accuracy: 0.9200 - val_loss: 0.1903 - val_accuracy: 0.9452 - 15s/epoch - 34ms/step\n",
            "Epoch 865/1000\n",
            "\n",
            "Epoch 865: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2601 - accuracy: 0.9204 - val_loss: 0.1925 - val_accuracy: 0.9412 - 15s/epoch - 35ms/step\n",
            "Epoch 866/1000\n",
            "\n",
            "Epoch 866: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2615 - accuracy: 0.9191 - val_loss: 0.1935 - val_accuracy: 0.9448 - 15s/epoch - 35ms/step\n",
            "Epoch 867/1000\n",
            "\n",
            "Epoch 867: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2632 - accuracy: 0.9187 - val_loss: 0.2006 - val_accuracy: 0.9436 - 15s/epoch - 36ms/step\n",
            "Epoch 868/1000\n",
            "\n",
            "Epoch 868: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2643 - accuracy: 0.9195 - val_loss: 0.2026 - val_accuracy: 0.9378 - 15s/epoch - 36ms/step\n",
            "Epoch 869/1000\n",
            "\n",
            "Epoch 869: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2627 - accuracy: 0.9189 - val_loss: 0.2059 - val_accuracy: 0.9384 - 15s/epoch - 35ms/step\n",
            "Epoch 870/1000\n",
            "\n",
            "Epoch 870: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2607 - accuracy: 0.9197 - val_loss: 0.1933 - val_accuracy: 0.9436 - 15s/epoch - 36ms/step\n",
            "Epoch 871/1000\n",
            "\n",
            "Epoch 871: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2576 - accuracy: 0.9214 - val_loss: 0.1875 - val_accuracy: 0.9460 - 15s/epoch - 34ms/step\n",
            "Epoch 872/1000\n",
            "\n",
            "Epoch 872: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2599 - accuracy: 0.9188 - val_loss: 0.1930 - val_accuracy: 0.9418 - 15s/epoch - 35ms/step\n",
            "Epoch 873/1000\n",
            "\n",
            "Epoch 873: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2651 - accuracy: 0.9194 - val_loss: 0.2002 - val_accuracy: 0.9434 - 15s/epoch - 34ms/step\n",
            "Epoch 874/1000\n",
            "\n",
            "Epoch 874: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2636 - accuracy: 0.9191 - val_loss: 0.1894 - val_accuracy: 0.9450 - 15s/epoch - 34ms/step\n",
            "Epoch 875/1000\n",
            "\n",
            "Epoch 875: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2636 - accuracy: 0.9198 - val_loss: 0.1901 - val_accuracy: 0.9442 - 15s/epoch - 35ms/step\n",
            "Epoch 876/1000\n",
            "\n",
            "Epoch 876: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2592 - accuracy: 0.9189 - val_loss: 0.2024 - val_accuracy: 0.9394 - 15s/epoch - 34ms/step\n",
            "Epoch 877/1000\n",
            "\n",
            "Epoch 877: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2630 - accuracy: 0.9200 - val_loss: 0.2113 - val_accuracy: 0.9350 - 15s/epoch - 34ms/step\n",
            "Epoch 878/1000\n",
            "\n",
            "Epoch 878: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2577 - accuracy: 0.9197 - val_loss: 0.1967 - val_accuracy: 0.9392 - 15s/epoch - 35ms/step\n",
            "Epoch 879/1000\n",
            "\n",
            "Epoch 879: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2602 - accuracy: 0.9190 - val_loss: 0.1944 - val_accuracy: 0.9414 - 15s/epoch - 35ms/step\n",
            "Epoch 880/1000\n",
            "\n",
            "Epoch 880: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2625 - accuracy: 0.9192 - val_loss: 0.1940 - val_accuracy: 0.9418 - 15s/epoch - 36ms/step\n",
            "Epoch 881/1000\n",
            "\n",
            "Epoch 881: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2618 - accuracy: 0.9193 - val_loss: 0.2072 - val_accuracy: 0.9398 - 15s/epoch - 35ms/step\n",
            "Epoch 882/1000\n",
            "\n",
            "Epoch 882: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2625 - accuracy: 0.9194 - val_loss: 0.1800 - val_accuracy: 0.9500 - 15s/epoch - 35ms/step\n",
            "Epoch 883/1000\n",
            "\n",
            "Epoch 883: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2595 - accuracy: 0.9182 - val_loss: 0.1965 - val_accuracy: 0.9426 - 15s/epoch - 36ms/step\n",
            "Epoch 884/1000\n",
            "\n",
            "Epoch 884: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2627 - accuracy: 0.9195 - val_loss: 0.1954 - val_accuracy: 0.9428 - 15s/epoch - 35ms/step\n",
            "Epoch 885/1000\n",
            "\n",
            "Epoch 885: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2648 - accuracy: 0.9188 - val_loss: 0.1908 - val_accuracy: 0.9466 - 15s/epoch - 34ms/step\n",
            "Epoch 886/1000\n",
            "\n",
            "Epoch 886: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2588 - accuracy: 0.9194 - val_loss: 0.1982 - val_accuracy: 0.9404 - 15s/epoch - 34ms/step\n",
            "Epoch 887/1000\n",
            "\n",
            "Epoch 887: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2649 - accuracy: 0.9175 - val_loss: 0.1862 - val_accuracy: 0.9426 - 15s/epoch - 34ms/step\n",
            "Epoch 888/1000\n",
            "\n",
            "Epoch 888: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2629 - accuracy: 0.9196 - val_loss: 0.2048 - val_accuracy: 0.9392 - 15s/epoch - 35ms/step\n",
            "Epoch 889/1000\n",
            "\n",
            "Epoch 889: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2581 - accuracy: 0.9216 - val_loss: 0.1987 - val_accuracy: 0.9414 - 15s/epoch - 34ms/step\n",
            "Epoch 890/1000\n",
            "\n",
            "Epoch 890: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2602 - accuracy: 0.9197 - val_loss: 0.2083 - val_accuracy: 0.9404 - 17s/epoch - 40ms/step\n",
            "Epoch 891/1000\n",
            "\n",
            "Epoch 891: val_accuracy did not improve from 0.95240\n",
            "429/429 - 18s - loss: 0.2615 - accuracy: 0.9203 - val_loss: 0.2071 - val_accuracy: 0.9418 - 18s/epoch - 41ms/step\n",
            "Epoch 892/1000\n",
            "\n",
            "Epoch 892: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2586 - accuracy: 0.9202 - val_loss: 0.1910 - val_accuracy: 0.9460 - 15s/epoch - 35ms/step\n",
            "Epoch 893/1000\n",
            "\n",
            "Epoch 893: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2610 - accuracy: 0.9204 - val_loss: 0.1875 - val_accuracy: 0.9490 - 15s/epoch - 35ms/step\n",
            "Epoch 894/1000\n",
            "\n",
            "Epoch 894: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2626 - accuracy: 0.9196 - val_loss: 0.2062 - val_accuracy: 0.9462 - 15s/epoch - 35ms/step\n",
            "Epoch 895/1000\n",
            "\n",
            "Epoch 895: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2558 - accuracy: 0.9210 - val_loss: 0.1975 - val_accuracy: 0.9404 - 15s/epoch - 36ms/step\n",
            "Epoch 896/1000\n",
            "\n",
            "Epoch 896: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2649 - accuracy: 0.9184 - val_loss: 0.1889 - val_accuracy: 0.9448 - 16s/epoch - 37ms/step\n",
            "Epoch 897/1000\n",
            "\n",
            "Epoch 897: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2627 - accuracy: 0.9193 - val_loss: 0.2206 - val_accuracy: 0.9350 - 17s/epoch - 41ms/step\n",
            "Epoch 898/1000\n",
            "\n",
            "Epoch 898: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2602 - accuracy: 0.9200 - val_loss: 0.1978 - val_accuracy: 0.9466 - 17s/epoch - 39ms/step\n",
            "Epoch 899/1000\n",
            "\n",
            "Epoch 899: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2624 - accuracy: 0.9180 - val_loss: 0.2161 - val_accuracy: 0.9378 - 15s/epoch - 35ms/step\n",
            "Epoch 900/1000\n",
            "\n",
            "Epoch 900: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2642 - accuracy: 0.9196 - val_loss: 0.2010 - val_accuracy: 0.9426 - 15s/epoch - 34ms/step\n",
            "Epoch 901/1000\n",
            "\n",
            "Epoch 901: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2618 - accuracy: 0.9199 - val_loss: 0.1883 - val_accuracy: 0.9454 - 15s/epoch - 34ms/step\n",
            "Epoch 902/1000\n",
            "\n",
            "Epoch 902: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2628 - accuracy: 0.9196 - val_loss: 0.1908 - val_accuracy: 0.9418 - 15s/epoch - 35ms/step\n",
            "Epoch 903/1000\n",
            "\n",
            "Epoch 903: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2616 - accuracy: 0.9188 - val_loss: 0.1770 - val_accuracy: 0.9518 - 15s/epoch - 34ms/step\n",
            "Epoch 904/1000\n",
            "\n",
            "Epoch 904: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2610 - accuracy: 0.9200 - val_loss: 0.2530 - val_accuracy: 0.9242 - 15s/epoch - 34ms/step\n",
            "Epoch 905/1000\n",
            "\n",
            "Epoch 905: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2612 - accuracy: 0.9190 - val_loss: 0.1914 - val_accuracy: 0.9460 - 15s/epoch - 35ms/step\n",
            "Epoch 906/1000\n",
            "\n",
            "Epoch 906: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2650 - accuracy: 0.9187 - val_loss: 0.1800 - val_accuracy: 0.9500 - 15s/epoch - 35ms/step\n",
            "Epoch 907/1000\n",
            "\n",
            "Epoch 907: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2630 - accuracy: 0.9187 - val_loss: 0.1866 - val_accuracy: 0.9496 - 15s/epoch - 35ms/step\n",
            "Epoch 908/1000\n",
            "\n",
            "Epoch 908: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2606 - accuracy: 0.9193 - val_loss: 0.1901 - val_accuracy: 0.9442 - 15s/epoch - 35ms/step\n",
            "Epoch 909/1000\n",
            "\n",
            "Epoch 909: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2633 - accuracy: 0.9184 - val_loss: 0.2077 - val_accuracy: 0.9394 - 15s/epoch - 35ms/step\n",
            "Epoch 910/1000\n",
            "\n",
            "Epoch 910: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2613 - accuracy: 0.9197 - val_loss: 0.2055 - val_accuracy: 0.9422 - 15s/epoch - 35ms/step\n",
            "Epoch 911/1000\n",
            "\n",
            "Epoch 911: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2590 - accuracy: 0.9196 - val_loss: 0.1968 - val_accuracy: 0.9432 - 15s/epoch - 36ms/step\n",
            "Epoch 912/1000\n",
            "\n",
            "Epoch 912: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2591 - accuracy: 0.9202 - val_loss: 0.1906 - val_accuracy: 0.9436 - 15s/epoch - 36ms/step\n",
            "Epoch 913/1000\n",
            "\n",
            "Epoch 913: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2576 - accuracy: 0.9212 - val_loss: 0.1914 - val_accuracy: 0.9454 - 15s/epoch - 35ms/step\n",
            "Epoch 914/1000\n",
            "\n",
            "Epoch 914: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2597 - accuracy: 0.9202 - val_loss: 0.1974 - val_accuracy: 0.9434 - 15s/epoch - 34ms/step\n",
            "Epoch 915/1000\n",
            "\n",
            "Epoch 915: val_accuracy did not improve from 0.95240\n",
            "429/429 - 14s - loss: 0.2621 - accuracy: 0.9182 - val_loss: 0.1907 - val_accuracy: 0.9440 - 14s/epoch - 34ms/step\n",
            "Epoch 916/1000\n",
            "\n",
            "Epoch 916: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2563 - accuracy: 0.9209 - val_loss: 0.1997 - val_accuracy: 0.9432 - 15s/epoch - 34ms/step\n",
            "Epoch 917/1000\n",
            "\n",
            "Epoch 917: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2629 - accuracy: 0.9194 - val_loss: 0.2054 - val_accuracy: 0.9394 - 15s/epoch - 34ms/step\n",
            "Epoch 918/1000\n",
            "\n",
            "Epoch 918: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2682 - accuracy: 0.9173 - val_loss: 0.1874 - val_accuracy: 0.9468 - 15s/epoch - 34ms/step\n",
            "Epoch 919/1000\n",
            "\n",
            "Epoch 919: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2627 - accuracy: 0.9182 - val_loss: 0.2148 - val_accuracy: 0.9360 - 15s/epoch - 36ms/step\n",
            "Epoch 920/1000\n",
            "\n",
            "Epoch 920: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2620 - accuracy: 0.9183 - val_loss: 0.1970 - val_accuracy: 0.9462 - 15s/epoch - 35ms/step\n",
            "Epoch 921/1000\n",
            "\n",
            "Epoch 921: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2609 - accuracy: 0.9197 - val_loss: 0.1727 - val_accuracy: 0.9510 - 15s/epoch - 36ms/step\n",
            "Epoch 922/1000\n",
            "\n",
            "Epoch 922: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2625 - accuracy: 0.9191 - val_loss: 0.2544 - val_accuracy: 0.9200 - 15s/epoch - 35ms/step\n",
            "Epoch 923/1000\n",
            "\n",
            "Epoch 923: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2612 - accuracy: 0.9201 - val_loss: 0.2199 - val_accuracy: 0.9336 - 15s/epoch - 36ms/step\n",
            "Epoch 924/1000\n",
            "\n",
            "Epoch 924: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2593 - accuracy: 0.9206 - val_loss: 0.1848 - val_accuracy: 0.9492 - 15s/epoch - 35ms/step\n",
            "Epoch 925/1000\n",
            "\n",
            "Epoch 925: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2641 - accuracy: 0.9184 - val_loss: 0.2165 - val_accuracy: 0.9346 - 16s/epoch - 36ms/step\n",
            "Epoch 926/1000\n",
            "\n",
            "Epoch 926: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2604 - accuracy: 0.9190 - val_loss: 0.2072 - val_accuracy: 0.9372 - 15s/epoch - 35ms/step\n",
            "Epoch 927/1000\n",
            "\n",
            "Epoch 927: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2600 - accuracy: 0.9200 - val_loss: 0.2031 - val_accuracy: 0.9416 - 15s/epoch - 34ms/step\n",
            "Epoch 928/1000\n",
            "\n",
            "Epoch 928: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2608 - accuracy: 0.9194 - val_loss: 0.1973 - val_accuracy: 0.9402 - 15s/epoch - 34ms/step\n",
            "Epoch 929/1000\n",
            "\n",
            "Epoch 929: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2609 - accuracy: 0.9197 - val_loss: 0.1849 - val_accuracy: 0.9452 - 15s/epoch - 34ms/step\n",
            "Epoch 930/1000\n",
            "\n",
            "Epoch 930: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2627 - accuracy: 0.9188 - val_loss: 0.2005 - val_accuracy: 0.9410 - 15s/epoch - 34ms/step\n",
            "Epoch 931/1000\n",
            "\n",
            "Epoch 931: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2620 - accuracy: 0.9176 - val_loss: 0.2101 - val_accuracy: 0.9366 - 15s/epoch - 34ms/step\n",
            "Epoch 932/1000\n",
            "\n",
            "Epoch 932: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2632 - accuracy: 0.9193 - val_loss: 0.2057 - val_accuracy: 0.9394 - 15s/epoch - 34ms/step\n",
            "Epoch 933/1000\n",
            "\n",
            "Epoch 933: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2617 - accuracy: 0.9202 - val_loss: 0.1839 - val_accuracy: 0.9494 - 15s/epoch - 35ms/step\n",
            "Epoch 934/1000\n",
            "\n",
            "Epoch 934: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2565 - accuracy: 0.9207 - val_loss: 0.1836 - val_accuracy: 0.9470 - 15s/epoch - 34ms/step\n",
            "Epoch 935/1000\n",
            "\n",
            "Epoch 935: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2635 - accuracy: 0.9180 - val_loss: 0.1788 - val_accuracy: 0.9500 - 15s/epoch - 35ms/step\n",
            "Epoch 936/1000\n",
            "\n",
            "Epoch 936: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2627 - accuracy: 0.9192 - val_loss: 0.2196 - val_accuracy: 0.9354 - 15s/epoch - 35ms/step\n",
            "Epoch 937/1000\n",
            "\n",
            "Epoch 937: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2633 - accuracy: 0.9182 - val_loss: 0.1802 - val_accuracy: 0.9496 - 16s/epoch - 36ms/step\n",
            "Epoch 938/1000\n",
            "\n",
            "Epoch 938: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2581 - accuracy: 0.9194 - val_loss: 0.2163 - val_accuracy: 0.9374 - 16s/epoch - 37ms/step\n",
            "Epoch 939/1000\n",
            "\n",
            "Epoch 939: val_accuracy did not improve from 0.95240\n",
            "429/429 - 32s - loss: 0.2615 - accuracy: 0.9200 - val_loss: 0.1866 - val_accuracy: 0.9438 - 32s/epoch - 74ms/step\n",
            "Epoch 940/1000\n",
            "\n",
            "Epoch 940: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2611 - accuracy: 0.9185 - val_loss: 0.2011 - val_accuracy: 0.9418 - 15s/epoch - 35ms/step\n",
            "Epoch 941/1000\n",
            "\n",
            "Epoch 941: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2605 - accuracy: 0.9186 - val_loss: 0.2051 - val_accuracy: 0.9422 - 15s/epoch - 35ms/step\n",
            "Epoch 942/1000\n",
            "\n",
            "Epoch 942: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2610 - accuracy: 0.9207 - val_loss: 0.1819 - val_accuracy: 0.9512 - 15s/epoch - 34ms/step\n",
            "Epoch 943/1000\n",
            "\n",
            "Epoch 943: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2605 - accuracy: 0.9190 - val_loss: 0.1906 - val_accuracy: 0.9420 - 15s/epoch - 34ms/step\n",
            "Epoch 944/1000\n",
            "\n",
            "Epoch 944: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2611 - accuracy: 0.9199 - val_loss: 0.2064 - val_accuracy: 0.9420 - 15s/epoch - 34ms/step\n",
            "Epoch 945/1000\n",
            "\n",
            "Epoch 945: val_accuracy did not improve from 0.95240\n",
            "429/429 - 14s - loss: 0.2601 - accuracy: 0.9197 - val_loss: 0.2131 - val_accuracy: 0.9364 - 14s/epoch - 34ms/step\n",
            "Epoch 946/1000\n",
            "\n",
            "Epoch 946: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2593 - accuracy: 0.9197 - val_loss: 0.1829 - val_accuracy: 0.9458 - 15s/epoch - 34ms/step\n",
            "Epoch 947/1000\n",
            "\n",
            "Epoch 947: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2617 - accuracy: 0.9198 - val_loss: 0.1874 - val_accuracy: 0.9440 - 15s/epoch - 35ms/step\n",
            "Epoch 948/1000\n",
            "\n",
            "Epoch 948: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2618 - accuracy: 0.9182 - val_loss: 0.1829 - val_accuracy: 0.9472 - 15s/epoch - 35ms/step\n",
            "Epoch 949/1000\n",
            "\n",
            "Epoch 949: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2628 - accuracy: 0.9196 - val_loss: 0.2066 - val_accuracy: 0.9388 - 15s/epoch - 35ms/step\n",
            "Epoch 950/1000\n",
            "\n",
            "Epoch 950: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2606 - accuracy: 0.9189 - val_loss: 0.1944 - val_accuracy: 0.9426 - 15s/epoch - 36ms/step\n",
            "Epoch 951/1000\n",
            "\n",
            "Epoch 951: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2602 - accuracy: 0.9210 - val_loss: 0.2042 - val_accuracy: 0.9408 - 15s/epoch - 35ms/step\n",
            "Epoch 952/1000\n",
            "\n",
            "Epoch 952: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2592 - accuracy: 0.9197 - val_loss: 0.1913 - val_accuracy: 0.9446 - 16s/epoch - 37ms/step\n",
            "Epoch 953/1000\n",
            "\n",
            "Epoch 953: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2604 - accuracy: 0.9205 - val_loss: 0.1839 - val_accuracy: 0.9492 - 17s/epoch - 39ms/step\n",
            "Epoch 954/1000\n",
            "\n",
            "Epoch 954: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2637 - accuracy: 0.9200 - val_loss: 0.1922 - val_accuracy: 0.9474 - 16s/epoch - 38ms/step\n",
            "Epoch 955/1000\n",
            "\n",
            "Epoch 955: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2591 - accuracy: 0.9207 - val_loss: 0.2101 - val_accuracy: 0.9378 - 15s/epoch - 34ms/step\n",
            "Epoch 956/1000\n",
            "\n",
            "Epoch 956: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2610 - accuracy: 0.9191 - val_loss: 0.1928 - val_accuracy: 0.9440 - 15s/epoch - 34ms/step\n",
            "Epoch 957/1000\n",
            "\n",
            "Epoch 957: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2600 - accuracy: 0.9204 - val_loss: 0.1962 - val_accuracy: 0.9454 - 15s/epoch - 34ms/step\n",
            "Epoch 958/1000\n",
            "\n",
            "Epoch 958: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2645 - accuracy: 0.9192 - val_loss: 0.2169 - val_accuracy: 0.9342 - 15s/epoch - 34ms/step\n",
            "Epoch 959/1000\n",
            "\n",
            "Epoch 959: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2598 - accuracy: 0.9180 - val_loss: 0.1966 - val_accuracy: 0.9448 - 15s/epoch - 34ms/step\n",
            "Epoch 960/1000\n",
            "\n",
            "Epoch 960: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2596 - accuracy: 0.9190 - val_loss: 0.2032 - val_accuracy: 0.9432 - 15s/epoch - 34ms/step\n",
            "Epoch 961/1000\n",
            "\n",
            "Epoch 961: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2624 - accuracy: 0.9185 - val_loss: 0.1935 - val_accuracy: 0.9468 - 15s/epoch - 35ms/step\n",
            "Epoch 962/1000\n",
            "\n",
            "Epoch 962: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2621 - accuracy: 0.9191 - val_loss: 0.2045 - val_accuracy: 0.9376 - 15s/epoch - 35ms/step\n",
            "Epoch 963/1000\n",
            "\n",
            "Epoch 963: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2590 - accuracy: 0.9201 - val_loss: 0.2095 - val_accuracy: 0.9396 - 15s/epoch - 35ms/step\n",
            "Epoch 964/1000\n",
            "\n",
            "Epoch 964: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2626 - accuracy: 0.9190 - val_loss: 0.2089 - val_accuracy: 0.9384 - 15s/epoch - 36ms/step\n",
            "Epoch 965/1000\n",
            "\n",
            "Epoch 965: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2614 - accuracy: 0.9200 - val_loss: 0.2093 - val_accuracy: 0.9414 - 15s/epoch - 35ms/step\n",
            "Epoch 966/1000\n",
            "\n",
            "Epoch 966: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2629 - accuracy: 0.9182 - val_loss: 0.2063 - val_accuracy: 0.9402 - 15s/epoch - 35ms/step\n",
            "Epoch 967/1000\n",
            "\n",
            "Epoch 967: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2582 - accuracy: 0.9215 - val_loss: 0.2027 - val_accuracy: 0.9368 - 16s/epoch - 36ms/step\n",
            "Epoch 968/1000\n",
            "\n",
            "Epoch 968: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2618 - accuracy: 0.9189 - val_loss: 0.2021 - val_accuracy: 0.9378 - 15s/epoch - 35ms/step\n",
            "Epoch 969/1000\n",
            "\n",
            "Epoch 969: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2608 - accuracy: 0.9194 - val_loss: 0.1937 - val_accuracy: 0.9436 - 15s/epoch - 35ms/step\n",
            "Epoch 970/1000\n",
            "\n",
            "Epoch 970: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2628 - accuracy: 0.9187 - val_loss: 0.1845 - val_accuracy: 0.9486 - 15s/epoch - 34ms/step\n",
            "Epoch 971/1000\n",
            "\n",
            "Epoch 971: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2591 - accuracy: 0.9191 - val_loss: 0.1935 - val_accuracy: 0.9406 - 15s/epoch - 34ms/step\n",
            "Epoch 972/1000\n",
            "\n",
            "Epoch 972: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2627 - accuracy: 0.9186 - val_loss: 0.1874 - val_accuracy: 0.9462 - 15s/epoch - 34ms/step\n",
            "Epoch 973/1000\n",
            "\n",
            "Epoch 973: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2626 - accuracy: 0.9183 - val_loss: 0.2296 - val_accuracy: 0.9302 - 15s/epoch - 35ms/step\n",
            "Epoch 974/1000\n",
            "\n",
            "Epoch 974: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2617 - accuracy: 0.9195 - val_loss: 0.1887 - val_accuracy: 0.9472 - 15s/epoch - 34ms/step\n",
            "Epoch 975/1000\n",
            "\n",
            "Epoch 975: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2608 - accuracy: 0.9196 - val_loss: 0.1919 - val_accuracy: 0.9444 - 15s/epoch - 35ms/step\n",
            "Epoch 976/1000\n",
            "\n",
            "Epoch 976: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2606 - accuracy: 0.9191 - val_loss: 0.1968 - val_accuracy: 0.9410 - 15s/epoch - 34ms/step\n",
            "Epoch 977/1000\n",
            "\n",
            "Epoch 977: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2621 - accuracy: 0.9188 - val_loss: 0.1861 - val_accuracy: 0.9468 - 15s/epoch - 36ms/step\n",
            "Epoch 978/1000\n",
            "\n",
            "Epoch 978: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2617 - accuracy: 0.9182 - val_loss: 0.2610 - val_accuracy: 0.9232 - 15s/epoch - 35ms/step\n",
            "Epoch 979/1000\n",
            "\n",
            "Epoch 979: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2617 - accuracy: 0.9193 - val_loss: 0.2192 - val_accuracy: 0.9296 - 15s/epoch - 35ms/step\n",
            "Epoch 980/1000\n",
            "\n",
            "Epoch 980: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2628 - accuracy: 0.9192 - val_loss: 0.1788 - val_accuracy: 0.9496 - 15s/epoch - 35ms/step\n",
            "Epoch 981/1000\n",
            "\n",
            "Epoch 981: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2574 - accuracy: 0.9201 - val_loss: 0.1916 - val_accuracy: 0.9412 - 16s/epoch - 37ms/step\n",
            "Epoch 982/1000\n",
            "\n",
            "Epoch 982: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2593 - accuracy: 0.9198 - val_loss: 0.2247 - val_accuracy: 0.9372 - 15s/epoch - 35ms/step\n",
            "Epoch 983/1000\n",
            "\n",
            "Epoch 983: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2605 - accuracy: 0.9191 - val_loss: 0.1916 - val_accuracy: 0.9454 - 15s/epoch - 35ms/step\n",
            "Epoch 984/1000\n",
            "\n",
            "Epoch 984: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2635 - accuracy: 0.9198 - val_loss: 0.1848 - val_accuracy: 0.9486 - 15s/epoch - 34ms/step\n",
            "Epoch 985/1000\n",
            "\n",
            "Epoch 985: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2597 - accuracy: 0.9193 - val_loss: 0.1964 - val_accuracy: 0.9418 - 15s/epoch - 35ms/step\n",
            "Epoch 986/1000\n",
            "\n",
            "Epoch 986: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2585 - accuracy: 0.9197 - val_loss: 0.1979 - val_accuracy: 0.9412 - 15s/epoch - 35ms/step\n",
            "Epoch 987/1000\n",
            "\n",
            "Epoch 987: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2614 - accuracy: 0.9191 - val_loss: 0.1918 - val_accuracy: 0.9434 - 15s/epoch - 34ms/step\n",
            "Epoch 988/1000\n",
            "\n",
            "Epoch 988: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2619 - accuracy: 0.9184 - val_loss: 0.1833 - val_accuracy: 0.9498 - 15s/epoch - 34ms/step\n",
            "Epoch 989/1000\n",
            "\n",
            "Epoch 989: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2599 - accuracy: 0.9196 - val_loss: 0.1816 - val_accuracy: 0.9484 - 15s/epoch - 34ms/step\n",
            "Epoch 990/1000\n",
            "\n",
            "Epoch 990: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2618 - accuracy: 0.9197 - val_loss: 0.1956 - val_accuracy: 0.9408 - 15s/epoch - 35ms/step\n",
            "Epoch 991/1000\n",
            "\n",
            "Epoch 991: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2587 - accuracy: 0.9207 - val_loss: 0.1978 - val_accuracy: 0.9450 - 15s/epoch - 35ms/step\n",
            "Epoch 992/1000\n",
            "\n",
            "Epoch 992: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2608 - accuracy: 0.9185 - val_loss: 0.1899 - val_accuracy: 0.9438 - 15s/epoch - 36ms/step\n",
            "Epoch 993/1000\n",
            "\n",
            "Epoch 993: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2620 - accuracy: 0.9196 - val_loss: 0.2029 - val_accuracy: 0.9386 - 15s/epoch - 35ms/step\n",
            "Epoch 994/1000\n",
            "\n",
            "Epoch 994: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2590 - accuracy: 0.9188 - val_loss: 0.1953 - val_accuracy: 0.9418 - 15s/epoch - 36ms/step\n",
            "Epoch 995/1000\n",
            "\n",
            "Epoch 995: val_accuracy did not improve from 0.95240\n",
            "429/429 - 16s - loss: 0.2628 - accuracy: 0.9191 - val_loss: 0.1802 - val_accuracy: 0.9484 - 16s/epoch - 37ms/step\n",
            "Epoch 996/1000\n",
            "\n",
            "Epoch 996: val_accuracy did not improve from 0.95240\n",
            "429/429 - 17s - loss: 0.2606 - accuracy: 0.9198 - val_loss: 0.1923 - val_accuracy: 0.9416 - 17s/epoch - 40ms/step\n",
            "Epoch 997/1000\n",
            "\n",
            "Epoch 997: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2613 - accuracy: 0.9205 - val_loss: 0.1887 - val_accuracy: 0.9480 - 15s/epoch - 35ms/step\n",
            "Epoch 998/1000\n",
            "\n",
            "Epoch 998: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2626 - accuracy: 0.9205 - val_loss: 0.1888 - val_accuracy: 0.9464 - 15s/epoch - 35ms/step\n",
            "Epoch 999/1000\n",
            "\n",
            "Epoch 999: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2583 - accuracy: 0.9191 - val_loss: 0.2132 - val_accuracy: 0.9388 - 15s/epoch - 35ms/step\n",
            "Epoch 1000/1000\n",
            "\n",
            "Epoch 1000: val_accuracy did not improve from 0.95240\n",
            "429/429 - 15s - loss: 0.2589 - accuracy: 0.9209 - val_loss: 0.1827 - val_accuracy: 0.9514 - 15s/epoch - 34ms/step\n"
          ]
        }
      ],
      "source": [
        "MnistHistory = MnistModel.fit(\n",
        "    datagen.flow(xTrainMnist, yTrainMnist, batch_size=128), \n",
        "    epochs=1000, \n",
        "    validation_data=(xValidMnist, yValidMnist), \n",
        "    callbacks=[CBCheckPoint], \n",
        "    verbose=2, \n",
        "    steps_per_epoch=xTrainMnist.shape[0] // 128\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### lets see accuracy of the Mnist model : \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "assure the 4 dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "xTestMnist = np.expand_dims(xTestMnist, axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " Evaluate the model on the test set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 4ms/step - loss: 0.1940 - accuracy: 0.9446\n",
            "Pérdida en el conjunto de prueba: 19.40%\n",
            "Precisión en el conjunto de prueba: 94.46%\n"
          ]
        }
      ],
      "source": [
        "loss, acc = MnistModel.evaluate(xTestMnist, yTestMnist)\n",
        "print(f\"Pérdida en el conjunto de prueba: {loss * 100:.2f}%\")\n",
        "print(f\"Precisión en el conjunto de prueba: {acc* 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " error  of the model : 19.40%\n"
          ]
        }
      ],
      "source": [
        "plt.plot(MnistHistory.history['loss'], label='Loss (TRAIN)')\n",
        "plt.plot(MnistHistory.history['val_loss'], label='Loss (VALIDATION)')\n",
        "plt.xlabel('EPOCHS')\n",
        "plt.ylabel('LOSS')\n",
        "plt.title('Loss per epoch')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "print(f\" error  of the model : {loss*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy of the model : 94.46%\n"
          ]
        }
      ],
      "source": [
        "plt.plot(MnistHistory.history['accuracy'], label='accuracy (TRAIN)')\n",
        "plt.plot(MnistHistory.history['val_accuracy'], label='accuracy (VALIDATION)')\n",
        "plt.xlabel('EPOCHS')\n",
        "plt.ylabel('accuracy')\n",
        "plt.title('accuracy per epoch')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "print(f\"accuracy of the model : {acc*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## proceed with a class prediciton for our random sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "use a random variable to our data sample "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "id = np.random.choice(xTestMnist.shape[0], 1000, replace=False) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 674ms/step\n"
          ]
        }
      ],
      "source": [
        "MnistPredictions = MnistModel.predict(xTestMnist[id[0]:id[0]+1]) # make predictions on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "predicted class: [3]\n",
            "rial class:  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAZ1klEQVR4nO3de3BUZZrH8afDJQTIxRByk4ABBFQkziBCBsU4pBJxlwFkXPEyC1MUFAiUId4qUwoyY20crEVLCuGPnSFSpdxmuAzoxMVgkkETLNBshlWRMFHCQmBkKwkECbmcrfew6aE1SJ2mw9Pd5/upeqvT3efJORxO+tfvOW+/7bEsyxIAAK6ziOu9QgAADAIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKnpKkOno6JATJ05IdHS0eDwe7c0BADhk5jc4e/aspKamSkREROgEkAmftLQ07c0AAFyjuro6GTRoUOgEkOn5GHfLA9JTemlvDgDAoTZplX3yrvf1/LoH0Jo1a+SVV16R+vp6ycjIkNWrV8tdd9111brO024mfHp6CCAACDn/P8Po1S6jdMsghM2bN0t+fr4sX75cPvnkEzuAcnNz5fTp092xOgBACOqWAFq1apXMmzdPfvnLX8qtt94q69atk759+8rvf//77lgdACAEBTyALl68KAcPHpTs7Ox/rCQiwr5fUVHxveVbWlqkqanJpwEAwl/AA+ibb76R9vZ2SUpK8nnc3DfXg76rsLBQYmNjvY0RcADgDuofRC0oKJDGxkZvM8P2AADhL+Cj4BISEqRHjx5y6tQpn8fN/eTk5O8tHxkZaTcAgLsEvAfUu3dvGTt2rJSUlPjMbmDuZ2ZmBnp1AIAQ1S2fAzJDsGfPni133nmn/dmf1157TZqbm+1RcQAAdFsAPfzww/L3v/9dli1bZg88uOOOO6S4uPh7AxMAAO7lscyscUHEDMM2o+GyZBozIQBACGqzWqVUdtoDy2JiYoJ3FBwAwJ0IIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqOips1ogfHTc8yPnRS9+47hkzy27HNe0Wx3ij9Pt5x3XPJT/lOOafn/Y77gG4YMeEABABQEEAAiPAHrxxRfF4/H4tFGjRgV6NQCAENct14Buu+02ef/99/+xkp5cagIA+OqWZDCBk5yc3B2/GgAQJrrlGtCRI0ckNTVVhg4dKo899pgcO3bsisu2tLRIU1OTTwMAhL+AB9D48eOlqKhIiouLZe3atVJbWyv33HOPnD17tsvlCwsLJTY21tvS0tICvUkAADcE0JQpU+Shhx6SMWPGSG5urrz77rvS0NAgW7Zs6XL5goICaWxs9La6urpAbxIAIAh1++iAuLg4GTFihNTU1HT5fGRkpN0AAO7S7Z8DOnfunBw9elRSUlK6e1UAADcH0NNPPy1lZWXy1VdfyUcffSQzZsyQHj16yCOPPBLoVQEAQljAT8EdP37cDpszZ87IwIED5e6775bKykr7ZwAAui2ANm3aFOhfCVw35/5lguOaJ1/a6LhmQp//cVxzrE2um6Qezq/LPrCs1HHNX95LdFzTcYURtQg9zAUHAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEAAgPL+QDggl76561XHN2I35jmuGPVMhwez4H29zXFM1YYPjmg9jhzuuYTLS8EEPCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACggtmwgcv8b3u745qb3mmRcNPyVbTzognOS6yoSOdFCBv0gAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKhgMlLgMvMfX+K4pveXxx3XOJ/yNDy1pN3guKbnkW7ZFCigBwQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFk5ECl4n4y6dhNbFowy8y/ap77+evOK455ceO6POZ84lc25yvBkGKHhAAQAUBBAAIjQAqLy+XqVOnSmpqqng8HtmxY4fP85ZlybJlyyQlJUWioqIkOztbjhzhCzwAANcYQM3NzZKRkSFr1qzp8vmVK1fK66+/LuvWrZP9+/dLv379JDc3Vy5cuOB0VQCAMOZ4EMKUKVPs1hXT+3nttdfk+eefl2nTptmPbdiwQZKSkuye0qxZs659iwEAYSGg14Bqa2ulvr7ePu3WKTY2VsaPHy8VFRVd1rS0tEhTU5NPAwCEv4AGkAkfw/R4Lmfudz73XYWFhXZIdba0tLRAbhIAIEipj4IrKCiQxsZGb6urq9PeJABAqAVQcnKyfXvq1Cmfx839zue+KzIyUmJiYnwaACD8BTSA0tPT7aApKSnxPmau6ZjRcJmZ/n0iGwAQnhyPgjt37pzU1NT4DDyoqqqS+Ph4GTx4sOTl5clLL70kN998sx1IL7zwgv2ZoenTpwd62wEAbgqgAwcOyH333ee9n5+fb9/Onj1bioqK5Nlnn7U/KzR//nxpaGiQu+++W4qLi6VPnz6B3XIAQEjzWObDO0HEnLIzo+GyZJr09PTS3hwgaNT+m/PT2P8xa61f63qn8Q7HNZ/k/chxTUSZ88lfEfzarFYplZ32wLIfuq6vPgoOAOBOBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAIDQ+DoG4HqL3TfAcc1LaX/ya125/5nnuKb/Eeeztj/wyEeOa7YMXOW4ZkLFfPHHTXP+5rgmopmZreEMPSAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqmIwUQS+93xnHNcN6Rvm1ri8fWCfXQ4R4HNfc8fFcxzWDH/qr+KPDryrAGXpAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVDAZKYLef41zfpj+k2eCX+s6/tSdjmuaR7Y4rvlD1lrHNT8fWuW45r2HJ4k/ojdX+lUHOEEPCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAomI0XQs9rartu6bnz5o+uynse3znVcU/2TIsc1B58YLP5o2exXGeAIPSAAgAoCCAAQGgFUXl4uU6dOldTUVPF4PLJjxw6f5+fMmWM/fnm7//77A7nNAAA3BlBzc7NkZGTImjVrrriMCZyTJ09628aNG691OwEAbh+EMGXKFLv9kMjISElOTr6W7QIAhLluuQZUWloqiYmJMnLkSFm4cKGcOXPmisu2tLRIU1OTTwMAhL+AB5A5/bZhwwYpKSmR3/72t1JWVmb3mNrb27tcvrCwUGJjY70tLS0t0JsEAHDD54BmzZrl/fn222+XMWPGyLBhw+xe0eTJk7+3fEFBgeTn53vvmx4QIQQA4a/bh2EPHTpUEhISpKam5orXi2JiYnwaACD8dXsAHT9+3L4GlJKS0t2rAgCE8ym4c+fO+fRmamtrpaqqSuLj4+22YsUKmTlzpj0K7ujRo/Lss8/K8OHDJTc3N9DbDgBwUwAdOHBA7rvvPu/9zus3s2fPlrVr10p1dbW8+eab0tDQYH9YNScnR37zm9/Yp9oAAPA7gLKyssSyrCs+/9577zn9lYDrJG6Icl70E+clN/c/7bxIRD7r29dxTcf5836tC+7FXHAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAgPD4Sm4AV9fvo66/IfiH/Kn5Bsc1TW1+zLotIlZ7m191gBP0gAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKhgMlJAQfNPhjuu+Vm/PY5r3jwZLf6wWur9qgOcoAcEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABZORAteoR1ys45pTv/jWcU2EeBzXNFyIEn/4VwU4Qw8IAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACiYjBa7R54UjHdd8OXGt45pJf33IcU3cEx3ijza/qgBn6AEBAFQQQACA4A+gwsJCGTdunERHR0tiYqJMnz5dDh8+7LPMhQsXZNGiRTJgwADp37+/zJw5U06dOhXo7QYAuCmAysrK7HCprKyUPXv2SGtrq+Tk5Ehzc7N3maVLl8quXbtk69at9vInTpyQBx98sDu2HQDglkEIxcXFPveLiorsntDBgwdl0qRJ0tjYKL/73e/k7bfflp/+9Kf2MuvXr5dbbrnFDq0JEyYEdusBAO68BmQCx4iPj7dvTRCZXlF2drZ3mVGjRsngwYOloqKiy9/R0tIiTU1NPg0AEP78DqCOjg7Jy8uTiRMnyujRo+3H6uvrpXfv3hIXF+ezbFJSkv3cla4rxcbGeltaWpq/mwQAcEMAmWtBhw4dkk2bNl3TBhQUFNg9qc5WV1d3Tb8PABDGH0RdvHix7N69W8rLy2XQoEHex5OTk+XixYvS0NDg0wsyo+DMc12JjIy0GwDAXRz1gCzLssNn+/btsnfvXklPT/d5fuzYsdKrVy8pKSnxPmaGaR87dkwyMzMDt9UAAHf1gMxpNzPCbefOnfZngTqv65hrN1FRUfbt3LlzJT8/3x6YEBMTI0uWLLHDhxFwAAC/A2jt2kvzV2VlZfk8boZaz5kzx/751VdflYiICPsDqGaEW25urrzxxhtOVgMAcAGPZc6rBREzDNv0pLJkmvT09NLenJDj6en8sl7zz8b6ta6+2/ZLsOqRMMCvuqTdFx3XrB60x3HNfVX/6rhmYF6r45r2mlrHNcC1arNapVR22gPLzJmwK2EuOACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIABA6HwjKoLXl/9+p+OaX2T9xa91VdQ7X1fPhm8d13w9PcFxzYLH3nFcY9fF/c1xzYh3ljiuubXwtOOattqvHdcAwYweEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABVMRhpmHppU6bhmWcJf/VrXuS0HHNd0WJbjmpiIPo5r/rv1ovjjrsJ8xzUj3tjvuKato91xDRBu6AEBAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQwWSkYaZ69ijHNTNWJ/u1rj8Of8dxzZbmRMc1z38w03HNqDeaxB+J1R/5VQfAOXpAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVDAZaZjpqP7CcU3Lvf6t659lrFwPI+RjxzUd3bIlAAKJHhAAQAUBBAAI/gAqLCyUcePGSXR0tCQmJsr06dPl8OHDPstkZWWJx+PxaQsWLAj0dgMA3BRAZWVlsmjRIqmsrJQ9e/ZIa2ur5OTkSHNzs89y8+bNk5MnT3rbypUrA73dAAA3DUIoLi72uV9UVGT3hA4ePCiTJk3yPt63b19JTvbvWzYBAO5wTdeAGhsb7dv4+Hifx9966y1JSEiQ0aNHS0FBgZw/f/6Kv6OlpUWampp8GgAg/Pk9DLujo0Py8vJk4sSJdtB0evTRR2XIkCGSmpoq1dXV8txzz9nXibZt23bF60orVqzwdzMAACHKY1mW5U/hwoUL5c9//rPs27dPBg0adMXl9u7dK5MnT5aamhoZNmxYlz0g0zqZHlBaWppkyTTp6enlz6YBABS1Wa1SKjvts2QxMTGB7QEtXrxYdu/eLeXl5T8YPsb48ePt2ysFUGRkpN0AAO7iKIBMZ2nJkiWyfft2KS0tlfT09KvWVFVV2bcpKSn+byUAwN0BZIZgv/3227Jz5077s0D19fX247GxsRIVFSVHjx61n3/ggQdkwIAB9jWgpUuX2iPkxowZ013/BgBAuF8DMh8q7cr69etlzpw5UldXJ48//rgcOnTI/myQuZYzY8YMef7553/wPODlzDUgE2hcAwKA0NQt14CullUmcMyHVQEAuBrmggMAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqOgpQcayLPu2TVpFLv0IAAgh9uv3Za/nIRNAZ8+etW/3ybvamwIAuMbX89jY2Cs+77GuFlHXWUdHh5w4cUKio6PF4/H4PNfU1CRpaWlSV1cnMTEx4lbsh0vYD5ewHy5hPwTPfjCxYsInNTVVIiIiQqcHZDZ20KBBP7iM2aluPsA6sR8uYT9cwn64hP0QHPvhh3o+nRiEAABQQQABAFSEVABFRkbK8uXL7Vs3Yz9cwn64hP1wCfsh9PZD0A1CAAC4Q0j1gAAA4YMAAgCoIIAAACoIIACAipAJoDVr1shNN90kffr0kfHjx8vHH38sbvPiiy/as0Nc3kaNGiXhrry8XKZOnWp/qtr8m3fs2OHzvBlHs2zZMklJSZGoqCjJzs6WI0eOiNv2w5w5c753fNx///0STgoLC2XcuHH2TCmJiYkyffp0OXz4sM8yFy5ckEWLFsmAAQOkf//+MnPmTDl16pS4bT9kZWV973hYsGCBBJOQCKDNmzdLfn6+PbTwk08+kYyMDMnNzZXTp0+L29x2221y8uRJb9u3b5+Eu+bmZvv/3LwJ6crKlSvl9ddfl3Xr1sn+/fulX79+9vFhXojctB8MEziXHx8bN26UcFJWVmaHS2VlpezZs0daW1slJyfH3jedli5dKrt27ZKtW7fay5upvR588EFx234w5s2b53M8mL+VoGKFgLvuustatGiR9357e7uVmppqFRYWWm6yfPlyKyMjw3Izc8hu377de7+jo8NKTk62XnnlFe9jDQ0NVmRkpLVx40bLLfvBmD17tjVt2jTLTU6fPm3vi7KyMu//fa9evaytW7d6l/n888/tZSoqKiy37Afj3nvvtZ588kkrmAV9D+jixYty8OBB+7TK5fPFmfsVFRXiNubUkjkFM3ToUHnsscfk2LFj4ma1tbVSX1/vc3yYOajMaVo3Hh+lpaX2KZmRI0fKwoUL5cyZMxLOGhsb7dv4+Hj71rxWmN7A5ceDOU09ePDgsD4eGr+zHzq99dZbkpCQIKNHj5aCggI5f/68BJOgm4z0u7755htpb2+XpKQkn8fN/S+++ELcxLyoFhUV2S8upju9YsUKueeee+TQoUP2uWA3MuFjdHV8dD7nFub0mznVlJ6eLkePHpVf/epXMmXKFPuFt0ePHhJuzMz5eXl5MnHiRPsF1jD/571795a4uDjXHA8dXewH49FHH5UhQ4bYb1irq6vlueees68Tbdu2TYJF0AcQ/sG8mHQaM2aMHUjmANuyZYvMnTtXddugb9asWd6fb7/9dvsYGTZsmN0rmjx5soQbcw3EvPlyw3VQf/bD/PnzfY4HM0jHHAfmzYk5LoJB0J+CM91H8+7tu6NYzP3k5GRxM/Mub8SIEVJTUyNu1XkMcHx8nzlNa/5+wvH4WLx4sezevVs++OADn69vMf/n5rR9Q0ODK46HxVfYD10xb1iNYDoegj6ATHd67NixUlJS4tPlNPczMzPFzc6dO2e/mzHvbNzKnG4yLyyXHx/mC7nMaDi3Hx/Hjx+3rwGF0/Fhxl+YF93t27fL3r177f//y5nXil69evkcD+a0k7lWGk7Hg3WV/dCVqqoq+zaojgcrBGzatMke1VRUVGR99tln1vz58624uDirvr7ecpOnnnrKKi0ttWpra60PP/zQys7OthISEuwRMOHs7Nmz1qeffmo3c8iuWrXK/vnrr7+2n3/55Zft42Hnzp1WdXW1PRIsPT3d+vbbby237Afz3NNPP22P9DLHx/vvv2/9+Mc/tm6++WbrwoULVrhYuHChFRsba/8dnDx50tvOnz/vXWbBggXW4MGDrb1791oHDhywMjMz7RZOFl5lP9TU1Fi//vWv7X+/OR7M38bQoUOtSZMmWcEkJALIWL16tX1Q9e7d2x6WXVlZabnNww8/bKWkpNj74MYbb7TvmwMt3H3wwQf2C+53mxl23DkU+4UXXrCSkpLsNyqTJ0+2Dh8+bLlpP5gXnpycHGvgwIH2MOQhQ4ZY8+bNC7s3aV39+01bv369dxnzxuOJJ56wbrjhBqtv377WjBkz7BdnN+2HY8eO2WETHx9v/00MHz7ceuaZZ6zGxkYrmPB1DAAAFUF/DQgAEJ4IIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCIhv8DBoWvMr6FDsEAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "predicted_class = np.argmax(MnistPredictions, axis=1) # get the predicted class by taking the index of the maximum value in the prediction array\n",
        "plt.imshow(xTestMnist[id[0]])\n",
        "\n",
        "print(\"predicted class:\" , predicted_class)\n",
        "print(\"rial class: \",yTestMnist[id[0]]) # display the true class of the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "predicted class: [3]\n",
            "rial class:  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "predicted_class = np.argmax(MnistPredictions, axis=1) # get the predicted class by taking the index of the maximum value in the prediction array\n",
        "plt.imshow(xTestMnist[id[0]])\n",
        "\n",
        "print(\"predicted class:\" , predicted_class)\n",
        "print(\"rial class: \",yTestMnist[id[0]]) # display the true class of the image\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "icrGGZQg7Kb1"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cnnDvlEnv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
